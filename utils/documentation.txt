On top of the features offered by the open-source vectorbt, VectorBT PRO implements many major enhancements and optimizations in the following areas:
Info
To keep the pages short, only some of the most interesting features of each release are showcased. The detailed release notes are available to subscribers only. If you're on the private website, go to Getting started → Release notes.
Tags are releases where features were introduced for the first time. Note that most features are continuously updated, thus the following examples are meant to be run with the latest vectorbt PRO version installed 
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
Get a competitive advantage in trading — be among the first to try the cutting-edge software!
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.

   
We're thrilled to have you with us!
We believe in a future where each individual, whether you're a seasoned trader, a data scientist with a Ph.D., or just a hobbyist, has access to an entire palette of cutting-edge tools to drive their decisions and compete against big players 
This tool is just the first piece of an ecosystem in the making that will empower this goal while already breaking barriers and creating an entirely new universe of possibilities to analyze financial data of any complexity. All it takes is your time and motivation to adopt our approach and extract insights from any data that had been looking like unspectacular numbers before!
Before making first steps with vectorbt, make sure to join our Discord server, where we support each other, report issues, create feature requests, and just chat! Upon joining, please post your GitHub account name (such as @polakowo) to the welcome channel for us to manually link your Discord and GitHub accounts and assign you a member role that will open all other channels. If you don't provide the GitHub name, you'll remain in the welcome channel forever!
If you end your sponsorship, you'll be kicked from the server. If you start your sponsorship again, please redo the steps above.
To make sure that only sponsors have access to the exclusive content, the website of this project has been split into two websites: the public website accessible under https://vectorbt.pro, and the private website that you're on right now, accessible via the link provided in the private repository (bookmark this page: https://github.com/polakowo/vectorbt.pro/blob/pvt-links/README.md  ). The private website is a directory located on the same host as the public one but prepended with a random hash that gets renewed once in a while to discourage information theft. You can identify which website you're on by looking at the closed lock (= public website) or open lock (= private website) in the top left corner.
The website contains 4 major sections:
 Getting started
Contains the installation guide for vectorbtpro and the change log
 Installation
 Release notes
 Features
Contains examples of major enhancements added with each release
 Features
 Tutorials
Contains all the exclusive tutorials
 Tutorials
 Documentation
Contains the documentation on the latest version of vectorbtpro in a user-friendly format
 Documentation
 API
Contains the API documentation on the latest version of vectorbtpro
 API
 Cookbook
Contains short and sweet examples of using vectorbtpro
 Cookbook
If you've ever worked with other backtesting frameworks, you'd expect to implement your trading strategy by taking a Python class, overriding some methods, and then using a limited set of commands to communicate with the backtester, such as to place orders. Using vectorbt isn't anything like that: since it's a quantitative analysis package that operates primarily on arrays, it's more similar to libraries such as Pandas than to frameworks such as backtrader. 
The technical difference between a framework and library lies in a term called inversion of control. When you use a framework, the framework is in charge of the flow. It provides some places for you to plug in your code, but it calls the code you plugged in as needed. When you use a library though, you are in charge of the flow of the application; you are choosing when and where to call the library. Because of that, the functionality of vectorbt is distributed across dozens of modules, each being optional and usually having its own documentation.
For example, even though vectorbt gives you this large module and the related documentation on storing and manipulating data, you can skip it entirely and use just Pandas and NumPy arrays. This fact makes it difficult to compile a perfect getting-started guide since each use case is different and requires a varying set of modules, and as the experience shows, you'll need only a fraction of the functions that vectorbt offers to you!
After you've been added to the list of collaborators and accepted the repository invitation, the first step is to install the package.
What to do next? Here's a list of recommended steps:
 1 - Fundamentals
Learn the fundamental concepts of vectorbt. Why do we use Numba? What is represented by rows and columns? Why is broadcasting so important? How most vectorbt classes work?
 Fundamentals
 Building blocks
 2 - Basic RSI strategy
Apply the fundamental concepts to backtest a basic RSI strategy. Make the example run on your own data, try a different parameter set, add a trading commission, and just experiment!
 Basic RSI strategy
 3 - SuperFast SuperTrend
Learn how to develop, compile, and backtest a Supertrend indicator. Adapt the example to an indicator that's interesting to you. See this documentation for help.
 SuperFast SuperTrend
 4 - Signal development
After building an indicator, learn how to detect events in data and convert them into backtestable signals. This is one of the most important tutorials out there, don't miss!
 Signal development
 5 - Portfolio
After creating signal arrays, learn how to simulate them and analyze their performance. To better understand the workings of the engine, see this and then this documentation.
 Portfolio
 5 - Cross-validation
You've got a promising trading strategy that does well on historical data? It's time to cross-validate it! Not only CV can help to detect overfitting, but it can also unveil market regimes where the trading strategy performs best and worst. 
 Cross-validation
 Freedom
By now, you should have collected enough experience to backtest signal-based strategies. Visit the remaining tutorials and documentation, and get familiar with the API documentation to implement a backtesting pipeline from scratch. Good luck!
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          
Get tips, technical guides, and best practices. Once a month. Right in your inbox.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          
Get tips, technical guides, and best practices. Once a month. Right in your inbox.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          
Get tips, technical guides, and best practices. Once a month. Right in your inbox.
On top of the features offered by the open-source vectorbt, VectorBT PRO implements many major enhancements and optimizations in the following areas:
Info
To keep the pages short, only some of the most interesting features of each release are showcased. The detailed release notes are available to subscribers only. If you're on the private website, go to Getting started → Release notes.
Tags are releases where features were introduced for the first time. Note that most features are continuously updated, thus the following examples are meant to be run with the latest vectorbt PRO version installed 
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
Get a competitive advantage in trading — be among the first to try the cutting-edge software!
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.

   
We're thrilled to have you with us!
We believe in a future where each individual, whether you're a seasoned trader, a data scientist with a Ph.D., or just a hobbyist, has access to an entire palette of cutting-edge tools to drive their decisions and compete against big players 
This tool is just the first piece of an ecosystem in the making that will empower this goal while already breaking barriers and creating an entirely new universe of possibilities to analyze financial data of any complexity. All it takes is your time and motivation to adopt our approach and extract insights from any data that had been looking like unspectacular numbers before!
Before making first steps with vectorbt, make sure to join our Discord server, where we support each other, report issues, create feature requests, and just chat! Upon joining, please post your GitHub account name (such as @polakowo) to the welcome channel for us to manually link your Discord and GitHub accounts and assign you a member role that will open all other channels. If you don't provide the GitHub name, you'll remain in the welcome channel forever!
If you end your sponsorship, you'll be kicked from the server. If you start your sponsorship again, please redo the steps above.
To make sure that only sponsors have access to the exclusive content, the website of this project has been split into two websites: the public website accessible under https://vectorbt.pro, and the private website that you're on right now, accessible via the link provided in the private repository (bookmark this page: https://github.com/polakowo/vectorbt.pro/blob/pvt-links/README.md  ). The private website is a directory located on the same host as the public one but prepended with a random hash that gets renewed once in a while to discourage information theft. You can identify which website you're on by looking at the closed lock (= public website) or open lock (= private website) in the top left corner.
The website contains 4 major sections:
 Getting started
Contains the installation guide for vectorbtpro and the change log
 Installation
 Release notes
 Features
Contains examples of major enhancements added with each release
 Features
 Tutorials
Contains all the exclusive tutorials
 Tutorials
 Documentation
Contains the documentation on the latest version of vectorbtpro in a user-friendly format
 Documentation
 API
Contains the API documentation on the latest version of vectorbtpro
 API
 Cookbook
Contains short and sweet examples of using vectorbtpro
 Cookbook
If you've ever worked with other backtesting frameworks, you'd expect to implement your trading strategy by taking a Python class, overriding some methods, and then using a limited set of commands to communicate with the backtester, such as to place orders. Using vectorbt isn't anything like that: since it's a quantitative analysis package that operates primarily on arrays, it's more similar to libraries such as Pandas than to frameworks such as backtrader. 
The technical difference between a framework and library lies in a term called inversion of control. When you use a framework, the framework is in charge of the flow. It provides some places for you to plug in your code, but it calls the code you plugged in as needed. When you use a library though, you are in charge of the flow of the application; you are choosing when and where to call the library. Because of that, the functionality of vectorbt is distributed across dozens of modules, each being optional and usually having its own documentation.
For example, even though vectorbt gives you this large module and the related documentation on storing and manipulating data, you can skip it entirely and use just Pandas and NumPy arrays. This fact makes it difficult to compile a perfect getting-started guide since each use case is different and requires a varying set of modules, and as the experience shows, you'll need only a fraction of the functions that vectorbt offers to you!
After you've been added to the list of collaborators and accepted the repository invitation, the first step is to install the package.
What to do next? Here's a list of recommended steps:
 1 - Fundamentals
Learn the fundamental concepts of vectorbt. Why do we use Numba? What is represented by rows and columns? Why is broadcasting so important? How most vectorbt classes work?
 Fundamentals
 Building blocks
 2 - Basic RSI strategy
Apply the fundamental concepts to backtest a basic RSI strategy. Make the example run on your own data, try a different parameter set, add a trading commission, and just experiment!
 Basic RSI strategy
 3 - SuperFast SuperTrend
Learn how to develop, compile, and backtest a Supertrend indicator. Adapt the example to an indicator that's interesting to you. See this documentation for help.
 SuperFast SuperTrend
 4 - Signal development
After building an indicator, learn how to detect events in data and convert them into backtestable signals. This is one of the most important tutorials out there, don't miss!
 Signal development
 5 - Portfolio
After creating signal arrays, learn how to simulate them and analyze their performance. To better understand the workings of the engine, see this and then this documentation.
 Portfolio
 5 - Cross-validation
You've got a promising trading strategy that does well on historical data? It's time to cross-validate it! Not only CV can help to detect overfitting, but it can also unveil market regimes where the trading strategy performs best and worst. 
 Cross-validation
 Freedom
By now, you should have collected enough experience to backtest signal-based strategies. Visit the remaining tutorials and documentation, and get familiar with the API documentation to implement a backtesting pipeline from scratch. Good luck!
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          
Get tips, technical guides, and best practices. Once a month. Right in your inbox.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          
Get tips, technical guides, and best practices. Once a month. Right in your inbox.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          
Get tips, technical guides, and best practices. Once a month. Right in your inbox.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          




            Prevent this user from interacting with your repositories and sending you notifications.
          Learn more about blocking users.
        

              You must be logged in to block users.
            

        Contact GitHub support about this user’s behavior.
        Learn more about reporting abuse.
      

        Find your trading edge, using the fastest engine for backtesting, algorithmic trading, and research. 
      



Python





            3.4k
          




            533
          


        Your new Telegram buddy powered by transformers
      



Jupyter Notebook





            415
          




            105
          


        Documentation for data enthusiasts
      



JavaScript





            92
          




            10
          


        Some of my ML projects and Kaggle competitions
      



Jupyter Notebook





            15
          




            2
          


        Visualizing the Global Terrorism Database (GTD) with D3.js
      



Jupyter Notebook





            18
          




            7
          


        Applications using state-of-the-art in NLP
      



Jupyter Notebook





            5
          




            3
          


    Seeing something unexpected? Take a look at the
    GitHub profile guide.
  
Get a competitive advantage in trading — be among the first to try the cutting-edge software!
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          
Get tips, technical guides, and best practices. Once a month. Right in your inbox.
Info
VectorBT PRO is a totally different beast compared to the open-source version. In fact, the PRO version redesigns the underlying core to enable groundbreaking features. 
To avoid using an outdated code, make sure to only import vectorbtpro!
After you've been added to the list of collaborators and accepted the repository invitation, the next step is to create a Personal Access Token for your GitHub account in order to access the PRO repository programmatically (from the command line or GitHub Actions workflows):
Generate the token and save it in a safe place
Important
After a few months you may get an email from GitHub stating that your personal access token has expired. If so, please go over the steps above and generate a new token. This has nothing to do with the access to the vectorbtpro repository!
If you don't have Git, install it.
To use TA-Lib for Python, you need to install the actual library. Follow these instructions.
Other approaches are described here.
If you're on Windows, it's recommended to use WSL for development.
If you plan to use vectorbtpro locally, it's recommended to establish a new environment solely for vectorbtpro. 
The easiest way is to download Anaconda, which has a graphical installer and comes with many popular data science packages required by vectorbtpro such as NumPy, Pandas, Plotly, and more.
After the installation, create a new environment:
Activate the new environment:
Note
You need to activate the environment every time you start a new terminal session.
You should now see vectorbtpro in the list of all environments and being active (notice *):
You can now proceed with the installation of the actual package.
If you primarily work with an IDE, you can create a separate environment for each project. Here is how to create a new environment with PyCharm. The same but for Visual Studio Code is explained here.
The PRO version can be installed with pip.
Hint
It's highly recommended creating a new virtual environment solely for vectorbtpro, such as with Anaconda.
Uninstall the open-source version if installed:
Install the base PRO version (with recommended dependencies) using git+https:
Info
Whenever you are prompted for a password, paste the token that you generated in the previous steps.
To avoid re-entering the token over and over again, you can add it to your system or set an environment variable GH_TOKEN and then install the package as follows:
On some systems, such as macOS, the token is usually remembered automatically.
Read more on managing tokens here.
Same using git+ssh (see Connecting to GitHub with SSH):
Lightweight version (with only required dependencies):
For other optional dependencies, see extra-requirements.txt.
Whenever a new version of vectorbtpro is released, the package will not update by itself - you need to install the update. Gladly, you can use the same exact command that you used to install the package to also update it.
To install from the develop branch:
Set your GitHub username and token using %env:
Warning
Make sure to delete this cell when sharing the notebook with others!
Install TA-Lib:
Install VectorBT PRO:
Restart the runtime, and you're all set!
With setuptools adding vectorbtpro as a dependency to your Python package can be done by listing it in setup.py or in your requirements files:
Of course, you can pull vectorbtpro directly from git:
Install the package:
The command above takes around 1GB of disk space, to create a shallow clone:
To convert the clone back into a complete one:
Using Docker is a great way to get up and running in a few minutes, as it comes with all dependencies pre-installed.
Docker image of vectorbtpro is based on Jupyter Docker Stacks - a set of ready-to-run Docker images containing Jupyter applications and interactive computing tools. Particularly, the image is based on jupyter/scipy-notebook, which includes a minimally-functional JupyterLab server and preinstalled popular packages from the scientific Python ecosystem, and extends it with Plotly and Dash for interactive visualizations and plots, and vectorbtpro and most of its optional dependencies. The image requires the source of vectorbtpro to be available in the current depository.
Before proceeding, make sure to have Docker installed.
Launch Docker using Docker Desktop.
Clone the vectorbtpro repository (if not already). Run this from a directory where you want vectorbtpro to reside, for example, in Documents/GitHub:
Go into the directory:
Build the image (can take some time):
Create a working directory inside the current directory:
Start a container running a JupyterLab Server on the port 8888:
Info
The use of the -v flag in the command mounts the current working directory on the host ({PWD/work} in the example command) as /home/jovyan/work in the container. The server logs appear in the terminal. Due to the usage of the flag --rm Docker automatically cleans up the container and removes the file system when the container exits, but any changes made to the ~/work directory and its files in the container will remain intact on the host. The -it flag allocates pseudo-TTY.
Alternatively, if the port 8888 is already in use, use another port (here 10000):
Once the server has been launched, visit its address in a browser. The address is printed in the console, for example: http://127.0.0.1:8888/lab?token=9e85949d9901633d1de9dad7a963b43257e29fb232883908
Note
Change the port if necessary.
This will open JupyterLab where you can create a new notebook and start working with vectorbtpro 
To make use of any files on the host, put them into to the working directory work on the host and they will appear in the file browser of JupyterLab. Alternatively, you can drag and drop them directly into the file browser of JupyterLab.
To stop the container, first hit Ctrl+C, and then upon prompt, type y and hit Enter
To upgrade the Docker image to a new version of vectorbtpro, first, update the local version of the repository from the remote:
Then, rebuild the image:
Info
This won't rebuild the entire image, only the vectorbtpro installation step.
In case of connectivity issues, the package can be also installed manually:
To install a custom release:
Replace filename with the actual file name.
Note
If the file name ends with (1) because there's already a file with the same name, make sure to remove the previous file and remove the (1) suffix from the newer one.
If you're getting the error "ModuleNotFoundError: No module named 'pybind11'", install pybind11 prior to the installation of vectorbtpro:
If you're getting the error "Cannot uninstall 'llvmlite'", install llvmlite prior to the installation of vectorbtpro:
If image generation hangs (such as when calling show_svg()), downgrade the Kaleido package:
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
All notable changes in reverse chronological order.
Note
In your code, replace RowIdx to rowidx, ColIdx to colidx, RowPoints to pointidx, RowRanges to rangeidx, and ElemIdx to idx.
Example
If you previously used pd.Index([0.1, 0.2]) to test multiple values, now use vbt.Param([0.1, 0.2]).
Note
Reconstruction of free cash would only yield the same result as during the simulation if leverage is disabled. For enabled leverage, use fill_state to pre-compute the array.
Example
Previously, if a list ["longonly", "shortonly", "both"] was applied per column and could be used to test multiple position directions, now the same list will be applied per row, thus use [["longonly", "shortonly", "both"]]
Warning
If you have any vectorbt objects pickled by the previous versions of vectorbt, they won't be unpickled by the newer version. It's advised to either recalculate them using the newer version, or first unpickle them using the previous version, save their components separately, and then import and connect them using the newer version. Ping me if you meet any issues.
Info
This section briefly describes major changes made to the open-source version. For more details, see commits.
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
On top of the features offered by the open-source vectorbt, VectorBT PRO implements many major enhancements and optimizations in the following areas:
Info
To keep the pages short, only some of the most interesting features of each release are showcased. The detailed release notes are available to subscribers only. If you're on the private website, go to Getting started → Release notes.
Tags are releases where features were introduced for the first time. Note that most features are continuously updated, thus the following examples are meant to be run with the latest vectorbt PRO version installed 
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
One of the main powers of vectorbt (PRO) is the ability to create and backtest numerous strategy configurations in the blink of an eye. In this introductory example, we will explore how profitable is the following RSI strategy commonly used by beginners:
If the RSI is less than 30, it indicates a stock is reaching oversold conditions and may see a trend reversal, or bounceback, towards a higher share price. Once the reversal is confirmed, a buy trade is placed. Conversely, if the RSI is more than 70, it indicates that a stock is reaching an overbought condition and may see a trend reversal, or pullback, in price. After a confirmation of the reversal, a sell trade is placed.
As a bonus, we will gradually expand the analysis towards multiple parameter combinations. Sounds fun? Let's start. 
First, we will take care of data. Using a one-liner, we will download all available daily data for the pair BTC/USDT from Binance:
 
100%

The returned object is of type BinanceData, which extends Data to communicate with the Binance API. The class Data is a vectorbt's in-house container for retrieving, storing, and managing data. Upon receiving a DataFrame, it post-processes and stores the DataFrame inside the dictionary Data.data keyed by pair (also referred to as a "symbol" in vectorbt). We can get our DataFrame either from this dictionary, or by using the convenient method Data.get, which also allows for specifying one or more columns instead of returning the entire DataFrame at once.
Let's plot the data with Data.plot:

Another way to describe the data is by using the Pandas' info method. The tabular format is especially useful for counting null values (which our data apparently doesn't have - good!)
In our example, we will generate signals based on the opening price and execute them based on the closing price. We can also place orders a soon as the signal is generated, or at any later time, but we will illustrate how to separate generation of signals from their execution.
It's time to run the indicator!
VectorBT PRO supports 5 (!) different implementations of RSI: one implemented using Numba, and the other four ported from three different technical analysis libraries. Each indicator has been wrapped with the almighty IndicatorFactory 
To list all the available indicators or to search for a specific indicator, we can use IndicatorFactory.list_indicators:
We can then retrieve the actual indicator class as follows:
Or manually:
Here's a rule of thumb on which implementation to choose:
To run any indicator, use the method run. To see what arguments the method accepts, pass it to phelp:
As we can see above, we need to at least provide close, which can be any numeric time series. Also, by default, the rolling window is 14 bars long and uses the Wilder's smoothed moving average. Since we want to make decisions based on the opening price, we will pass open_price as close:
That's all! By executing the method RSI.run, we calculated the RSI values and have received an instance with various methods and properties for their analysis. To retrieve the resulting Pandas object, we need to query the rsi attribute (see "Outputs" in the output of phelp).
Having the RSI array, we now want to generate an entry signal whenever any RSI value crosses below 30 and an exit signal whenever any RSI value crosses above 70:
The same can be done using the methods RSI.rsi_crossed_below and RSI.rsi_crossed_above that were auto-generated for the output rsi by IndicatorFactory:
Hint
If you are curious what else has been generated, print dir(rsi) or look into the API generated for the class.
Before we proceed with the portfolio modeling, let's plot the RSI and signals to ensure that we did everything right:

The graph looks legit. But notice how there are multiple entries between two exits and vice versa? How does vectorbt handle it? When using Portfolio.from_signals, vectorbt will automatically filter out all entry signals if the position has already been entered, and exit signals if the position has already been exited. But to make our analysis cleaner, let's keep each first signal:

We can immediately see the difference. But what other methods exist to analyze the distribution of signals? How to quantify such analysis? That's what vectorbt is all about. Let's compute various statistics of clean_entries and clean_exits using SignalsAccessor:
We are ready for modeling! We will be using the class method Portfolio.from_signals, which will receive the signal arrays, process each signal one by one, and generate orders. It will then create an instance of Portfolio that can be used to assess the performance of the strategy.
Our experiment is simple: buy $100 of Bitcoin upon an entry signal and close the position upon an exit signal. Start with an infinite capital to not limit our buying power at any time.
Info
Running the method above for the first time may take some time as it must be compiled first. Compilation will take place each time a new combination of data types is discovered. But don't worry: Numba caches most compiled functions and re-uses them in each new runtime.
Hint
If you look into the API of Portfolio.from_signals, you will find many arguments to be set to None. The value None has a special meaning that instructs vectorbt to pull the default value from the global settings. You can discover all the default values for the Portfolio class here.
Let's print the statistics of our portfolio:
Hint
That are lots of statistics, right? If you're looking for the way they are implemented, print pf.metrics and look for the calc_func argument of the metric of interest. If some function is a lambda, look into the source code to reveal its contents.
Our strategy is not too bad: the portfolio has gained over 71% in profit over the last years, but holding Bitcoin is still better - staggering 450%. Despite the Bitcoin's high volatility, the minimum recorded portfolio value sits at $97 from $100 initially invested. The total time exposure of 38% means that we were in the market 38% of the time. The maximum gross exposure of 100% means that we invested 100% of our available cash balance, each single trade. The maximum drawdown (MDD) of 46% is the maximum distance our portfolio value fell after recording a new high (stop loss to the rescue?). 
The total number of orders matches the total number of (cleaned) signals, but why is the total number of trades suddenly 8 instead of 15? By default, a trade in the vectorbt's universe is a sell order; as soon as an exit order has been filled (by reducing or closing the current position), the profit and loss (PnL) based on the weighted average entry and exit price is calculated. The win rate of 70% means that 70% of the trades (sell orders) generated a profit, with the best trade bringing 54% in profit and the worst one bringing 32% in loss. Since the average winning trade generating more profit than the average losing trade generating loss, we can see various metrics being positive, such as the profit factor and the expectancy.

Hint
A benefit of an interactive plot like above is that you can use tools from the Plotly toolbar to draw a vertical line that connects orders, their P&L, and how they affect the cumulative returns. Try it out!
So, how do we improve from here?
Even such a basic strategy as ours has many potential parameters:
To make our analysis as flexible as possible, we will write a function that lets us specify all of that information, and return a subset of statistics:
Note
We removed the signal cleaning step because it makes no difference when signals are passed to Portfolio.from_signals (which cleans the signals automatically anyway).
By raising the upper threshold to 80% and lowering the lower threshold to 20%, the number of trades has decreased to just 2 because it becomes more difficult to cross the thresholds. We can also observe how the total return fell to roughly 7% - not a good sign. But how do we actually know whether this negative result indicates that our strategy is trash and not because of a pure luck? Testing one parameter combination from a huge space usually means making a wild guess.
Let's generate multiple parameter combinations for thresholds, simulate them, and concatenate their statistics for further analysis:
We just simulated 121 different combinations of the upper and lower threshold and stored their statistics inside a list. In order to analyze this list, we need to convert it to a DataFrame first, with metrics arranged as columns:
But how do we know which row corresponds to which parameter combination? We will build a MultiIndex with two levels, lower_th and upper_th, and make it the index of comb_stats_df:
Much better! We can now analyze every piece of the retrieved information from different angles. Since we have the same number of lower and upper thresholds, let's create a heatmap with the X axis reflecting the lower thresholds, the Y axis reflecting the upper thresholds, and the color bar reflecting the expectancy:

We can explore entire regions of parameter combinations that yield positive or negative results.
As you might have read in the documentation, vectorbt loves processing multidimensional data. In particular, it's built around the idea that you can represent each asset, period, parameter combination, and a backtest in general, as a column in a two-dimensional array.
Instead of computing everything in a loop (which isn't too bad but usually executes magnitudes slower than a vectorized solution) we can change our code to accept parameters as arrays. A function that takes such array will automatically convert multiple parameters into multiple columns. A big benefit of this approach is that we don't have to collect our results, put them in a list, and convert into a DataFrame - it's all done by vectorbt!
First, define the parameters that we would like to test:
Instead of applying itertools.product, we will instruct various parts of our pipeline to build a product instead, so we can observe how each part affects the column hierarchy.
The RSI part is easy: we can pass param_product=True to build a product of windows and wtypes and run the calculation over each column in open_price:
We see that RSI appended two levels to the column hierarchy: rsi_window and rsi_wtype. Those are similar to the ones we created manually for thresholds in Using for-loop. There are now 39 columns in total, which is just len(open_price.columns) x len(windows) x len(wtypes).
The next part are crossovers. In contrast to indicators, they are regular functions that take any array-like object, broadcast it to the rsi array, and search for crossovers. The broadcasting step is done using broadcast, which is a very powerful function for bringing multiple arrays to a single shape (learn more about broadcasting in the documentation).
In our case, we want to build a product of lower_ths, upper_th_index, and all columns in rsi. Since both rsi_crossed_below and rsi_crossed_above are two different functions, we need to build a product of the threshold values manually and then instruct each crossover function to combine them with every column in rsi:
We have produced over 4719 columns - madness! But did you notice that entries and exits have different columns now? The first one has lower_th as one of the column levels, the second one has upper_th. How are we supposed to pass differently labeled arrays (including close_price with one column) to Portfolio.from_signals?
No worries, vectorbt knows exactly how to merge this information. Let's see:
Congrats! We just backtested 4719 parameter combinations in less than a second 
Important
Even though we gained some unreal performance, we need to be careful to not occupy the entire RAM with our wide arrays. We can check the size of any Pickleable instance using Pickleable.getsize. For example, to print the total size of our portfolio in a human-readable format:
Even though the portfolio holds about 10 MB of compressed data, it must generate many arrays, such as the portfolio value, that have the same shape as the number of timestamps x parameter combinations:
We can see that each floating array occupies 65 MB of memory. By creating a dozen of such arrays (which is often the worst case), the memory consumption may jump to 1 GB very quickly.
One option is to use Pandas itself to analyze the produced statistics. For example, calculate the mean expectancy of each rsi_window:
The longer is the RSI window, the higher is the mean expectancy.
Display the top 5 parameter combinations:
To analyze any particular combination using vectorbt, we can select it from the portfolio the same way as we selected a column in a regular Pandas DataFrame. Let's plot the equity of the most successful combination:

Hint
Instead of selecting a column from a portfolio, which will create a new portfolio with only that column, you can also check whether the method you want to call supports the argument column and pass your column using this argument. For instance, we could have also used pf.plot_value(column=(22, 80, 20, "wilder")).
Even though, in theory, the best found setting doubles our money, it's still inferior to simply holding Bitcoin - our basic RSI strategy cannot beat the market 
But even if it did, there is much more to just searching for right parameters: we need at least to (cross-) validate the strategy. We can also observe how the strategy behaves on other assets. Curious how to do it? Just expand open_price and close_price to contain multiple assets, and each example would work out-of-the-box!
 
100%

Your homework is to run the examples on this data.
The final columns should become as follows:
We see that the column hierarchy now contains another level - symbol - denoting the asset. Let's visualize the distribution of the expectancy across both assets:

ETH seems to react more aggressively to our strategy on average than BTC, maybe due to the market's higher volatility, a different structure, or just pure randomness.
And here's one of the main takeaways of such analysis: using strategies with simple and explainable mechanics, we can try to explain the mechanics of the market itself. Not only can we use this to improve ourselves and design better indicators, but use this information as an input to ML models, which are better at connecting dots than humans. Possibilities are endless!
VectorBT PRO is a powerful vehicle that enables us to discover uncharted territories faster and analyze them in more detail. Instead of using overused and outdated charts and indicators from books and YouTube videos, we can build our own tools that go hand in hand with the market. We can backtest thousands of strategy configurations to learn how the market reacts to each one of them - in a matter of milliseconds. All it takes is creativity 
 Python code  Notebook
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
VectorBT PRO was implemented to address common performance shortcomings of backtesting libraries. It builds upon the idea that each instance of a trading strategy can be represented in a vectorized form, so multiple strategy instances can be packed into a single multi-dimensional array, processed in a highly efficient manner, and analyzed easily.
Thanks to the time-series nature of trading data, most of the aspects related to backtesting can be translated into arrays. In particular, vectorbt operates on NumPy arrays, which are very fast due to optimized, pre-compiled C code. NumPy arrays are supported by numerous scientific packages in the vibrant Python ecosystem, such as Pandas, NumPy, and Numba. There is a great chance that you already used some of those packages!
While NumPy excels at performance, it's not necessarily the most intuitive package for time series analysis. Consider the following moving average using NumPy:
While it might be ultrafast, it takes time for the user to understand what is going on and also some mastery to write such vectorized code without bugs. What about other rolling functions that are powering more complex indicators? And what about resampling, grouping, and other operations on dates and time?
Here comes Pandas to the rescue! Pandas provides rich time series functionality, data alignment, NA-friendly statistics, groupby, merge and join methods, and lots of other conveniences. It has two primary data structures: Series (one-dimensional) and DataFrame (two-dimensional). You can imagine them as NumPy arrays wrapped with valuable information, such as timestamps and column names. Our moving average can be implemented in a one-liner:
VectorBT PRO relies heavily upon Pandas, but not in the way you think. Pandas has one big drawback for our use case: it's slow for bigger datasets and custom-defined functions. Many functions such as the rolling mean are implemented using Cython under the hood and are sufficiently fast. But once you try to implement a more complex function, such as some rolling ranking metric based on multiple time series, things are becoming complicated and slow. In addition, what about functions that cannot be vectorized? A portfolio strategy involving money management cannot be simulated directly using vector calculations. We arrived at a point where we need to write a fast iterative code that processes data in an element-by-element fashion.
What if I told you that there exists a Python package that lets you run for-loops at machine code speed? And that it understands NumPy well and doesn't require adapting Python code much? It would solve many of our problems: our code could suddenly become incredibly fast while staying perfectly readable. This package is Numba. Numba translates a subset of Python and NumPy code into fast machine code.
We can now clearly understand what is going on: we iterate over our time series one timestamp at a time, check whether there is enough data in the window, and if there is, we take the mean of it. Not only Numba is great for writing a human-readable and less error-prone code, it's also as fast as C!
Hint
If you're interested in how vectorbt uses Numba, just look at any directory or file with the name nb. This sub-package implements all the basic functions, while this module implements some hard-core stuff ( adults only).
So where is the caveat? Sadly, Numba only understands NumPy, but not Pandas. This leaves us without datetime index and other features so crucial for time series analysis. And that's where vectorbt comes into play: it replicates many Pandas functions using Numba and even adds some interesting features to them. This way, we not only make a subset of Pandas faster, but also more powerful! 
This is done as follows:
Or using vectorbt:
Notice how vbt is attached directly to the Series object? This is called an accessor - a convenient way to extend Pandas objects without subclassing them. Using an accessor we can easily switch between native Pandas and vectorbt functionality. Moreover, each vectorbt method is flexible towards inputs and can work on both Series and DataFrames.
You can learn more about vectorbt's accessors here. For instance, rolling_mean is part of the accessor GenericAccessor, which can be accessed directly using vbt. Another popular accessor ReturnsAccessor for processing returns is a subclass of GenericAccessor and can be accessed using vbt.returns.
Important
Each accessor expects the data to be in the ready-to-use format. This means that the accessor for working with returns expects the data to be returns, not the price!
Remember when we mentioned that vectorbt differs from traditional backtesters by taking and processing trading data as multi-dimensional arrays? In particular, vectorbt treats each column as a separate backtesting instance rather than a feature. Consider a simple OHLC DataFrame:
Here, columns are separate features describing the same abstract object - price. While it may appear intuitive to pass this DataFrame to vectorbt (as you may have done with scikit-learn and other ML tools, which expect DataFrames with features as columns), this approach has several key drawbacks in backtesting:
VectorBT PRO addresses this heterogeneity of features by processing them as separate arrays. So instead of passing one big DataFrame, we need to provide each feature independently:
Now, in case when we want to process multiple abstract objects, such as ticker symbols, we can simply pass DataFrames instead of Series:
Here, each column (also often referred as "line" in vectorbt) in each feature DataFrame represents a separate backtesting instance and generates a separate equity curve. Thus, adding one more backtest is as simple as adding one more column to the features 
Keeping features separated has another big advantage: we can combine them easily. And not only this: we combine all backtesting instances at once using vectorization. Consider the following example where we place an entry signal whenever the previous candle was green and an exit signal whenever the previous candle was red (which is pretty dumb but anyway):
The Pandas objects multi_close and multi_open can be Series and DataFrames of arbitrary shapes and our micro-pipeline will still work as expected.
In the example above, we created our multi-OHLC DataFrames with two columns - p1 and p2 - so we can easily identify them later during the analysis phase. For this reason, vectorbt ensures that those columns are preserved across the whole backtesting pipeline - from signal generation to performance modeling.
But what if individual columns corresponded to more complex configurations, such as those involving multiple hyperparameter combinations? Storing complex objects as column labels wouldn't work after all. Thanks to Pandas, there are hierarchical columns, which are just like regular columns but stacked upon each other. Each level of such hierarchy can help us to identify a specific input or parameter.
Take a simple crossover strategy as an example: it depends upon the lengths of the fast and slow windows. Each of these hyperparameters becomes an additional dimension for manipulating data and gets stored as a separate column level. Below is a more complex example of the column hierarchy of a MACD indicator:
The columns above capture two different backtesting configurations that can now be easily analyzed and compared using Pandas - a very powerful technique to analyze data. We may, for example, consider grouping our performance by macd_fast_window to see how the size of the fast window impacts the profitability of our strategy. Isn't this magic?
One of the most important concepts in vectorbt is broadcasting. Since vectorbt functions take time series as independent arrays, they need to know how to connect the elements of those arrays such that there is 1) complete information, 2) across all arrays, and 3) at each time step.
If all arrays are of the same size, vectorbt can easily perform any operation on an element-by-element basis. Whenever any of the arrays is of a smaller size though, vectorbt looks for a possibility to "stretch" it such that it can match the length of other arrays. This approach is heavily inspired by (and internally based upon) NumPy's broadcasting. The only major difference to NumPy is that one-dimensional arrays always broadcast along rows since we're working primarily with time series data.
Why should we care about broadcasting? Because it allows us to pass array-like objects of any shape to almost every function in vectorbt, be it constants or full-blown DataFrames, and vectorbt will automatically figure out where the respective elements belong to.
Hint
As a rule of thumb:
In contrast to NumPy and Pandas, vectorbt knows how to broadcast labels: in case where columns or individual column levels in both objects are different, they are stacked upon each other. Consider checking whenever the fast moving average is higher than the slow moving average, using the following window combinations: (2, 3) and (3, 4).
Hint
Appending .vbt to a Pandas object on the left will broadcast both operands with vectorbt and execute the operation with NumPy/Numba - ultimate combo 
In contrast to Pandas, vectorbt broadcasts rows and columns by their absolute positions, not labels. This broadcasting style is very similar to that of NumPy:
Important
If you pass multiple arrays of data to vectorbt, ensure that their columns connect well positionally!
In case your columns are not properly ordered, you will notice this by the result having multiple column levels with identical labels but different ordering.
Another feature of vectorbt is that it can broadcast objects with incompatible shapes but overlapping multi-index levels - those having the same name or values. Continuing with the previous example, check whenever the fast moving average is higher than the price:
And here comes more (bear with me): we can easily test multiple scalar-like hyperparameters by passing them as a Pandas Index. Let's compare whether the price is within thresholds:
As you see, smart broadcasting is  when it comes to merging information. See broadcast to learn more about broadcasting principles and new exciting techniques to combine arrays.
Broadcasting many big arrays consumes a lot of RAM and ultimately makes processing slower. That's why vectorbt introduces a concept of "flexible indexing", which does selection of one element out of a one-dimensional or two-dimensional array of an arbitrary shape. For example, if a one-dimensional array has only one element, and it needs to broadcast along 1000 rows, vectorbt will return that one element irrespective of the row being queried since this array would broadcast against any shape:
This is equivalent to the following:
Two-dimensional arrays have more options. Consider an example where we want to process 1000 columns, and we have a plenty of parameters that should apply per each element. Some parameters may be scalars that are the same for each element, some may be one-dimensional arrays that are the same for each column, and some may be the same for each row. Instead of broadcasting them fully, we can keep the number of their elements and just expand them to two dimensions in a way that would broadcast them nicely using NumPy:
A nice feature of this is that such an operation has almost no additional memory footprint and can broadcast in any direction infinitely - an open secret to how Portfolio.from_signals manages to broadcast more than 50 arguments without losing any memory or performance 
 Python code
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
Base class for working with data sources.
Base mixin class for working with data.
Subclasses
Assert that feature exists.
Assert that symbol exists.
Column wrapper.
List of features.
Get one or more features of one or more symbols of data.
Get feature that match a feature index or label.
Return the index of a feature.
Get symbol that match a symbol index or label.
Return the index of a symbol.
Whether feature exists.
Check whether there are one or multiple keys.
Whether symbol exists.
Select one or more features by index.
Returns a new instance.
Select one or more features.
Returns a new instance.
Select one or more symbols by index.
Returns a new instance.
Select one or more symbols.
Returns a new instance.
Symbol wrapper.
List of symbols.
Class that downloads, updates, and manages data coming from a data source.
Superclasses
Inherited members
Subclasses
Align data to have the same columns.
See Data.align_index() for missing.
Align data.
Removes any index duplicates, prepares the datetime index, and aligns the index and columns.
Align data to have the same index.
The argument missing accepts the following values:
For defaults, see data.
Build feature config documentation.
Check whether the argument conforms to a data dictionary.
Key classes.
Stack multiple Data instances along columns.
Uses ArrayWrapper.column_stack() to stack the wrappers.
Return the column type.
Columns.
Based on the default symbol wrapper.
Concatenate keys along columns.
Data dictionary.
Has the type feature_dict for feature-oriented data or symbol_dict for symbol-oriented data.
Delisted flag per symbol of type symbol_dict.
Return the dict type.
Feature classes.
Column config of Data.
Returns Data._feature_config, which gets (hybrid-) copied upon creation of each instance. Thus, changing this config won't affect the class.
To change fields, you can either change the config in-place, override this property, or overwrite the instance variable Data._feature_config.
Whether data has features as keys.
Exists for backward compatibility. Use Data.pull() instead.
Fetch a feature.
Can also return a dictionary that will be accessible in Data.returned_kwargs. If there are keyword arguments tz_localize, tz_convert, or freq in this dict, will pop them and use them to override global settings.
This is an abstract method - override it to define custom logic.
Keyword arguments of type symbol_dict initially passed to Data.fetch_symbol().
Fetch a symbol.
Can also return a dictionary that will be accessible in Data.returned_kwargs. If there are keyword arguments tz_localize, tz_convert, or freq in this dict, will pop them and use them to override global settings.
This is an abstract method - override it to define custom logic.
Fix dict type for data.
Fix dict types in keyword arguments.
Frequency.
Based on the default symbol wrapper.
Use CSVData to load data from CSV and switch the class back to this class.
Use fetch_kwargs to provide keyword arguments that were originally used in fetching.
Create a new Data instance from data.
Args
For defaults, see data.
Parse a Data instance from a string.
For example: YFData:BTC-USD or just BTC-USD where the data class is YFData by default.
Use DuckDBData to load data from a DuckDB database and switch the class back to this class.
Use fetch_kwargs to provide keyword arguments that were originally used in fetching.
Use FeatherData to load data from Feather and switch the class back to this class.
Use fetch_kwargs to provide keyword arguments that were originally used in fetching.
Use HDFData to load data from HDF and switch the class back to this class.
Use fetch_kwargs to provide keyword arguments that were originally used in fetching.
Use ParquetData to load data from Parquet and switch the class back to this class.
Use fetch_kwargs to provide keyword arguments that were originally used in fetching.
Use SQLData to load data from a SQL database and switch the class back to this class.
Use fetch_kwargs to provide keyword arguments that were originally used in fetching.
CustomData.get_setting with path_id="base".
CustomData.get_settings with path_id="base".
Get wrapper with features as columns.
Get wrapper with keys as columns.
If attach_classes is True, attaches Data.classes by stacking them over the keys using stack_indexes().
Other keyword arguments are passed to the constructor of the wrapper.
Get keys depending on the provided dict type.
Get wrapper with symbols as columns.
CustomData.has_setting with path_id="base".
CustomData.has_settings with path_id="base".
Check whether the argument contains any data dictionary.
Index.
Based on the default symbol wrapper.
Perform indexing on Data.
Invert data and return a new instance.
Invert data by swapping keys and columns.
Key wrapper.
Keys in data.
Features if feature_dict and symbols if symbol_dict.
Last fetched index per symbol of type symbol_dict.
Level name(s) for keys.
Keys are symbols or features depending on the data dict type.
Must be a sequence if keys are tuples, otherwise a hashable. If False, no level names will be used.
Merge multiple Data instances.
Can merge both symbols and features. Data is overridden in the order as provided in datas.
Metrics supported by Data.
Returns Data._metrics, which gets (hybrid-) copied upon creation of each instance. Thus, changing this config won't affect the class.
To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Data._metrics.
Argument missing passed to Data.align_columns().
Argument missing passed to Data.align_index().
Number of dimensions.
Based on the default symbol wrapper.
Call this method on each subclass that overrides Data.feature_config.
Plot either one feature of multiple symbols, or OHLC(V) of one symbol.
Args
Name of the feature or symbol to plot.
Depends on the data orientation.
Dictionary mapping the feature names to OHLCV.
Applied only if OHLC(V) is plotted.
Whether to plot volume beneath.
Applied only if OHLC(V) is plotted.
Rebase all series of a feature to a given initial base.
Note
The feature must contain prices.
Applied only if lines are plotted.
Usage
 
100%



Defaults for PlotsBuilderMixin.plots().
Merges PlotsBuilderMixin.plots_defaults and plots from data.
Prepare datetime index and columns.
If parse_dates is True, will try to convert any index and column with object data type into a datetime format using prepare_dt_index(). If parse_dates is a list or dict, will first check whether the name of the column is among the names that are in parse_dates.
If to_utc is True or to_utc is "index" or to_utc is a sequence and index name is in this sequence, will localize/convert any datetime index to the UTC timezone. If to_utc is True or to_utc is "columns" or to_utc is a sequence and column name is in this sequence, will localize/convert any datetime column to the UTC timezone.
Prepare datetime column.
See Data.prepare_dt_index() for arguments.
Prepare datetime index.
If parse_dates is True, will try to convert the index with an object data type into a datetime format using prepare_dt_index().
If tz_localize is not None, will localize a datetime-naive index into this timezone.
If tz_convert is not None, will convert a datetime-aware index into this timezone. If force_tz_convert is True, will convert regardless of whether the index is datetime-aware.
Prepare a timezone-aware index of a Pandas object.
Uses Data.prepare_dt_index() with parse_dates=True and force_tz_convert=True.
For defaults, see data.
Pull data.
Fetches each feature/symbol with Data.fetch_feature()/Data.fetch_symbol() and prepares it with Data.from_data().
Iteration over features/symbols is done using execute(). That is, it can be distributed and parallelized when needed.
Args
One or multiple keys.
Depending on keys_are_features will be set to features or symbols.
One or multiple features.
If provided as a dictionary, will use keys as features and values as keyword arguments.
Note
Tuple is considered as a single feature (tuple is a hashable).
One or multiple symbols.
If provided as a dictionary, will use keys as symbols and values as keyword arguments.
Note
Tuple is considered as a single symbol (tuple is a hashable).
See Data.classes.
Can be a hashable (single value), a dictionary (class names as keys and class values as values), or a sequence of such.
Note
Tuple is considered as a single class (tuple is a hashable).
Whether to silence all warnings.
Will also forward this argument to Data.fetch_feature()/Data.fetch_symbol() if in the signature.
Passed to Data.fetch_feature()/Data.fetch_symbol().
If two features/symbols require different keyword arguments, pass key_dict or feature_dict/symbol_dict for each argument.
For defaults, see data.
Perform realigning on Data.
Looks for realign_func of each feature in Data.feature_config. If no function provided, resamples feature "open" with GenericAccessor.realign_opening() and other features with GenericAccessor.realign_closing().
Rename keys using rename dict that maps old keys to new keys.
Applies to symbols if data has the type symbol_dict and features if feature_dict.
Rename keys in a dict.
See Configured.replace().
Replaces the data's index and/or columns if they were changed in the wrapper.
Perform resampling on Data.
Features "open", "high", "low", "close", "volume", "trade count", and "vwap" (case-insensitive) are recognized and resampled automatically.
Looks for resample_func of each feature in Data.feature_config. The function must accept the Data instance, object, and resampler.
CustomData.resolve_setting with path_id="base".
Resolve argument.
Resolve metadata for keys.
Keyword arguments of type symbol_dict returned by Data.fetch_symbol().
Stack multiple Data instances along rows.
Uses ArrayWrapper.row_stack() to stack the wrappers.
Run a function on data.
Looks into the signature of the function and searches for arguments with the name data or those found among features or attributes.
For example, the argument open will be substituted by Data.open.
func can be one of the following:
Use rename_args to rename arguments. For example, in Portfolio, data can be passed instead of close.
Set unpack to True, "dict", or "frame" to use IndicatorBase.unpack(), IndicatorBase.to_dict(), and IndicatorBase.to_frame() respectively.
Any argument in *args and **kwargs can be wrapped with run_func_dict/run_arg_dict to specify the value per function/argument name or index when func is iterable.
Multiple function calls are executed with execute().
Create a new Data instance with one or more keys from this instance.
Applies to features if data has the type feature_dict and symbols if symbol_dict.
Select a feature or symbol from Data.classes.
Select a feature or symbol from Data.delisted.
Select the dictionary value belonging to a feature.
Select the keyword arguments belonging to a feature.
Select a feature or symbol from Data.fetch_kwargs.
Select keys from a dict.
Select the dictionary value belonging to a feature or symbol.
Select the keyword arguments belonging to a feature or symbol.
Select a feature or symbol from Data.last_index.
Select a feature or symbol from Data.returned_kwargs.
Select the dictionary value belonging to a symbol.
Select the keyword arguments belonging to a symbol.
CustomData.set_settings with path_id="base".
Shape.
Based on the default symbol wrapper.
Shape as if the object was two-dimensional.
Based on the default symbol wrapper.
Whether there is only one feature in Data.data.
Whether there is only one key in Data.data.
Whether there is only one symbol in Data.data.
Run a SQL query on this instance using DuckDB.
First, connection gets established. Then, Data.get() gets invoked with **kwargs passed as keyword arguments and as_dict=True. Then, each returned object gets registered within the database. Finally, the query gets executed with duckdb.sql and the relation as a DataFrame gets returned. If squeeze is True, a DataFrame with one column will be converted into a Series.
Defaults for StatsBuilderMixin.stats().
Merges StatsBuilderMixin.stats_defaults and stats from data.
Subplots supported by Data.
Returns Data._subplots, which gets (hybrid-) copied upon creation of each instance. Thus, changing this config won't affect the class.
To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Data._subplots.
Switch the class of the data instance.
Symbol classes.
Whether data has symbols as keys.
Save data to CSV file(s).
Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html
Any argument can be provided per feature using feature_dict or per symbol using symbol_dict, depending on the format of the data dictionary.
If path_or_buf is a path to a directory, will save each feature/symbol to a separate file. If there's only one file, you can specify the file path via path_or_buf. If there are multiple files, use the same argument but wrap the multiple paths with key_dict.
Save data to a DuckDB database.
Any argument can be provided per feature using feature_dict or per symbol using symbol_dict, depending on the format of the data dictionary.
If connection is None or a string, will resolve a connection with DuckDBData.resolve_connection(). It can additionally return the connection if return_connection is True or entire metadata (all passed arguments as feature_dict or symbol_dict). In this case, the engine won't be disposed by default.
If write_format is None and write_path is a directory (default), will persist each feature/symbol to a table (see https://duckdb.org/docs/guides/python/import_pandas). If catalog is not None, will make it default for this connection. If schema is not None, and it doesn't exist, will create a new schema in the current catalog and make it default for this connection. Any new table will be automatically created under this schema.
If if_exists is "fail", will raise an error if a table with the same name already exists. If if_exists is "replace", will drop the existing table first. If if_exists is "append", will append the new table to the existing one.
If write_format is not None, it must be either "csv", "parquet", or "json". If write_path is a directory or has no suffix (meaning it's not a file), each feature/symbol will be saved to a separate file under that path and with the provided write_format as extension. The data will be saved using a COPY mechanism (see https://duckdb.org/docs/sql/statements/copy.html). To provide options to the write operation, pass them as a dictionary or an already formatted string (without brackets). For example, dict(compression="gzip") is same as "COMPRESSION 'gzip'".
For to_utc and remove_utc_tz, see Data.prepare_dt(). If to_utc is None, uses the corresponding setting of DuckDBData.
Save data to Feather file(s) using PyArrow.
Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_feather.html
Any argument can be provided per feature using feature_dict or per symbol using symbol_dict, depending on the format of the data dictionary.
If path_or_buf is a path to a directory, will save each feature/symbol to a separate file. If there's only one file, you can specify the file path via path_or_buf. If there are multiple files, use the same argument but wrap the multiple paths with key_dict.
Convert this instance to the feature-oriented format.
Returns self if the data is already properly formatted.
Save data to an HDF file using PyTables.
Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html
Any argument can be provided per feature using feature_dict or per symbol using symbol_dict, depending on the format of the data dictionary.
If path_or_buf exists and it's a directory, will create inside it a file named after this class.
Save data to Parquet file(s) using PyArrow or FastParquet.
Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html
Any argument can be provided per feature using feature_dict or per symbol using symbol_dict, depending on the format of the data dictionary.
If path_or_buf is a path to a directory, will save each feature/symbol to a separate file. If there's only one file, you can specify the file path via path_or_buf. If there are multiple files, use the same argument but wrap the multiple paths with key_dict.
If partition_cols and partition_by are None, path_or_buf must be a file, otherwise it must be a directory. If partition_by is not None, will group the index by using ArrayWrapper.get_index_grouper() with **groupby_kwargs and put it inside partition_cols. In this case, partition_cols must be None.
Save data to a SQL database using SQLAlchemy.
Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html
Any argument can be provided per feature using feature_dict or per symbol using symbol_dict, depending on the format of the data dictionary.
Each feature/symbol gets saved to a separate table.
If engine is None or a string, will resolve an engine with SQLData.resolve_engine() and dispose it afterward if dispose_engine is None or True. It can additionally return the engine if return_engine is True or entire metadata (all passed arguments as feature_dict or symbol_dict). In this case, the engine won't be disposed by default.
If schema is not None and it doesn't exist, will create a new schema.
For to_utc and remove_utc_tz, see Data.prepare_dt(). If to_utc is None, uses the corresponding setting of SQLData.
Convert this instance to the symbol-oriented format.
Returns self if the data is already properly formatted.
Transform data.
If one key (i.e., feature or symbol), passes the entire Series/DataFrame. If per_feature is True, passes the Series/DataFrame of each feature. If per_symbol is True, passes the Series/DataFrame of each symbol. If both are True, passes each feature and symbol combination as a Series if pass_frame is False or as a DataFrame with one column if pass_frame is True. If both are False, concatenates all features and symbols into a single DataFrame and calls transform_func on it. Then, splits the data by key and builds a new Data instance. Keyword arguments key_wrapper_kwargs are passed to Data.get_key_wrapper() to control, for example, attachment of classes.
After the transformation, the new data is aligned using Data.align_data().
Note
The returned object must have the same type and dimensionality as the input object.
Number of columns (i.e., features and symbols) and their names must stay the same. To remove columns, use either indexing or Data.select() (depending on the data orientation). To add new columns, use either column stacking or Data.merge().
Index, on the other hand, can be changed freely.
Try to fetch a feature using Data.fetch_feature().
Try to fetch a symbol using Data.fetch_symbol().
Try to run a function on data.
Try to update a feature using Data.update_feature().
Try to update a symbol using Data.update_symbol().
Timezone to convert a datetime-aware to, which is initially passed to Data.pull().
Timezone to localize a datetime-naive index to, which is initially passed to Data.pull().
Update data.
Fetches new data for each feature/symbol using Data.update_feature()/Data.update_symbol().
Args
Whether to silence all warnings.
Will also forward this argument to Data.update_feature()/Data.update_symbol() if accepted by Data.fetch_feature()/Data.fetch_symbol().
Passed to Data.update_feature()/Data.update_symbol().
If two features/symbols require different keyword arguments, pass key_dict or feature_dict/symbol_dict for each argument.
Note
Returns a new Data instance instead of changing the data in place.
Update a feature.
Can also return a dictionary that will be accessible in Data.returned_kwargs.
This is an abstract method - override it to define custom logic.
Update Data.fetch_kwargs. Returns a new instance.
Update Data.returned_kwargs. Returns a new instance.
Update a symbol.
Can also return a dictionary that will be accessible in Data.returned_kwargs.
This is an abstract method - override it to define custom logic.
Copy feature config from another Data class.
Class exposes a read-only class property DataWithFeatures.field_config.
Subclasses
Column config of ${cls_name}.
Meta class that exposes a read-only class property StatsBuilderMixin.metrics.
Superclasses
Inherited members
Meta class that exposes a read-only class property MetaFeatures.feature_config.
Superclasses
Subclasses
Column config.
Mixin class for working with OHLC data.
Superclasses
Inherited members
Subclasses
Close.
OHLCDataMixin.get_daily_log_returns() with default arguments.
OHLCDataMixin.get_daily_returns() with default arguments.
OHLCDataMixin.get_drawdowns() with default arguments.
Daily log returns.
Daily returns.
Generate drawdown records.
See Drawdowns.
Log returns.
Returns.
Return accessor of type ReturnsAccessor.
Whether the instance has all the OHLC features.
Whether the instance has all the OHLCV features.
High.
HLC/3.
OHLCDataMixin.get_log_returns() with default arguments.
Low.
Return a OHLCDataMixin instance with the OHLC features only.
OHLC/4.
Return a OHLCDataMixin instance with the OHLCV features only.
Open.
OHLCDataMixin.get_returns() with default arguments.
OHLCDataMixin.get_returns_acc() with default arguments.
Trade count.
Volume.
VWAP.
Dict that contains features as keys.
Superclasses
Inherited members
Dict that contains features or symbols as keys.
Superclasses
Inherited members
Subclasses
Dict that contains argument names as keys for Data.run().
Superclasses
Inherited members
Dict that contains function names as keys for Data.run().
Superclasses
Inherited members
Dict that contains symbols as keys.
Superclasses
Inherited members
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
VectorBT PRO works on Pandas and NumPy arrays, but where those arrays are coming from? Getting the financial data manually is a challenging task, especially when an exchange can return only one bunch of data at a time such that iteration over time ranges, concatenation of results, and alignment of index and columns are effectively outsourced to the user. The task gets only trickier when multiple symbols are involved. 
To simplify and automate data retrieval and management, vectorbt implements the Data class, which allows seamless handling of features (such as OHLC) and symbols (such as "BTC-USD"). It's a semi-abstract class, meaning you have to subclass it and define your own logic at various places to be able to use its rich functionality to the full extent. Gladly, there is a collection of custom data classes already implemented for us, but it's always good to know how to create such a data class on our own.
The steps discussed below can be visualized using the following graph:
(Reload the page if the diagram doesn't show up)
Class Data implements an abstract class method Data.fetch_symbol for generating, loading, or fetching one symbol of data from any data source. It has to be overridden and implemented by the user, and return a single (Pandas or NumPy) array given some set of parameters, such as the starting date, the ending date, and the frequency.
Let's write a function that returns any symbol of data from Yahoo Finance using yfinance:
Info
Why the returned data starts from 2019-12-31 and not from 2020-01-01? The provided start and end dates are defined in the local timezone and then converted into UTC. In the Europe/Berlin timezone, depending upon the time of the year, 2020-01-01 gets translated into 2019-12-31 22:00:00, which is the date Yahoo Finance actually receives. To provide any date directly as a UTC date, append "UTC": 2020-01-01 UTC or construct a proper Timestamp instance.
Managing data in a Pandas format is acceptable when we are dealing with one symbol, but what about multiple symbols? Remember how vectorbt wants us to provide each of the open price, high price, and other features as separate variables? Each of those variables must have symbols laid out as columns, which means that we would have to manually fetch all symbols and properly reorganize their data layout. Having some symbols with different index or columns would just add to our headache. 
Luckily, there is a class method Data.pull that solves most of the issues related to iterating over, fetching, and merging symbols. It takes from one to multiple symbols, fetches each one with Data.fetch_symbol, puts it into a dictionary, and passes this dictionary to Data.from_data for post-processing and class instantiation.
Building upon our example, let's subclass Data and override the Data.fetch_symbol method to call our get_yf_symbol function:
Hint
You can replace get_yf_symbol with any other function that returns any array-like data!
That's it, YFData is now a full-blown data class capable of pulling data from Yahoo Finance and storing it:
 
Symbol 2/2

The pulled data is stored inside the Data.data dictionary with symbols being keys and values being Pandas objects returned by Data.fetch_symbol:
If Data.fetch_symbol returned None or an empty Pandas object or NumPy array, the symbol will be skipped entirely. Data.pull will also catch any exception raised in Data.fetch_symbol and skip the symbol if the argument skip_on_error is True (it's False by default!), otherwise, it will abort the procedure.
Generally, it's the task of Data.fetch_symbol to handle issues. Whenever there is a lot of data points to fetch and the fetcher relies upon a loop to concatenate different data bunches together, the best approach is to show the user a warning whenever an exception is thrown and return the data fetched up to the most recent point in time, similarly to how this was implemented in BinanceData and CCXTData. In such a case, vectorbt will replace the missing data points with NaN or drop them altogether, and keep track of the last index. You can then wait until your connection is stable and re-fetch the missing data using Data.update.
Along with the data, Data.fetch_symbol can also return a dictionary with custom keyword arguments acting as a context of the fetching operation. This context can later be accessed in the symbol dictionary Data.returned_kwargs. For instance, this context may include any information on why the fetching process failed, the length of the remaining data left to fetch, or which rows the fetched data represents when reading a local file (as implemented by CSVData for data updates).
Just for the sake of example, let's save the current timestamp:
Info
symbol_dict is a regular dictionary where information is grouped by symbol.
Like most classes that hold data, the class Data subclasses Analyzable, so we can perform Pandas indexing on the class instance itself to select rows and columns in all Pandas objects stored inside that instance. Doing a single Pandas indexing operation on multiple Pandas objects with different labels is impossible, so what happens if we fetched symbol data from different date ranges or with different columns? Whenever Data.pull passes the (unaligned) data dictionary to Data.from_data, it calls Data.align_data, which does the following:
Let's illustrate this workflow in practice:
 
Symbol 2/2

Notice how we ended up with the same index and columns across all Pandas objects. We can now use this data in any vectorbt function without fearing any indexing errors.
If some rows are present in one symbol and are missing in another, vectorbt will raise a warning with the text "Symbols have mismatching index". By default, the missing rows will be replaced by NaN. To drop them or raise an error instead, use the missing_index argument:
 
Symbol 2/2

Updating is a regular fetching operation that can be used both to update the existing data points and to add new ones. It requires specifying the first timestamp or row index of the update, and assumes that the data points prior to this timestamp or row index remain unchanged.
Similarly to Data.fetch_symbol, updating must be manually implemented by overriding a method Data.update_symbol. In contrast to the fetcher, the updater is an instance method and can access the data fetched earlier. For instance, it can access the keyword arguments initially passed to the fetcher, accessible in the symbol dictionary Data.fetch_kwargs. Those arguments can be used as default arguments or be overridden by any argument passed directly to the updater. Every data instance has also a symbol dictionary Data.last_index, which holds the last fetched index per symbol. We can use this index as the starting point of the next update.
Let's build a new YFData class that can also perform updates to the stored data:
Once the Data.update_symbol method is implemented, we can call the method Data.update to iterate over each symbol and update its data. Under the hood, this method also aligns the index and column labels of all the returned Pandas objects, appends the new data to the old data through concatenation along rows, and updates the last index of each symbol for the use in the next data update. Finally, it produces a new instance of Data by using Configured.replace.
Important
Updating data never overwrites the existing data instance but always returns a new instance. Remember that most classes in vectorbt are read-only to enable caching and avoid side effects.
First, we'll fetch the same data as previously:
 
Symbol 2/2

Even though both DataFrames end with the same date, our YFData instance knows that the BTC-USD symbol is 2 rows behind the ETH-USD symbol:
We can also access the keyword arguments passed to the initial fetching operation:
The start argument of each symbol will be replaced by its respective entry in Data.last_index, while the end argument can be overridden by any date that we specify during the update.
Note
Without specifying the end date, vectorbt will update only the latest data point of each symbol.
Let's update both symbols up to the same date:
Each symbol has been updated separately based on their last_index value: the symbol BTC-USD has received new rows ranging from 2020-01-02 to 2020-01-05, while the symbol ETH-USD has only received new rows between 2020-01-04 to 2020-01-05. We can now see that both symbols have been successfully synced up to the same ending date:
If the last index of the data update lies before the current last_index (that is, we want to update any data in the middle), all the data after the new last index will be disregarded:
Note
The last data point of an update is considered to be the most up-to-date point, thus no data stored previously can come after it.
By default, the returned data instance contains the whole data - the old data with the new data concatenated together. To return only the updated data, disable concat:
The returned data instance skips two timestamps: 2019-12-31 and 2020-01-01, which weren't changed during that update. But even though the symbol ETH-USD only received new rows between 2020-01-04 to 2020-01-05, it contains the old data for 2020-01-02 and 2020-01-03 as well, why so? Those timestamps were updated in the BTC-USD dataset, and because the index across all symbols must be aligned, we need to include some old data to avoid setting NaNs.
After the data has been fetched and a new Data instance has been created, getting the data is straight-forward using the Data.data dictionary or the method Data.get.
 
Symbol 2/2

Get all features of one symbol of data:
Get specific features of one symbol of data:
Get one feature of all symbols of data:
Notice how symbols have become columns in the returned DataFrame? This is the format so much loved by vectorbt.
Get multiple features of multiple symbols of data:
Hint
As you might have noticed, vectorbt returns different formats depending upon when there is one or multiple features/symbols captured by the data instance. To produce a consisting format irrespective of the number of features/symbols, pass features/symbols as a list or any other collection.
For example, running yf_data.get(features="Close") when there is only one symbol will produce a Series instead of a DataFrame. To force vectorbt to always return a DataFrame, pass features=["Close"].
Magnet features are features with case-insensitive names that the Data class knows how to detect and query. They include static features such as OHLCV, but also those that can be computed dynamically such as VWAP, HLC/3, OHLC/4, and returns. Each feature is also associated with an instance property that returns that feature for all symbols in a data instance. For example, to get the close price and returns:
Thanks to the unambiguous nature of magnet features, we can use them in feeding many functions across vectorbt, and since most functions don't accept data directly but expect features such as close to be provided separately, there is an urgent need for a method that can recognize what a function wants and pass the data to it accordingly. Such a method is Data.run: it accepts a function, parses its arguments, and upon recognition of a magnet feature, simply forwards it. This is especially useful for quickly running indicators, which are recognized automatically by their names:
If there are multiple third-party libraries that have the same indicator name, it's advisable to also provide a prefix with the name of the library to avoid any confusion:
This method also accepts names of all the simulation methods available in Portfolio, such as Portfolio.from_holding:
Class Data implements various dictionaries that hold data per symbol, but also methods that let us manipulate that data.
We can view the list of features and symbols using the Data.features and Data.symbols property respectively:
Additionally, there is a flag Data.single_key that is True if this instance holds only one symbol of data (or feature in case the instance is feature-oriented!). This has implications on Getting as we discussed in the hints above.
Each data instance holds at least 5 dictionaries:
Each dictionary is a regular dictionary of either the type symbol_dict (mostly when the instance is symbol-oriented) or feature_dict (mostly when the instance is feature-oriented).
Important
Do not change the values of the above dictionaries in-place. Whenever working with keyword arguments, make sure to build a new dict after selecting a symbol: dict(data.fetch_kwargs[symbol]) - this won't change the parent dict in case you want to modify the keyword arguments for some task.
One or more symbols can be selected using Data.select:
The operation above produced a new YFData instance with only one symbol - BTC-USD.
Note
Updating the data in a child instance won't affect the parent instance we copied from because updating creates a new Pandas object. But changing the data in-place will also propagate the change to the parent instance. To make both instances fully independent, pass copy_mode_="deep" (see Configured.replace).
Info
If the instance is feature-oriented, this method will apply to features rather than symbols.
Symbols can be renamed using Data.rename:
Warning
Renaming symbols may (and mostly will) break their updating. Use this only for getting.
Info
If the instance is feature-oriented, this method will apply to features rather than symbols.
Classes come in handy when we want to introduce another level of abstraction over symbols, such as to further divide symbols into industries and sectors; this would allow us to analyze symbols within their classes, and entire classes themselves. Classes can be provided to the fetcher via the argument classes; they must be specified per symbol, unless there is only one class that should be applied to all symbols. In the end, they will be converted into a (multi-)index and stacked on top of symbol columns when getting the symbol wrapper using Data.get_symbol_wrapper. Each class can be either provided as a string (which will be stored under the class name symbol_class), or as a dictionary where keys are class names and values are class values:
Apart from feeding classes to the fetcher, we can also replace them in any existing data instance, which will return a new data instance:
Or by using Data.update_classes:
Info
If the instance is feature-oriented and the dictionary with classes is of the type feature_dict, the classes will be applied to features rather than symbols.
We don't need data instances to work with vectorbt since the main objects of vectorbt's operation are Pandas and NumPy arrays, but sometimes it's much more convenient having all the data located under the same Data container because it can be managed (aligned, resampled, transformed, etc.) in a standardized way. To wrap any custom Pandas object with a Data class, we can use the class method Data.from_data, which can take either a single Pandas object (will be stored under the symbol symbol), a symbol dictionary consisting of multiple Pandas objects - one per symbol, or a feature dictionary consisting of multiple Pandas objects - one per feature.
The Series/DataFrame to be wrapped normally has columns associated with features such as OHLC as opposed to symbols such as BTCUSDT, for example:
We can also wrap multiple Pandas objects keyed by symbol:
If our data happen to have symbols as columns, enable columns_are_symbols:
In this case, the instance will become feature-oriented, that is, the DataFrame above will be stored in a feature_dict and the behavior of symbols and features will be swapped across many methods. To make the instance symbol-oriented as in most of our examples, additionally pass invert_data=True.
As you might have already noticed, the process of aligning data is logically separated from the process of fetching data, enabling us to merge and align any data retrospectively.
Instead of storing and managing all symbols as a single monolithic entity, we can manage them separately and merge into one data instance whenever this is actually needed. Such an approach may be particularly useful when symbols are distributed over multiple data classes, such as a mixture of remote and local data sources. For this, we can use the class method Data.merge, which takes two or more data instances, merges their information, and forwards the merged information to Data.from_data:
The benefit of this method is that it not only merges different symbols across different data instances, but it can also merge Pandas objects corresponding to the same symbol:
We called Data on the class YFData, which automatically creates an instance of that class. Having an instance of YFData, we can update the data the same way as we did before.But what if the data instances to be merged originate from different data classes? If we used YFData for merging CCXTData and BinanceData instances, we wouldn't be able to update the data objects anymore since the method YFData.update_symbol was implemented specifically for the symbols supported by Yahoo Finance. 
In such case, either use Data, which will raise an error when attempting to update, or create a subclass of it to handle updates using different data providers (which is fairly easy if you know which symbol belongs to which data class - just call the respective fetch_symbol or update_symbol method):
We just created a flexible data class that can fetch, update, and manage symbols from multiple data providers. Great!
As a subclass of Wrapping, each data instance stores the normalized metadata of all Pandas objects stored in that instance. This metadata can be used for resampling (i.e., changing the time frame) of all Pandas objects at once. Since many data classes, such as CCXTData, have a fixed feature layout, we can define the resampling function for each of their features in a special config called "feature config" (stored under Data.feature_config) and bind that config to the class itself for the use by all instances. Similar to field configs in Records, this config also can be attached to an entire data class or on any of its instances. Whenever a new instance is created, the config of the class is copied over such that rewriting it wouldn't affect the class config.
Here's, for example, how the feature config of BinanceData looks like:
Wondering where are the resampling functions for all the OHLCV features? Those features are universal, and recognized and resampled automatically.
Let's resample the entire daily BTC/USD data from Yahoo Finance to the monthly frequency:
Since vectorbt works with custom target indexes just as well as with frequencies, we can provide a custom index to resample to:
Note
Whenever providing a custom index, vectorbt will aggregate all the values after each index entry. The last entry aggregates all the values up to infinity. See GenericAccessor.resample_to_index.
If a data class doesn't have a fixed feature layout, such as HDFData, we need to adapt the feature config to each data instance instead of setting it to the entire data class. For example, if we convert bn_data_btc to a generic Data instance:
The same can be done with a single copy operation using Data.use_feature_config_of:
Similarly to resampling, realignment also changes the frequency of data, but in contrast to resampling, it doesn't aggregate the data but includes only the latest data point available at each step in the target index. It uses GenericAccessor.realign_opening for "open" and GenericAccessor.realign_closing for any other feature. This has two major use cases: aligning multiple symbols from different timezones to a single index, and upsampling data. Let's align symbols with different timings:
The main challenge in transforming any data is that each symbol must have the same index and columns because we need to concatenate them into one Pandas object later, thus any transformation operation must ensure that it's applied on each symbol in the same way. To enforce that, the method Data.transform concatenates the data across all symbols and features into one big DataFrame, and passes it to an UDF for transformation. Once transformed, the method splits the result back into multiple smaller Pandas objects - one per symbol, aligns them, creates a new data wrapper based on the aligned index and columns, and finally, initializes a new data instance.
Let's drop any row that contains at least one NaN:
We can also decide to pass only one feature or symbol at a time by setting per_feature=True and per_symbol=True respectively. By enabling both arguments simultaneously, we can instruct vectorbt to pass only one feature and symbol combination as a Pandas Series at a time.
Each data class subclasses Analyzable, which makes it analyzable and indexable.
We can perform Pandas indexing on the data instance to select rows and columns in all fetched Pandas objects. Supported operations are iloc, loc, xs, and []:
Note
Don't attempt to select symbols in this way - this notation is reserved for rows and columns only. Use Data.select instead.
Info
If the instance is feature-oriented, this method will apply to features rather than symbols.
As with every Analyzable instance, we can compute and plot various properties of the data stored in the instance.
Very often, a simple call of DataFrame.info and DataFrame.describe on any of the stored Series or DataFrames is enough to print a concise summary:
 
Symbol 2/2

But since any data instance can capture multiple symbols, using StatsBuilderMixin.stats can provide us with information on symbols as well:
To plot the data, we can use the method Data.plot, which produces an OHLC(V) chart whenever the Pandas object is a DataFrame with regular price features, and a line chart otherwise. The former can plot only one symbol of data, while the latter can plot only one feature of data; both can be specified with the symbol and feature argument respectively.
Since different symbols mostly have different starting values, we can provide an argument base, which will rebase the time series to start from the same point on chart:
 
Symbol 2/2


Info
This only works for line traces since we cannot plot multiple OHLC(V) traces on the same chart.
Like most things, the same can be replicated using a chain of simple commands:
In addition, Data can display a subplot per symbol using PlotsBuilderMixin.plots, which utilizes Data.plot under the hood:

By also specifying a column, we can plot one feature per symbol of data:

We can select one or more symbols by passing them via the template_context dictionary:

If you look into the Data.subplots config, you'll notice only one subplot defined as a template. During the resolution phase, the template will be evaluated and the subplot will be expanded into multiple subplots - one per symbol - with the same name plot but prefixed with the index of that subplot in the expansion. For illustration, let's change the colors of both lines and plot their moving averages:

If you're hungry for a challenge, subclass the YFData class and override the Data.plot method such that it also runs and plots the SMA over the time series. This would make the plotting procedure ultra-flexible because now you can display the SMA for every feature and symbol without caring about the subplot's position and other things.
 Python code
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
Info
VectorBT PRO is a totally different beast compared to the open-source version. In fact, the PRO version redesigns the underlying core to enable groundbreaking features. 
To avoid using an outdated code, make sure to only import vectorbtpro!
After you've been added to the list of collaborators and accepted the repository invitation, the next step is to create a Personal Access Token for your GitHub account in order to access the PRO repository programmatically (from the command line or GitHub Actions workflows):
Generate the token and save it in a safe place
Important
After a few months you may get an email from GitHub stating that your personal access token has expired. If so, please go over the steps above and generate a new token. This has nothing to do with the access to the vectorbtpro repository!
If you don't have Git, install it.
To use TA-Lib for Python, you need to install the actual library. Follow these instructions.
Other approaches are described here.
If you're on Windows, it's recommended to use WSL for development.
If you plan to use vectorbtpro locally, it's recommended to establish a new environment solely for vectorbtpro. 
The easiest way is to download Anaconda, which has a graphical installer and comes with many popular data science packages required by vectorbtpro such as NumPy, Pandas, Plotly, and more.
After the installation, create a new environment:
Activate the new environment:
Note
You need to activate the environment every time you start a new terminal session.
You should now see vectorbtpro in the list of all environments and being active (notice *):
You can now proceed with the installation of the actual package.
If you primarily work with an IDE, you can create a separate environment for each project. Here is how to create a new environment with PyCharm. The same but for Visual Studio Code is explained here.
The PRO version can be installed with pip.
Hint
It's highly recommended creating a new virtual environment solely for vectorbtpro, such as with Anaconda.
Uninstall the open-source version if installed:
Install the base PRO version (with recommended dependencies) using git+https:
Info
Whenever you are prompted for a password, paste the token that you generated in the previous steps.
To avoid re-entering the token over and over again, you can add it to your system or set an environment variable GH_TOKEN and then install the package as follows:
On some systems, such as macOS, the token is usually remembered automatically.
Read more on managing tokens here.
Same using git+ssh (see Connecting to GitHub with SSH):
Lightweight version (with only required dependencies):
For other optional dependencies, see extra-requirements.txt.
Whenever a new version of vectorbtpro is released, the package will not update by itself - you need to install the update. Gladly, you can use the same exact command that you used to install the package to also update it.
To install from the develop branch:
Set your GitHub username and token using %env:
Warning
Make sure to delete this cell when sharing the notebook with others!
Install TA-Lib:
Install VectorBT PRO:
Restart the runtime, and you're all set!
With setuptools adding vectorbtpro as a dependency to your Python package can be done by listing it in setup.py or in your requirements files:
Of course, you can pull vectorbtpro directly from git:
Install the package:
The command above takes around 1GB of disk space, to create a shallow clone:
To convert the clone back into a complete one:
Using Docker is a great way to get up and running in a few minutes, as it comes with all dependencies pre-installed.
Docker image of vectorbtpro is based on Jupyter Docker Stacks - a set of ready-to-run Docker images containing Jupyter applications and interactive computing tools. Particularly, the image is based on jupyter/scipy-notebook, which includes a minimally-functional JupyterLab server and preinstalled popular packages from the scientific Python ecosystem, and extends it with Plotly and Dash for interactive visualizations and plots, and vectorbtpro and most of its optional dependencies. The image requires the source of vectorbtpro to be available in the current depository.
Before proceeding, make sure to have Docker installed.
Launch Docker using Docker Desktop.
Clone the vectorbtpro repository (if not already). Run this from a directory where you want vectorbtpro to reside, for example, in Documents/GitHub:
Go into the directory:
Build the image (can take some time):
Create a working directory inside the current directory:
Start a container running a JupyterLab Server on the port 8888:
Info
The use of the -v flag in the command mounts the current working directory on the host ({PWD/work} in the example command) as /home/jovyan/work in the container. The server logs appear in the terminal. Due to the usage of the flag --rm Docker automatically cleans up the container and removes the file system when the container exits, but any changes made to the ~/work directory and its files in the container will remain intact on the host. The -it flag allocates pseudo-TTY.
Alternatively, if the port 8888 is already in use, use another port (here 10000):
Once the server has been launched, visit its address in a browser. The address is printed in the console, for example: http://127.0.0.1:8888/lab?token=9e85949d9901633d1de9dad7a963b43257e29fb232883908
Note
Change the port if necessary.
This will open JupyterLab where you can create a new notebook and start working with vectorbtpro 
To make use of any files on the host, put them into to the working directory work on the host and they will appear in the file browser of JupyterLab. Alternatively, you can drag and drop them directly into the file browser of JupyterLab.
To stop the container, first hit Ctrl+C, and then upon prompt, type y and hit Enter
To upgrade the Docker image to a new version of vectorbtpro, first, update the local version of the repository from the remote:
Then, rebuild the image:
Info
This won't rebuild the entire image, only the vectorbtpro installation step.
In case of connectivity issues, the package can be also installed manually:
To install a custom release:
Replace filename with the actual file name.
Note
If the file name ends with (1) because there's already a file with the same name, make sure to remove the previous file and remove the (1) suffix from the newer one.
If you're getting the error "ModuleNotFoundError: No module named 'pybind11'", install pybind11 prior to the installation of vectorbtpro:
If you're getting the error "Cannot uninstall 'llvmlite'", install llvmlite prior to the installation of vectorbtpro:
If image generation hangs (such as when calling show_svg()), downgrade the Kaleido package:
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
VectorBT PRO was implemented to address common performance shortcomings of backtesting libraries. It builds upon the idea that each instance of a trading strategy can be represented in a vectorized form, so multiple strategy instances can be packed into a single multi-dimensional array, processed in a highly efficient manner, and analyzed easily.
Thanks to the time-series nature of trading data, most of the aspects related to backtesting can be translated into arrays. In particular, vectorbt operates on NumPy arrays, which are very fast due to optimized, pre-compiled C code. NumPy arrays are supported by numerous scientific packages in the vibrant Python ecosystem, such as Pandas, NumPy, and Numba. There is a great chance that you already used some of those packages!
While NumPy excels at performance, it's not necessarily the most intuitive package for time series analysis. Consider the following moving average using NumPy:
While it might be ultrafast, it takes time for the user to understand what is going on and also some mastery to write such vectorized code without bugs. What about other rolling functions that are powering more complex indicators? And what about resampling, grouping, and other operations on dates and time?
Here comes Pandas to the rescue! Pandas provides rich time series functionality, data alignment, NA-friendly statistics, groupby, merge and join methods, and lots of other conveniences. It has two primary data structures: Series (one-dimensional) and DataFrame (two-dimensional). You can imagine them as NumPy arrays wrapped with valuable information, such as timestamps and column names. Our moving average can be implemented in a one-liner:
VectorBT PRO relies heavily upon Pandas, but not in the way you think. Pandas has one big drawback for our use case: it's slow for bigger datasets and custom-defined functions. Many functions such as the rolling mean are implemented using Cython under the hood and are sufficiently fast. But once you try to implement a more complex function, such as some rolling ranking metric based on multiple time series, things are becoming complicated and slow. In addition, what about functions that cannot be vectorized? A portfolio strategy involving money management cannot be simulated directly using vector calculations. We arrived at a point where we need to write a fast iterative code that processes data in an element-by-element fashion.
What if I told you that there exists a Python package that lets you run for-loops at machine code speed? And that it understands NumPy well and doesn't require adapting Python code much? It would solve many of our problems: our code could suddenly become incredibly fast while staying perfectly readable. This package is Numba. Numba translates a subset of Python and NumPy code into fast machine code.
We can now clearly understand what is going on: we iterate over our time series one timestamp at a time, check whether there is enough data in the window, and if there is, we take the mean of it. Not only Numba is great for writing a human-readable and less error-prone code, it's also as fast as C!
Hint
If you're interested in how vectorbt uses Numba, just look at any directory or file with the name nb. This sub-package implements all the basic functions, while this module implements some hard-core stuff ( adults only).
So where is the caveat? Sadly, Numba only understands NumPy, but not Pandas. This leaves us without datetime index and other features so crucial for time series analysis. And that's where vectorbt comes into play: it replicates many Pandas functions using Numba and even adds some interesting features to them. This way, we not only make a subset of Pandas faster, but also more powerful! 
This is done as follows:
Or using vectorbt:
Notice how vbt is attached directly to the Series object? This is called an accessor - a convenient way to extend Pandas objects without subclassing them. Using an accessor we can easily switch between native Pandas and vectorbt functionality. Moreover, each vectorbt method is flexible towards inputs and can work on both Series and DataFrames.
You can learn more about vectorbt's accessors here. For instance, rolling_mean is part of the accessor GenericAccessor, which can be accessed directly using vbt. Another popular accessor ReturnsAccessor for processing returns is a subclass of GenericAccessor and can be accessed using vbt.returns.
Important
Each accessor expects the data to be in the ready-to-use format. This means that the accessor for working with returns expects the data to be returns, not the price!
Remember when we mentioned that vectorbt differs from traditional backtesters by taking and processing trading data as multi-dimensional arrays? In particular, vectorbt treats each column as a separate backtesting instance rather than a feature. Consider a simple OHLC DataFrame:
Here, columns are separate features describing the same abstract object - price. While it may appear intuitive to pass this DataFrame to vectorbt (as you may have done with scikit-learn and other ML tools, which expect DataFrames with features as columns), this approach has several key drawbacks in backtesting:
VectorBT PRO addresses this heterogeneity of features by processing them as separate arrays. So instead of passing one big DataFrame, we need to provide each feature independently:
Now, in case when we want to process multiple abstract objects, such as ticker symbols, we can simply pass DataFrames instead of Series:
Here, each column (also often referred as "line" in vectorbt) in each feature DataFrame represents a separate backtesting instance and generates a separate equity curve. Thus, adding one more backtest is as simple as adding one more column to the features 
Keeping features separated has another big advantage: we can combine them easily. And not only this: we combine all backtesting instances at once using vectorization. Consider the following example where we place an entry signal whenever the previous candle was green and an exit signal whenever the previous candle was red (which is pretty dumb but anyway):
The Pandas objects multi_close and multi_open can be Series and DataFrames of arbitrary shapes and our micro-pipeline will still work as expected.
In the example above, we created our multi-OHLC DataFrames with two columns - p1 and p2 - so we can easily identify them later during the analysis phase. For this reason, vectorbt ensures that those columns are preserved across the whole backtesting pipeline - from signal generation to performance modeling.
But what if individual columns corresponded to more complex configurations, such as those involving multiple hyperparameter combinations? Storing complex objects as column labels wouldn't work after all. Thanks to Pandas, there are hierarchical columns, which are just like regular columns but stacked upon each other. Each level of such hierarchy can help us to identify a specific input or parameter.
Take a simple crossover strategy as an example: it depends upon the lengths of the fast and slow windows. Each of these hyperparameters becomes an additional dimension for manipulating data and gets stored as a separate column level. Below is a more complex example of the column hierarchy of a MACD indicator:
The columns above capture two different backtesting configurations that can now be easily analyzed and compared using Pandas - a very powerful technique to analyze data. We may, for example, consider grouping our performance by macd_fast_window to see how the size of the fast window impacts the profitability of our strategy. Isn't this magic?
One of the most important concepts in vectorbt is broadcasting. Since vectorbt functions take time series as independent arrays, they need to know how to connect the elements of those arrays such that there is 1) complete information, 2) across all arrays, and 3) at each time step.
If all arrays are of the same size, vectorbt can easily perform any operation on an element-by-element basis. Whenever any of the arrays is of a smaller size though, vectorbt looks for a possibility to "stretch" it such that it can match the length of other arrays. This approach is heavily inspired by (and internally based upon) NumPy's broadcasting. The only major difference to NumPy is that one-dimensional arrays always broadcast along rows since we're working primarily with time series data.
Why should we care about broadcasting? Because it allows us to pass array-like objects of any shape to almost every function in vectorbt, be it constants or full-blown DataFrames, and vectorbt will automatically figure out where the respective elements belong to.
Hint
As a rule of thumb:
In contrast to NumPy and Pandas, vectorbt knows how to broadcast labels: in case where columns or individual column levels in both objects are different, they are stacked upon each other. Consider checking whenever the fast moving average is higher than the slow moving average, using the following window combinations: (2, 3) and (3, 4).
Hint
Appending .vbt to a Pandas object on the left will broadcast both operands with vectorbt and execute the operation with NumPy/Numba - ultimate combo 
In contrast to Pandas, vectorbt broadcasts rows and columns by their absolute positions, not labels. This broadcasting style is very similar to that of NumPy:
Important
If you pass multiple arrays of data to vectorbt, ensure that their columns connect well positionally!
In case your columns are not properly ordered, you will notice this by the result having multiple column levels with identical labels but different ordering.
Another feature of vectorbt is that it can broadcast objects with incompatible shapes but overlapping multi-index levels - those having the same name or values. Continuing with the previous example, check whenever the fast moving average is higher than the price:
And here comes more (bear with me): we can easily test multiple scalar-like hyperparameters by passing them as a Pandas Index. Let's compare whether the price is within thresholds:
As you see, smart broadcasting is  when it comes to merging information. See broadcast to learn more about broadcasting principles and new exciting techniques to combine arrays.
Broadcasting many big arrays consumes a lot of RAM and ultimately makes processing slower. That's why vectorbt introduces a concept of "flexible indexing", which does selection of one element out of a one-dimensional or two-dimensional array of an arbitrary shape. For example, if a one-dimensional array has only one element, and it needs to broadcast along 1000 rows, vectorbt will return that one element irrespective of the row being queried since this array would broadcast against any shape:
This is equivalent to the following:
Two-dimensional arrays have more options. Consider an example where we want to process 1000 columns, and we have a plenty of parameters that should apply per each element. Some parameters may be scalars that are the same for each element, some may be one-dimensional arrays that are the same for each column, and some may be the same for each row. Instead of broadcasting them fully, we can keep the number of their elements and just expand them to two dimensions in a way that would broadcast them nicely using NumPy:
A nice feature of this is that such an operation has almost no additional memory footprint and can broadcast in any direction infinitely - an open secret to how Portfolio.from_signals manages to broadcast more than 50 arguments without losing any memory or performance 
 Python code
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
In what follows, we will look at sub-packages, modules, and especially classes that act as building blocks for more advanced functionalities distributed across vectorbt, such as Portfolio. For illustration, we will gradually build our custom class CorrStats that will let us analyze the correlation between two arrays in the most performant and flexible way 
(Reload the page if the diagram doesn't show up)
VectorBT PRO deploys a modular project structure that is composed of a range of subpackages. Each subpackage is applicable to a certain area of analysis. 
Subpackage utils contains a set of utilities powering every part of vectorbt  They are loosely connected and provide small but powerful re-usable code snippets that can be used independently of other functionality. 
Info
The main reason why we don't import third-party packages but implement many utilities from scratch is because we want to retain full control over execution and code quality.
VectorBT PRO implements its own formatting engine that can pretty-print any Python object. It's far more superior to formatting with JSON because it respects native Python data types and injects some smart formatting logic when it comes to more structured data types, such as np.dtype and namedtuple. Often, we can even convert the string back into Python using eval.
Let's beautify a nested dictionary using prettify and convert the string back into an object:
Hint
Wondering why we used vbt.prettify instead of vbt.utils.formatting.prettify? Any utility that may prove useful to the end user can be accessed directly from vbt.
To see which utilities are accessible from the root of the package, visit vectorbtpro/utils/__init__.py or any other subpackage, and look for the objects that are listed in __all__.
Class Prettified implements the abstract method Prettified.prettify, which a subclass can override to pretty-print an instance using prettify. Read below to learn how instances of various classes can be introspected using this method.
Pickling is the process of converting a Python object into a byte stream to store it in a file/database. Class Pickleable enables pickling of objects of any complexity using Dill (or pickle if Dill is not installed). Each of its subclasses inherits ready-to-use methods for serializing, de-serializing, saving to a file, and loading from a file. This is truly amazing because it allows us to persist objects holding any type of data, including instances of Data and Portfolio.
VectorBT PRO heavily relies upon automation based on some sort of specification. The specification for most repetitive tasks is usually stored inside so-called "configs", which act as settings for a certain task, a data structure, or even a class. This makes most places in vectorbt transparent and easily traversable and changeable programmatically.
Class Config is a dictionary on steroids: it extends the Python's dict with various configuration features, such as frozen keys, read-only values, accessing keys via the dot notation, and nested updates. The most notable feature is the ability to reset a config to its initial state and even make checkpoints, which is particularly useful for settings. In addition, since Config inherits Pickleable, we can save any configuration to disk, while subclassing Prettified allows us to beautify it (by the way, this approach is being used to generate the API reference):
Configs are very common structures in vectorbt. There are three main types of configs (that either subclass or partially call Config) used throughout vectorbt: 
A config can be created like a regular dict, whereas keyword arguments coming with a trailing underscore, such as readonly_, are usually reserved for setting up the config:
Apart from the use cases listed above, Config is also being used by the class Configured, which is a base class to most core classes in vectorbt. It's a read-only class that holds a config of type Config with all arguments passed during the initialization. Whenever we initialize any subclass of Configured, any named arguments we passed to the initializer (__init__) are stored inside Configured.config. This way, the created instance is described and managed solely by its config:
The main requirement for all of this to work properly is immutability. And here we have arrived at the first very important design decision: most classes in vectorbt are meant to be immutable (read-only) and it's discouraged to change any attribute unless it's listed in a special variable called _writeable_attrs. There are multiple reasons why we require immutability:
Let's create our custom class that returns some correlation statistics of two arrays. In particular, it will compute the Pearson correlation coefficient and its rolling version using Pandas:
This is how most configured classes in vectorbt, such as Portfolio, are designed. Any argument that is being passed to CorrStats is forwarded down to Configured to initialize a new config:
Access to any attribute is read-only: whenever we try to set a read-only property or modify the config, an (expected) error will be thrown:
However, it won't (and can't) throw an error when setting a private attribute (with a leading underscore) or if any of the attributes were modified in place, which is a common pitfall you should avoid.
Warning
vectorbt assumes that the data in a configured instance always stays the same. Whenever there is a change to data, vectorbt won't register it and likely deliver erroneous results at some point in time.
The only approved way to change any data in an instance is to create another instance!
To change anything, pass the new data to Configured.replace, which takes the same arguments as the class but in the keyword-only format, merges them over the old config, and passes as keyword arguments to the class for instantiation.
Since all of our data is now stored inside a config, we can perform many actions on the instance as if we performed them on the config itself, such as saving to disk (thanks to Pickling):
Attribute resolution is handy when it comes to accessing attributes based on strings or some other logic, which is realized by the mixin AttrResolverMixin. You can imagine it implementing an arbitrary logic for a custom getattr operation. It's widely used in StatsBuilderMixin and PlotsBuilderMixin to execute metrics and subplots respectively as a chain of commands. In other classes, such as Portfolio, it's also being used for accessing shortcut properties, caching attribute access, and more. It works in conjunction with deep_getattr, which accesses a chain of attributes provided as a specification.
Let's compute the min of the rolling mean solely using Pandas and deep attribute resolution:
If any of the above operations were done on a subclass AttrResolverMixin, they could have been preprocessed and postprocessed easily.
Templates play an important role in the vectorbt's ecosystem. They allow postponing data resolution to a later point in time when there is more information available. There are many different templating classes, such as Rep for replacing an entire string and Sub for substituting only parts of it (those beginning with $). 
You can imagine templates being callbacks that are executed at some point during the execution, mostly after broadcasting or merging keyword arguments. Also, there exist functions that offer multiple potential substitution points; in such case, they either attempt to substitute the template multiple times until they succeed, or they match the template with a specific substitution id (sub_id), if provided. The actual substitution operation is performed by substitute_templates.
Subpackage base is the non-computational core of vectorbt. It offers a range of modules for working with and converting between Pandas and NumPy objects. In particular, it provides functions and classes for broadcasting, combining and wrapping NumPy arrays, grouping columns, managing MultiIndex, and more. These operations are essential for extending Pandas and replicating some of its functionality in custom classes.
Since vectorbt usually associates with processing multi-column data, where each column (or "line") represents a separate backtesting instance, the ability to group those columns into some sort of groups is a must-have feature.
Class Grouper implements functionality to validate and build groups of any Pandas Index, especially columns. It's capable of translating various metadata such as GroupBy objects and column levels into special NumPy arrays that can be used by Numba-compiled functions to aggregate multiple columns of data. This is especially useful for multi-asset portfolios where each group is composed of one or more assets.
The main purpose of indexing in vectorbt is to provide Pandas indexing to any custom class holding Pandas-like objects, in particular, to select rows, columns, and groups in each. This is done by forwarding a Pandas indexing operation to each Pandas-like object and instantiating the class using them, which is fairly easy using Configured. This way, one can index complex classes with dozens of Pandas-like objects using a single command.
The main indexer class PandasIndexer mimics a regular Pandas object by exposing properties PandasIndexer.iloc, PandasIndexer.loc, and PandasIndexer.xs. All we have to do is to subclass this class and override IndexingBase.indexing_func, which should take pd_indexing_func, apply it on each Pandas-like object, and initialize a new instance. 
Let's extend our newly created CorrStats class with Pandas indexing:
We just indexed two Pandas objects as a single entity. Yay!
Remember how vectorbt specializes at taking a Pandas object, extracting its NumPy array, processing the array, and converting the results back into a Pandas format? The last part is done by the class ArrayWrapper, which captures all the necessary metadata, such as the index, columns, and number of dimensions, and exposes methods such as ArrayWrapper.wrap to convert a NumPy object back into a Pandas format. 
Class ArrayWrapper combines many concepts we introduced earlier to behave just like a (supercharged) Pandas object. In particular, it uses Grouping to build and manage groups of columns, and Indexing to select rows, columns, and groups using Pandas indexing. Probably the most powerful features of an array wrapper is 1) the ability to behave like a grouped object, which isn't possible with Pandas alone, and 2) the ability to translate a Pandas indexing operation to a range of integer arrays that can be used to index NumPy arrays. The latter allows indexing without the need to hold Pandas objects, only the wrapper.
We can construct a wrapper in multiple ways, the easiest being using a Pandas object:
Let's create a function that sums all elements over each column using NumPy and returns a regular Pandas object:
The function above is already 20x faster than Pandas 
Since ArrayWrapper can manage groups of columns, let's adapt our function to sum all elements over each group of columns:
To avoid creating multiple array wrappers with the same metadata, there is the class Wrapping, which binds a single instance of ArrayWrapper to manage an arbitrary number of shape-compatible array-like objects. Instead of accepting multiple Pandas objects, it takes an array wrapper, and all other objects and arrays in any format (preferably NumPy), and wraps them using this wrapper. Additionally, any Wrapping subclass can utilize its wrapper to perform Pandas indexing on any kind of objects, including NumPy arrays, because ArrayWrapper can translate a Pandas indexing operation into universal row, column, and group indices. 
Coming back to our CorrStats class. There are two issues with the current implementation:
Let's upgrade our CorrStats class to work on NumPy arrays and with an array wrapper:
As you might have noticed, we replaced the superclasses Configured and PandasIndexer with a single superclass Wrapping, which already inherits them both. Another change applies to the arguments taken by CorrStats: instead of taking two Pandas objects, it now takes wrapper of type ArrayWrapper along with the NumPy arrays obj1 and obj2. This has several benefits: we're keeping Pandas metadata consistent and managed by a single variable, while all actions are efficiently performed using NumPy alone. Whenever there is a need to present the findings, we can call ArrayWrapper.wrap_reduced and ArrayWrapper.wrap to transform them back into a Pandas format, which is done inside the methods CorrStats.corr and CorrStats.rolling_corr respectively. 
Since we don't want to force ourselves and the user to create an array wrapper manually, we also implemented the class method CorrStats.from_objs, which broadcasts both arrays and instantiates CorrStats. This way, we can provide array-like objects of any kind and CorrStats will automatically build the wrapper for us. Let's illustrate this by computing the correlation coefficient for df1 and df2, and then for df1 and parametrized df2:
Here's why we ditched Pandas in favor of Numba:
Another addition we made concerns indexing. Since obj1 and obj2 are not regular Pandas objects anymore, we cannot simply apply pd_indexing_func on them. Instead, we can use the method ArrayWrapper.indexing_func_meta to get the rows, columns, and groups that this operation would select. We then apply those arrays on both NumPy objects. This approach is exceptionally useful because now we can select any data from the final shape:
Note
Not all classes support indexing on rows. To make sure you can select rows, check whether the instance property column_only_select is False.
That's how most high-tier classes in vectorbt are built. As a rule of thumb:
This subpackage also contains the accessor BaseAccessor, which exposes many basic operations to the end user and is being subclassed by all other accessors. It inherits Wrapping and thus we can do with it everything what we can do with our custom CorrStats class. Why calling an accessor a "base" accessor? Because it is the superclass of all other accessors in vectorbt and provides them with the core combining, reshaping, and indexing functions, such as BaseAccessor.to_2d_array to convert a Pandas object into a two-dimensional NumPy array.
The "access" to the accessor is simple:
In the example above, Vbt_DFAccessor is the main accessor for DataFrames, and as you can see in its definition, BaseAccessor appears in its superclasses.
Probably the most interesting is the method BaseAccessor.combine, which allows for broadcasting and combining the current Pandas object with an arbitrary number of other array-like objects given the function combine_func (mainly using NumPy). 
BaseAccessor implements a range of unary and binary magic methods using this method. For example, let's invoke BaseAccessor.__add__, which implements addition:
Hint
To learn more about  methods, see A Guide to Python's Magic Methods.
All of these magic methods were added using class decorators. There are a lot of class decorators for all kind of things in vectorbt. Usually, they take a config and attach many attributes at once in some automated way.
Subpackage generic is the computational core of vectorbt. It contains modules for processing and plotting time series and numeric data in general. More importantly: it implements an arsenal of Numba-compiled functions for accelerating and extending Pandas! Those functions are powering many corners of vectorbt, from indicators to portfolio analysis. But for now, let's focus on classes that could make our CorrStats class more powerful.
Builder mixins are classes that, once subclassed by a class, allow to build a specific functionality from this class' attributes. There are two prominent members: StatsBuilderMixin and PlotsBuilderMixin. The former exposes the method StatsBuilderMixin.stats to compute various metrics, while the latter exposes the method PlotsBuilderMixin.plots to display various subplots. Both are subclassed by almost every class that can analyze data.
Class Analyzable combines Wrapping and Builder mixins. It combines everything we introduced above to build a foundation for a seamless data analysis; that's why it's being subclassed by so many high-tier classes, such as Portfolio and Records.
What are we waiting for? Let's adapt our CorrStats class to become analyzable!
We changed a few things: replaced Wrapping with Analyzable, and added some metrics and subplots based on CorrStats.corr and CorrStats.rolling_corr. That's it! We can now pass arbitrary array-like objects to CorrStats.from_objs and it will return an instance that can be used to analyze the correlation between the objects, in particular using CorrStats.stats and CorrStats.plots:

There is nothing more satisfying than not having to write boilerplate code. Thanks to Analyzable, we can shift our focus entirely to analysis, while vectorbt takes care of everything else.
We don't have to look far to find a class that inherits Analyzable: class GenericAccessor extends the class BaseAccessor to deliver statistics and plots for any numeric data. It's a size-that-fits-all class with an objective to replicate, accelerate, and extend Pandas' core functionality. It implements in-house rolling, mapping, reducing, splitting, plotting, and many other kinds of methods, which can be used on any Series or DataFrame in the wild.
In a nutshell, GenericAccessor does the following:
Similarly to the Base accessor, the generic accessor uses class decorators and configs to attach many Numba-compiled and scikit-learn functions at once.
Usage is similar to CorrStats, except that we can call the generic accessor directly on Pandas objects since it's being directly subclassed by Vbt_DFAccessor!
Records are structured arrays - a NumPy array that can hold different data types, just like a Pandas DataFrame. Records have one big advantage over DataFrames though: they are well understood by Numba, thus we can generate and use them efficiently. So, what's the catch? Records have no (index) labels and the API is very limited. We also learned that vectorbt doesn't like heterogenous data and prefers to work with multiple homogeneous arrays (remember how we need to split OHLC into O, H, L, and C?). Nevertheless, they take a very important place in our ecosystem - as containers for event data.
Trading is all about events: executing trades, aggregating them into positions, analyzing drawdowns, and much more. Each of such events is a complex piece of data that needs a container optimized for fast writes and reads, especially inside a Numba-compiled code (but please not a list of dictionaries, they are very inefficient). Structured arrays is exactly the data structure we need! Each event is a record that holds all the required information, such the column and row where it originally happened.
Because structured arrays are hard to analyze, there is a class just for this - Records! By subclassing Analyzable (suprise, surprise), it wraps a structured NumPy array and provides us with useful tools for its analysis. Every Records instance can be indexed just like a regular Pandas object and compute various metrics and plot graphs. 
Let's generate the Drawdowns records for two columns of time series data:
That's a lot of information! Each field is a regular NumPy array, so where do we get this rich information from? Maybe to your surprise, but the labels of the DataFrames above were auto-generated from the metadata that Drawdowns holds. This metadata is called a "field config" - a regular Config that describes each field (for instance, Drawdowns.field_config). This makes possible automating and enhancing the behavior of each field. Class Records, the base class to all records classes, has many methods to read and interpret this config.
Records are one-dimensional structured NumPy arrays. Records from multiple columns are concatenated into a single array, so we need a mechanism to group them by column or group, for instance, to aggregate values column-wise. This is a non-trivial task because finding the records that correspond to a specific column requires searching all records, which is slow when done repeatedly. The task of the class ColumnMapper is to index all columns only once and cache the result (see ColumnMapper.col_map). A column mapper has at least two more advantages: it allows for grouping columns and enables efficient Indexing.
The produced column map means that the column a has two records at indices 0 and 1, while the column b is represented by only one record at index 2.
If Records is our own DataFrame for events, then MappedArray is our own Series! Each field in records can be mapped into a mapped array. In fact, a mapped array is where most calculations take place. It's just like GenericAccessor, but with a totally different representation of data: one-dimensional and clustered instead of two-dimensional and column-wise. We can even seemingly convert between both representations. Why wouldn't we then simply convert a mapped array into a regular Series and do all the analysis there? There are several reasons:
Let's analyze the drawdown values of drawdowns:
Thanks to ColumnMapper and Analyzable, we can select rows and columns from a mapped array the same way as from records or any regular Pandas object:
Kudos for following me all the way down here! The classes that we just covered build a strong foundation for data analysis with vectorbt; they implement design patterns that are encountered in most other places across the codebase, which makes them very easy to recognize and extend. In fact, the most hard-core class Portfolio is very similar to our CorrStats. 
You're now more than ready for using vectorbt, soldier 
 Python code
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
One of the main powers of vectorbt (PRO) is the ability to create and backtest numerous strategy configurations in the blink of an eye. In this introductory example, we will explore how profitable is the following RSI strategy commonly used by beginners:
If the RSI is less than 30, it indicates a stock is reaching oversold conditions and may see a trend reversal, or bounceback, towards a higher share price. Once the reversal is confirmed, a buy trade is placed. Conversely, if the RSI is more than 70, it indicates that a stock is reaching an overbought condition and may see a trend reversal, or pullback, in price. After a confirmation of the reversal, a sell trade is placed.
As a bonus, we will gradually expand the analysis towards multiple parameter combinations. Sounds fun? Let's start. 
First, we will take care of data. Using a one-liner, we will download all available daily data for the pair BTC/USDT from Binance:
 
100%

The returned object is of type BinanceData, which extends Data to communicate with the Binance API. The class Data is a vectorbt's in-house container for retrieving, storing, and managing data. Upon receiving a DataFrame, it post-processes and stores the DataFrame inside the dictionary Data.data keyed by pair (also referred to as a "symbol" in vectorbt). We can get our DataFrame either from this dictionary, or by using the convenient method Data.get, which also allows for specifying one or more columns instead of returning the entire DataFrame at once.
Let's plot the data with Data.plot:

Another way to describe the data is by using the Pandas' info method. The tabular format is especially useful for counting null values (which our data apparently doesn't have - good!)
In our example, we will generate signals based on the opening price and execute them based on the closing price. We can also place orders a soon as the signal is generated, or at any later time, but we will illustrate how to separate generation of signals from their execution.
It's time to run the indicator!
VectorBT PRO supports 5 (!) different implementations of RSI: one implemented using Numba, and the other four ported from three different technical analysis libraries. Each indicator has been wrapped with the almighty IndicatorFactory 
To list all the available indicators or to search for a specific indicator, we can use IndicatorFactory.list_indicators:
We can then retrieve the actual indicator class as follows:
Or manually:
Here's a rule of thumb on which implementation to choose:
To run any indicator, use the method run. To see what arguments the method accepts, pass it to phelp:
As we can see above, we need to at least provide close, which can be any numeric time series. Also, by default, the rolling window is 14 bars long and uses the Wilder's smoothed moving average. Since we want to make decisions based on the opening price, we will pass open_price as close:
That's all! By executing the method RSI.run, we calculated the RSI values and have received an instance with various methods and properties for their analysis. To retrieve the resulting Pandas object, we need to query the rsi attribute (see "Outputs" in the output of phelp).
Having the RSI array, we now want to generate an entry signal whenever any RSI value crosses below 30 and an exit signal whenever any RSI value crosses above 70:
The same can be done using the methods RSI.rsi_crossed_below and RSI.rsi_crossed_above that were auto-generated for the output rsi by IndicatorFactory:
Hint
If you are curious what else has been generated, print dir(rsi) or look into the API generated for the class.
Before we proceed with the portfolio modeling, let's plot the RSI and signals to ensure that we did everything right:

The graph looks legit. But notice how there are multiple entries between two exits and vice versa? How does vectorbt handle it? When using Portfolio.from_signals, vectorbt will automatically filter out all entry signals if the position has already been entered, and exit signals if the position has already been exited. But to make our analysis cleaner, let's keep each first signal:

We can immediately see the difference. But what other methods exist to analyze the distribution of signals? How to quantify such analysis? That's what vectorbt is all about. Let's compute various statistics of clean_entries and clean_exits using SignalsAccessor:
We are ready for modeling! We will be using the class method Portfolio.from_signals, which will receive the signal arrays, process each signal one by one, and generate orders. It will then create an instance of Portfolio that can be used to assess the performance of the strategy.
Our experiment is simple: buy $100 of Bitcoin upon an entry signal and close the position upon an exit signal. Start with an infinite capital to not limit our buying power at any time.
Info
Running the method above for the first time may take some time as it must be compiled first. Compilation will take place each time a new combination of data types is discovered. But don't worry: Numba caches most compiled functions and re-uses them in each new runtime.
Hint
If you look into the API of Portfolio.from_signals, you will find many arguments to be set to None. The value None has a special meaning that instructs vectorbt to pull the default value from the global settings. You can discover all the default values for the Portfolio class here.
Let's print the statistics of our portfolio:
Hint
That are lots of statistics, right? If you're looking for the way they are implemented, print pf.metrics and look for the calc_func argument of the metric of interest. If some function is a lambda, look into the source code to reveal its contents.
Our strategy is not too bad: the portfolio has gained over 71% in profit over the last years, but holding Bitcoin is still better - staggering 450%. Despite the Bitcoin's high volatility, the minimum recorded portfolio value sits at $97 from $100 initially invested. The total time exposure of 38% means that we were in the market 38% of the time. The maximum gross exposure of 100% means that we invested 100% of our available cash balance, each single trade. The maximum drawdown (MDD) of 46% is the maximum distance our portfolio value fell after recording a new high (stop loss to the rescue?). 
The total number of orders matches the total number of (cleaned) signals, but why is the total number of trades suddenly 8 instead of 15? By default, a trade in the vectorbt's universe is a sell order; as soon as an exit order has been filled (by reducing or closing the current position), the profit and loss (PnL) based on the weighted average entry and exit price is calculated. The win rate of 70% means that 70% of the trades (sell orders) generated a profit, with the best trade bringing 54% in profit and the worst one bringing 32% in loss. Since the average winning trade generating more profit than the average losing trade generating loss, we can see various metrics being positive, such as the profit factor and the expectancy.

Hint
A benefit of an interactive plot like above is that you can use tools from the Plotly toolbar to draw a vertical line that connects orders, their P&L, and how they affect the cumulative returns. Try it out!
So, how do we improve from here?
Even such a basic strategy as ours has many potential parameters:
To make our analysis as flexible as possible, we will write a function that lets us specify all of that information, and return a subset of statistics:
Note
We removed the signal cleaning step because it makes no difference when signals are passed to Portfolio.from_signals (which cleans the signals automatically anyway).
By raising the upper threshold to 80% and lowering the lower threshold to 20%, the number of trades has decreased to just 2 because it becomes more difficult to cross the thresholds. We can also observe how the total return fell to roughly 7% - not a good sign. But how do we actually know whether this negative result indicates that our strategy is trash and not because of a pure luck? Testing one parameter combination from a huge space usually means making a wild guess.
Let's generate multiple parameter combinations for thresholds, simulate them, and concatenate their statistics for further analysis:
We just simulated 121 different combinations of the upper and lower threshold and stored their statistics inside a list. In order to analyze this list, we need to convert it to a DataFrame first, with metrics arranged as columns:
But how do we know which row corresponds to which parameter combination? We will build a MultiIndex with two levels, lower_th and upper_th, and make it the index of comb_stats_df:
Much better! We can now analyze every piece of the retrieved information from different angles. Since we have the same number of lower and upper thresholds, let's create a heatmap with the X axis reflecting the lower thresholds, the Y axis reflecting the upper thresholds, and the color bar reflecting the expectancy:

We can explore entire regions of parameter combinations that yield positive or negative results.
As you might have read in the documentation, vectorbt loves processing multidimensional data. In particular, it's built around the idea that you can represent each asset, period, parameter combination, and a backtest in general, as a column in a two-dimensional array.
Instead of computing everything in a loop (which isn't too bad but usually executes magnitudes slower than a vectorized solution) we can change our code to accept parameters as arrays. A function that takes such array will automatically convert multiple parameters into multiple columns. A big benefit of this approach is that we don't have to collect our results, put them in a list, and convert into a DataFrame - it's all done by vectorbt!
First, define the parameters that we would like to test:
Instead of applying itertools.product, we will instruct various parts of our pipeline to build a product instead, so we can observe how each part affects the column hierarchy.
The RSI part is easy: we can pass param_product=True to build a product of windows and wtypes and run the calculation over each column in open_price:
We see that RSI appended two levels to the column hierarchy: rsi_window and rsi_wtype. Those are similar to the ones we created manually for thresholds in Using for-loop. There are now 39 columns in total, which is just len(open_price.columns) x len(windows) x len(wtypes).
The next part are crossovers. In contrast to indicators, they are regular functions that take any array-like object, broadcast it to the rsi array, and search for crossovers. The broadcasting step is done using broadcast, which is a very powerful function for bringing multiple arrays to a single shape (learn more about broadcasting in the documentation).
In our case, we want to build a product of lower_ths, upper_th_index, and all columns in rsi. Since both rsi_crossed_below and rsi_crossed_above are two different functions, we need to build a product of the threshold values manually and then instruct each crossover function to combine them with every column in rsi:
We have produced over 4719 columns - madness! But did you notice that entries and exits have different columns now? The first one has lower_th as one of the column levels, the second one has upper_th. How are we supposed to pass differently labeled arrays (including close_price with one column) to Portfolio.from_signals?
No worries, vectorbt knows exactly how to merge this information. Let's see:
Congrats! We just backtested 4719 parameter combinations in less than a second 
Important
Even though we gained some unreal performance, we need to be careful to not occupy the entire RAM with our wide arrays. We can check the size of any Pickleable instance using Pickleable.getsize. For example, to print the total size of our portfolio in a human-readable format:
Even though the portfolio holds about 10 MB of compressed data, it must generate many arrays, such as the portfolio value, that have the same shape as the number of timestamps x parameter combinations:
We can see that each floating array occupies 65 MB of memory. By creating a dozen of such arrays (which is often the worst case), the memory consumption may jump to 1 GB very quickly.
One option is to use Pandas itself to analyze the produced statistics. For example, calculate the mean expectancy of each rsi_window:
The longer is the RSI window, the higher is the mean expectancy.
Display the top 5 parameter combinations:
To analyze any particular combination using vectorbt, we can select it from the portfolio the same way as we selected a column in a regular Pandas DataFrame. Let's plot the equity of the most successful combination:

Hint
Instead of selecting a column from a portfolio, which will create a new portfolio with only that column, you can also check whether the method you want to call supports the argument column and pass your column using this argument. For instance, we could have also used pf.plot_value(column=(22, 80, 20, "wilder")).
Even though, in theory, the best found setting doubles our money, it's still inferior to simply holding Bitcoin - our basic RSI strategy cannot beat the market 
But even if it did, there is much more to just searching for right parameters: we need at least to (cross-) validate the strategy. We can also observe how the strategy behaves on other assets. Curious how to do it? Just expand open_price and close_price to contain multiple assets, and each example would work out-of-the-box!
 
100%

Your homework is to run the examples on this data.
The final columns should become as follows:
We see that the column hierarchy now contains another level - symbol - denoting the asset. Let's visualize the distribution of the expectancy across both assets:

ETH seems to react more aggressively to our strategy on average than BTC, maybe due to the market's higher volatility, a different structure, or just pure randomness.
And here's one of the main takeaways of such analysis: using strategies with simple and explainable mechanics, we can try to explain the mechanics of the market itself. Not only can we use this to improve ourselves and design better indicators, but use this information as an input to ML models, which are better at connecting dots than humans. Possibilities are endless!
VectorBT PRO is a powerful vehicle that enables us to discover uncharted territories faster and analyze them in more detail. Instead of using overused and outdated charts and indicators from books and YouTube videos, we can build our own tools that go hand in hand with the market. We can backtest thousands of strategy configurations to learn how the market reacts to each one of them - in a matter of milliseconds. All it takes is creativity 
 Python code  Notebook
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
Class IndicatorFactory is a one of the most powerful entities in the vectorbt's ecosystem - it can wrap any indicator function and make it parametrizable and analyzable.
An indicator is a pipeline that does the following:
Let's manually create an indicator that takes two time series, calculates their normalized moving averages, and returns the difference of both. We'll test different shapes as well as parameter combinations to take advantage of broadcasting:
Neat! We just created a pretty flexible pipeline that takes arbitrary input and parameter combinations. The end result of this pipeline is a DataFrame where each column corresponds to a single window combination applied on a single column in both ts1 and ts2. But is this pipeline user-friendly?  Having to deal with broadcasting, output concatenation, and the column hierarchy makes this code no different from a regular Pandas code.
The pipeline above can be well standardized, which is done by IndicatorBase.run_pipeline. It conveniently prepares inputs, parameters, and columns, while the calculation and output concatenation have to be performed by the user using custom_func. Let's rewrite the above example a bit:
We produced much less code and did the entire calculation using NumPy and Numba alone - a big win! But what is this monstrous output? 
This raw output is really meant to be used by vectorbt, not by the user themselves - it contains useful metadata for working the indicator. Additionally, if you look into the source of this function, you will notice that it accepts a ton of arguments. Great complexity enables great flexibility: each argument is targeted at configuring a specific step of the pipeline. But don't worry: we won't use this function directly.
Instead, we will use IndicatorFactory, which simplifies the usage of IndicatorBase.run_pipeline by providing a unified interface and various automations. Let's wrap our custom_func using the factory:
Hint
vbt.IF is a shortcut for IndicatorFactory.
As you see, IndicatorFactory took the specification for our indicator and created an entire Python class that knows how to communicate with IndicatorBase.run_pipeline and manipulate and format its results. In particular, it attached the class method MADiff.run that looks exactly like custom_func but prepares and forwards all arguments to IndicatorBase.run_pipeline under the hood. Whenever we call the run method, it sets up and returns an instance of MADiff with all the input and output data.
You might ask: "Why doesn't the factory create a function instead of a class? Having an indicator function would be more intuitive!" If you read through Building blocks, you would already be familiar with the class Analyzable, which is the go-to class for analyzing data. The indicator class created by the factory is a subclass of Analyzable, so we not only have access to the output, but also to many methods for analyzing this output! For example, the factory automatically attaches crossed_above, cross_below, stats, and many other methods for each input and output that appears in the indicator:
The main purpose of IndicatorFactory is to create a stand-alone indicator class that has a run method for running the indicator. For this, it needs to know what inputs, parameters, and outputs to expect. This information can be passed in form of input_names, param_names, and other arguments to the constructor:
Upon the initialization, it creates the skeleton of our indicator class of type IndicatorBase, accessible via IndicatorFactory.Indicator. Even though the factory has created the constructor of this class and attached various properties and methods for working with it, we can't run the indicator:
This is because we haven't provided it with the calculation function yet. To do this, there are multiple methods starting with the prefix with_. The base method all other methods are based upon is IndicatorFactory.with_custom_func (which we used above) - it overrides the abstract run method to execute the indicator using IndicatorBase.run_pipeline and returns a ready-to-use indicator class:
The calculation function has been attached successfully, we can now run this indicator!
Factory methods come in two different flavors: instance and class methods. The instance methods with the prefix with_, such as IndicatorFactory.with_custom_func, require instantiation of the indicator factory. That is, we have to do vbt.IF(...) and provide the required information manually as we did with MADiff. The class methods with the prefix from_, such as IndicatorFactory.from_expr, can parse the required information (semi-)automatically.
The method IndicatorFactory.with_custom_func takes a so-called "custom function", which is the most flexible way to define an indicator. But with great power comes great responsibility: it's up to the user to iterate through parameters, handle caching, and concatenate columns for each parameter (usually by apply_and_concat). Also, we must ensure that each output array has an appropriate number of columns, which is the number of columns in the input arrays multiplied by the number of parameter combinations. Additionally, the custom function receives commands passed by the pipeline, and it's the task of the user to properly execute those commands.
For example, if our custom function needs the index and the columns along with the NumPy arrays, we can instruct the pipeline to pass the wrapper, which is done by setting pass_wrapper=True in with_custom_func. This as well as all other arguments are forwarded directly to IndicatorBase.run_pipeline, which takes care of communicating with our custom function.
The method IndicatorFactory.with_apply_func simplifies indicator development a lot: it creates custom_func that handles caching, iteration over parameters with apply_and_concat, output concatenation with column_stack, and passes this function to IndicatorFactory.with_custom_func. Our part is writing a so-called "apply function", which accepts a single parameter combination and does the calculation. The resulting outputs are automatically concatenated along the column axis.
Note
An apply function has mostly the same signature as a custom function, but the parameters are single values as opposed to multiple values.
Let's implement our indicator using an apply function:
That's it! Under the hood, our code created a custom function that iterates over both parameter combinations and calls apply_func on each one. If we printed ts1, ts2, w1, and w2, we would see that ts1 and ts2 are the same, while w1 and w2 are now single values. This way, we can entirely abstract ourselves from the number of parameter combinations and work with a single set of parameters at a time.
Another advantage of this method is that apply functions are natural inhabitants of vectorbt  and we can use most regular and Numba-compiled functions that take two-dimensional NumPy arrays directly as apply functions! Let's illustrate this by building an indicator for the rolling covariance:
Here, the both input arrays and the window parameter were passed directly to rolling_cov_nb.
We can easily emulate apply_func using custom_func and apply_and_concat, for example, if we need the index of the current iteration and/or want to have access to all parameter combinations:
The same using IndicatorFactory.with_apply_func and select_params=False:
Since the same apply function is being called multiple times - once per parameter combination -, we can use one of the vectorbt's preset execution engines to distribute those calls sequentially (default), across multiple threads, or across multiple processes. In fact, the function apply_and_concat, which is used to iterate over all parameter combinations, takes care of this automatically by forwarding all calls to the executor function execute. Using keyword arguments in execute_kwargs, we can define the rules by which to distribute those calls. For example, to follow the execution of each parameter combination using a progress bar:
 
Iteration 100/100

When the apply function is Numba-compiled, the indicator factory makes the parameter selection function Numba-compiled as well (+ with GIL released), so we can utilize multithreading. This entire behavior can be disabled by setting jit_select_params to False. The keyword arguments used to set up the Numba-compiled function can be passed via the jit_kwargs argument.
Note
Setting jit_select_params will remove all keyword arguments since variable keyword arguments aren't supported by Numba (yet). To pass keyword arguments to the apply function anyway, set remove_kwargs to False or use the kwargs_as_args argument, which specifies which keyword arguments should be supplied as (variable) positional arguments.
Additionally, we can explicitly set jitted_loop to True to loop over each parameter combination in a Numba loop, which speeds up the iteration for shallow inputs over a huge number of columns, but slows it down otherwise.
Note
In this case, the execution will be performed by Numba, so you can't use execute_kwargs anymore.
Sometimes, it's not that clear which arguments are being passed to apply_func. Debugging in this scenario is usually easy: just replace your apply function with a generic apply function that takes variables arguments, and print those.
Parsers are the most convenient way to build indicator classes. For instance, there are dedicated parser methods for third-party technical analysis packages that can derive the specification of each indicator in an (semi-)automated way. In addition, there is a powerful expression parser to avoid writing complex Python functions for simpler indicators. Let's express our indicator as an expression:
Notice how we didn't have to call vbt.IF(...)? IndicatorFactory.from_expr is a class method that parses input_names and other information from the expression and creates a factory instance using solely this information. Crazy how we compressed our first implementation with mov_avg_crossover to just this while enjoying all the perks, right?
Once we built our indicator class, it's time to run it. The main method for executing an indicator is the class method IndicatorBase.run, which accepts positional and keyword arguments based on the specification provided to the IndicatorFactory. These arguments include input arrays, in-place output arrays, and parameters. Any additional arguments are forwarded down to IndicatorBase.run_pipeline, which can either use them to set up the pipeline, or forward them further down to the custom function and then, if provided, the apply function.
To see what arguments the run method accepts, use phelp:
We see that MADiff.run takes two input time series ts1 and ts2, two parameters w1 and w2, and produces a single output time series diff. Upon calling the class method, it runs the indicator and returns a new instance of MADiff with all the data being ready for analysis. In particular, we can access the output as a regular instance attribute MADiff.diff.
The second method for running indicators is IndicatorBase.run_combs, which takes the same inputs as the method above, but computes all combinations of the passed parameters based on a combinatorial function and returns multiple indicator instances that can be combined with each other. This is useful to compare multiple indicators of the same type but different parameters, such as for testing a moving average crossover, which involves two MA instances applied on the same time series:
In the example above, MA.run_combs generated the combinations of window using itertools.combinations and r=2. The first set of window combinations was passed to the first instance, the second set to the second instance. The above example can be easily replicated using the run method alone:
The main advantage of a single run_combs call over multiple run calls is that it doesn't need to re-compute each combination thanks to smart caching.
Note
run_combs should be only used for combining multiple indicators. To test multiple parameter combinations, use run and provide parameters as lists.
VectorBT PRO implements a collection of preset, fully Numba-compiled indicators (such as ATR) that take advantage of manual caching, extending, and plotting. You can use them to take an inspiration on how to create indicators in a classic but performant way.
Note
vectorbt uses SMA and EMA, while other technical analysis libraries and TradingView use the Wilder's method. There is no right or wrong method. See different smoothing methods.
 Python code
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
While Python is slower than many compiled languages, it's easy to use and extremely diverse. For many, especially in the data science domain, the practicality of the language beats the speed considerations - it's like a Swiss army knife for programmers and researchers alike.
Unfortunately for quants, Python becomes a real bottleneck when iterating over (a large amount of) data. For this reason, there is an entire ecosystem of scientific packages such as NumPy and Pandas, which are highly optimized for performance, with critical code paths often written in Cython or C. Those packages mostly work on arrays, giving us a common interface for processing data in an efficient manner. 
This ability is highly appreciated when constructing indicators that can be translated into a set of vectorized operations, such as OBV. But even non-vectorized operations, such as the exponential weighted moving average (EMA) powering numerous indicators such as MACD, were implemented in a compiled language and are offered as a ready-to-use Python function. But sometimes, an indicator is difficult or even impossible to develop solely using standard array operations because the indicator introduces a path dependency, where a decision today depends upon a decision made yesterday. One member of such a family of indicators is SuperTrend.
In this example, you will learn how to design and implement a SuperTrend indicator, and gradually optimize it towards a never-seen performance using TA-Lib and Numba. We will also backtest the newly created indicator on a range of parameters using vectorbt (PRO).
The first step is always getting the (right) data. In particular, we need a sufficient amount of data to benchmark different SuperTrend implementations. Let's pull 2 years of hourly Bitcoin and Ethereum data from Binance using the vectorbt's BinanceData class:
 
Symbol 2/2

 
Period 36/36

The fetching operation for both symbols took us around 80 seconds to complete. Since Binance, as any other exchange, will never return the whole data at once, vectorbt first requested the maximum amount of data starting on January 1st, 2020 and then gradually collected the remaining data by also respecting the Binance's API rate limits. In total, this resulted in 36 requests per symbol. Finally, vectorbt aligned both symbols in case their indexes or columns were different and made the final index timezone-aware (in UTC).
To avoid repeatedly hitting the Binance servers each time we start a new Python session, we should save the downloaded data locally using either the vectorbt's Data.to_csv or Data.to_hdf:
We can then access the saved data easily using HDFData:
 
Symbol 2/2

Hint
We can access any of the symbols in an HDF file using regular path expressions. For example, the same as above: vbt.HDFData.pull(['my_data.h5/BTCUSDT', 'my_data.h5/ETHUSDT']).
Once we have the data, let's take a quick look at what's inside. To get any of the stored DataFrames, use the Data.data dictionary with each DataFrame keyed by symbol:
We can also get an overview of all the symbols captured:
Each symbol has 17513 data points with no NaNs - good!
If you ever worked with vectorbt, you would know that vectorbt loves the data to be supplied with symbols as columns - one per backtest - rather than features as columns. Since SuperTrend depends upon the high, low, and close price, let's get those three features as separate DataFrames using Data.get:
Hint
To get a column of a particular symbol as a Series, use data.get('Close', 'BTCUSDT').
We're all set to design our first SuperTrend indicator!
SuperTrend is a trend-following indicator that uses Average True Range (ATR) and median price to define a set of upper and lower bands. The idea is rather simple: when the close price crosses above the upper band, the asset is considered to be entering an uptrend, hence a buy signal. When the close price crosses below the lower band, the asset is considered to have exited the uptrend, hence a sell signal. 
Unlike the idea, the calculation procedure is anything but simple:
Even though the basic bands can be well computed using the standard tools, you'll certainly get a headache when attempting to do this for the final bands. The consensus among most open-source solutions is to use a basic Python for-loop and write the array elements one at a time. But is this scalable? We're here to find out!
Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool. Since it's a go-to library for processing data in Python, let's write our first implementation using Pandas alone. It will take one column and one combination of parameters, and return four arrays: one for the SuperTrend (trend), one for the direction (dir_), one for the uptrend (long), and one for the downtrend (short). We'll also split the implementation into 5 parts for readability and to be able to optimize any component at any time:
Let's run the supertrend function on the BTCUSDT symbol:
If you print out the head of the supert Series using supert.head(10), you'll notice that the first 6 data points are all NaN. This is because the ATR's rolling period is 7, so the first 6 computed windows contained incomplete data.
A graph is worth 1,000 words. Let's plot the first month of data (January 2020):

We've generated and visualized the SuperTrend values, but what about performance? Can we already make our overfitting machine with thousands of parameter combinations rolling? Not so fast. As you might have guessed, the supertrend function takes some time to compute:
Ouch! Doing 1000 backtests would take us roughly 33 minutes. 
Let's hear what Pandas TA has to say about this:
That's a 3x speedup, mostly due to the fact that Pandas TA uses ATR from TA-Lib. 
Is it now acceptable? Of course not  Can we get better than this? Hell yeah!
Pandas shines whenever it comes to manipulating heterogeneous tabular data, but is this really applicable to indicators? You might have noticed that even though we used Pandas, none of the operations in any of our newly defined functions makes use of index or column labels. Moreover, most indicators take, manipulate, and return arrays of the same dimensions and shape, which makes indicator development a purely algebraic challenge that can be well decomposed into multiple vectorized steps or solved on the per-element basis (or both!). Given that Pandas just extends NumPy and the latter is considered as a faster (although lower level) package, let's adapt our logic to NumPy arrays instead.
Both functions get_med_price and get_basic_bands are based on basic arithmetic computations such as addition and multiplication, which are applicable to both Pandas and NumPy arrays and require no further changes. But what about get_atr and get_final_bands? The former can be re-implemented using NumPy and vectorbt's own arsenal of Numba-compiled functions:
The latter, on the other hand, is an iterative algorithm - it's rather a poor fit for NumPy and an ideal fit for Numba, which can easily run for-loops at a machine code speed:
If you look at the function above, you'll notice that 1) it's a regular Python code that can run even without being decorated with @njit, and 2) it's almost identical to the implementation with Pandas - the main difference is in each iloc[...] being replaced by [...]. We can write a simple Python function that operates on constants and NumPy arrays, and Numba will try to make it much faster, fully automatically. Isn't that impressive? 
Let's look at the result of this refactoring:
Info
When executing a Numba-decorated function for the first time, it may take longer due to compilation.
As expected, those are arrays similar to the ones returned by the supertrend function, just without any labels. To attach labels, we can simply do:
Wondering how much our code has gained in performance? Wonder no more:
That's a 780x speedup over an average Pandas TA run 
If you think that this result cannot be topped, then apparently you haven't worked with TA-Lib. Even though there is no SuperTrend indicator available in TA-Lib, we can still use its highly-optimized indicator functions for intermediate calculations. In particular, instead of reinventing the wheel and implementing the median price and ATR functionality from scratch, we can use the MEDPRICE and ATR TA-Lib functions respectively. They have two major advantages over our custom implementation:
Another 4x improvement - by the time another trader processed a single column of data, we would have processed around 3 thousand columns. Agreed, the speed of our indicator is slowly getting ridiculously high 
Let's stop here and ask ourselves: why do we even need such a crazy performance? 
That's when parameter optimization comes into play. The two parameters that we have - period and multiplier - are the default values commonly used in technical analysis. But what makes those values universal and how do we know whether there aren't any better values for the markets we're participating in? Imagine having a pipeline that can backtest hundreds or even thousands of parameters and reveal configurations and market regimes that correlate better on average?
IndicatorFactory is a vectorbt's own powerhouse that can make any indicator function parametrizable. To get a better idea of what this means, let's supercharge the faster_supertrend_talib function:
The indicator factory is a class that can generate so-called indicator classes. You can imagine it being a conveyor belt that can take a specification of your indicator function and produce a stand-alone Python class for running that function in a very flexible way. In our example, when we called vbt.IF(...), it has internally created an indicator class SuperTrend, and once we supplied faster_supertrend_talib to IndicatorFactory.with_apply_func, it attached a method SuperTrend.run for running the indicator. Let's try it out!
Notice how our SuperTrend indicator magically accepted two-dimensional Pandas arrays, even though the function itself can only work on one-dimensional NumPy arrays. Not only it computed the SuperTrend on each column, but it also converted the resulting arrays back into the Pandas format for pure convenience. So, how does all of this impact the performance?
Not that much! With all the pre- and postprocessing taking place, the indicator needs roughly one millisecond to process one column (that is, 17k data points).
If you think that calling vbt.IF(...) and providing input_names, param_names, and other information manually is too much work, well, vectorbt has something for you. Our faster_supertrend_talib is effectively a black box to the indicator factory - that's why the factory cannot introspect it and derive the required information programmatically. But it easily could if we converted faster_supertrend_talib into an expression! 
Expressions are regular strings that can be evaluated into Python code. By giving such a string to IndicatorFactory.from_expr, the factory will be able to see what's inside, parse the specification, and generate a full-blown indicator class.
Hint
Instance methods with the prefix with (such as with_apply_func) require the specification to be provided manually, while class methods with the prefix from (such as from_expr) can parse this information automatically.
Here's an expression for faster_supertrend_talib:
Using annotations with @ we tell the factory how to treat specific variables. For instance, any variable with the prefix @talib gets replaced by the respective TA-Lib function that has been upgraded with broadcasting and multidimensionality. You can also see that parameters were annotated with @p, while inputs and outputs weren't annotated at all - the factory knows exactly that high is the high price, while the latest line apparently returns 4 output objects.
For more examples, see the documentation on expression parsing.
By the way, this is exactly how WorldQuant's Alphas are implemented in vectorbt. Never stop loving Python for the magic it enables 
Remember how we previously plotted SuperTrend? We had to manually select the date range from each output array and add it to the plot by passing the figure around. Let's subclass SuperTrend and define a method plot that does all of this for us:
But how are we supposed to select the date range to plot? Pretty easy: the indicator factory made SuperTrend indexable just like any regular Pandas object! Let's plot the same date range and symbol but slightly change the color palette:

Beautiful!
Backtesting is usually the simplest step in vectorbt: convert the indicator values into two signal arrays - entries and exits - and supply them to Portfolio.from_signals. To make the test better reflect the reality, let's do several adjustments. Since we're calculating the SuperTrend values based on the current close price and vectorbt executes orders right away, we'll shift the execution of the signals by one tick forward:
We'll also apply the commission of 0.1%:
We've got a portfolio with two columns that can be analyzed with numerous built-in tools. For example, let's calculate and display the statistics for the ETHUSDT symbol:
Optimization in vectorbt can be performed in two ways: iteratively and column-wise.
The first approach involves a simple loop that goes through every combination of the strategy's parameters and runs the whole logic. This would require you to manually generate a proper parameter grid and concatenate the results for analysis. On the upside, you would be able to use Hyperopt and other tools that work on the per-iteration basis.
The second approach is natively supported by vectorbt and involves stacking columns. If you have 2 symbols and 5 parameters, vectorbt will generate 10 columns in total - one for each symbol and parameter, and backtest each column separately without leaving Numba (that's why most functions in vectorbt are specialized in processing two-dimensional data, by the way). Not only this has a huge performance benefit for small to medium-sized data, but this also enables parallelization with Numba and presentation of the results in a Pandas-friendly format.
Let's test the period values 4, 5, ..., 20, and the multiplier values 2, 2.1, 2.2, ..., 4, which would yield 336 parameter combinations in total. Since our indicator is now parametrized, we can pass those two parameter arrays directly to the SuperTrend.run method by also instructing it to do the Cartesian product using the param_product=True flag:
 
Iteration 672/672

The indicator did 672 iterations - 336 per symbol. Let's see the columns that have been stacked:
Each of the DataFrames has now 672 columns. Let's plot the latest combination by specifying the column as a regular tuple:

When stacking a huge number of columns, make sure that you are not running out of RAM. You can print the size of any pickleable object in vectorbt using the Pickleable.getsize method:
Which can be manually calculated as follows (without inputs and parameters):
Hint
To reduce the memory footprint, change the get_final_bands_nb function to produce the output arrays with a lesser floating point accuracy, such as np.float32 or even np.float16.
The backtesting part remains the same, irrespective of the number of columns:
Instead of computing all the statistics for each single combination, let's plot a heatmap of their Sharpe values with the periods laid out horizontally and the multipliers laid out vertically. Since we have an additional column level that contains symbols, we'll make it a slider:

We now have a nice overview of any parameter regions that performed well during the backtesting period, yay! 
Hint
To see how those Sharpe values perform against holding:
 Python code  Notebook
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
Signals are an additional level of abstraction added on top of orders: instead of specifying every bit of information on what needs to be ordered at each timestamp, we can first decide on what a typical order should look like, and then choose the timing of issuing such an order. The latter decision process can be realized through signals, which in the vectorbt's world are represented by a boolean mask where True means "order" and False means "no order". Additionally, we can change the meaning of each signal statically, or dynamically based on the current simulation state; for example, we can instruct the simulator to ignore an "order" signal if we're already in the market, which cannot be done by using the "from-orders" method alone. Finally, vectorbt loves data science, and so comparing multiple strategies with the same trading conditions but different signal permutations (i.e., order timings and directions) is much easier, less error-prone, and generally leads to fairer experiments.
Since we constantly buy and sell things, the ideal scenario would be to incorporate an order direction into each signal as well. But we cannot represent three states ("order to buy", "order to sell", and "no order") by using booleans - a data type with just two values. Thus, signals are usually distributed across two or more boolean arrays, where each array represents a different decision dimension. The most popular way to define signals is by using two direction-unaware arrays:  entries and  exits. Those two arrays have a different meaning based on the direction specified using a separate variable. For instance, when only the long direction is enabled, an entry signal opens a new long position and an exit signal closes it; when both directions are enabled, an entry signal opens a new long position and an exit signal reverses it to open a short one. To better control the decision on whether to reverse the current position or just close it out, we can define four direction-aware arrays:  long entries,  long exits,  short entries, and  short exits, which guarantees the most flexibility.
For example, to open a long position, close it, open a short position, and reverse it, the signals would look like this:
The same strategy can be also defined using an entry signal, an exit signal, and a direction:
Info
Direction-unaware signals can be easily translated into direction-aware signals:
But direction-aware signals cannot be translated into direction-unaware signals if both directions are enabled and there is an exit signal present:
Thus, we need to evaluate in detail which conditions we're interested in before generating signals.
But why not choosing an integer data type where a positive number means "order to buy", negative number means "order to sell", and zero means "no order", like done in backtrader, for example? Boolean arrays are much easier to generate and maintain by the user, but also, a boolean NumPy array requires 8x less memory than a 64-bit signed integer NumPy array. Furthermore, it's so much more convenient to combine and analyze masks than integer arrays! For example, we can use the logical OR (| in NumPy) operation to combine two masks, or sum the elements in a mask to get the number of signals since booleans are a subtype of integers and behave just like regular integers in most math expressions.
Generating signals properly can sometimes be orders of magnitude more difficult than simulating them. This is because we have to take into account not only their distribution, but also how they interact across multiple boolean arrays. For example, setting both an entry and an exit at the same timestamp will effectively eliminate both. That's why vectorbt deploys numerous functions and techniques to support us in this regard.
Signal generation usually starts with comparing two or more numeric arrays. Remember that by comparing entire arrays, we're iterating over each row and column (= element) in a vectorized manner, and compare their scalar values at that one element. So, essentially, we're just running the same comparison operation on each single element across all the arrays that are being compared together. Let's start our first example with Bollinger Bands run on two separate assets. At each timestamp, we'll place a signal whenever the low price is below the lower band, with an expectation that the price will reverse back to its rolling mean:
This operation has generated a mask that has a true value whenever the low price dips below the lower band. Such an array can already be used in simulation! But let's see what happens when we try to compare the lower band that has been generated for multiple combinations of the (upper and lower) multiplier:
The problem lies in Pandas being unable to compare DataFrames with different columns - the left DataFrame contains the columns BTCUSDT and ETHUSDT while the right DataFrame coming from the Bollinger Bands indicator now contains the columns (2, 2, BTCUSDT), (2, 2, ETHUSDT), (3, 3, BTCUSDT), and (3, 3, ETHUSDT). So, what's the solution? Right - vectorbt! By appending vbt to the left operand, we are comparing the accessor object of type BaseAccessor instead of the DataFrame itself. This will trigger the so-called magic method __lt__ of that accessor, which takes the DataFrame under the accessor and the DataFrame on the right, and combines them with BaseAccessor.combine and numpy.less as combine_func. This, in turn, will broadcast the shapes and indexes of both DataFrames using the vectorbt's powerful broadcasting mechanism, effectively circumventing the limitation of Pandas.
As the result, vectorbt will compare (2, 2, BTCUSDT) and (3, 3, BTCUSDT) only with BTCUSDT and (2, 2, ETHSDT) and (3, 3, ETHSDT) only with ETHSDT, and this using NumPy - faster!
Note
For vectorbt to be able to compare shapes that are not broadcastable, both DataFrames must have at least one column level in common, such as symbol that we had above.
As you might have recalled from the documentation on indicators, each indicator attaches a couple of helper methods for comparison - {name}_above, {name}_equal, and {name}_below, which do basically the same as we did above:
To compare a numeric array against two or more scalar thresholds (making them parameter combinations), we can use the same approach by either appending vbt, or by calling the method BaseAccessor.combine. Let's calculate the bandwidth of our single-combination indicator, which is the upper band minus the lower band divided by the middle band, and check whether it's higher than two different thresholds:
The latest example works also on arrays instead of scalars. Or, we can use pandas.concat to manually stack the results of any comparison to treat them as separate combinations:
So far we have touched basic vectorized comparison operations, but there is one operation that comes disproportionally often in technical analysis: crossovers. A crossover refers to a situation where two time series cross each other. There are two ways of finding the crossovers: naive and native. The naive approach compares both time series in a vectorized manner and then selects the first True value out of each "partition" of True values. A partition in the vectorbt's vocabulary for signal processing is just a bulk of consecutive True values produced by the comparison. While we already know how to do the first operation, the second one can be achieved with the help of the accessor for signals - SignalsAccessor, accessible via the attribute vbt.signals on any Pandas object.
In particular, we will be using the method SignalsAccessor.first, which takes a mask, assigns a rank to each True value in each partition using SignalsAccessor.pos_rank (enumerated from 0 to the length of the respective partition), and then keeps only those True values that have the rank 0. Let's get the crossovers of the lower price dipping below the lower band:
To make sure that the operation was successful, let's plot the BTCUSDT column of both time series using GenericAccessor.plot and the generated signals using SignalsSRAccessor.plot_as_markers:

Hint
To wait for a confirmation, use SignalsAccessor.nth to select the n-th signal in each partition.
But here's the catch: if the first low value is already below the first lower band value, it will also yield a crossover signal. To fix that, we need to pass after_false=True, which will discard the first partition if there is no False value before it.
And here's another catch: if the first bunch of values in the indicator are NaN, which results in False values in the mask, and the first value after the last NaN yields True, then the after_false argument becomes ineffective. To account for this, we need to manually set those values in the mask to True. Let's illustrate this issue on sample data:
Or, we can remove the buffer, do the operation, and then add the buffer back:
Info
We can apply the buffer-exclusive approach introduced above to basically any operation in vectorbt.
But here comes another issue: what happens if our data contains gaps and we encounter a NaN in the middle of a partition? We should make the second part of the partition False as forward-filling that NaN value would make waiting for a confirmation problematic. But also, doing so many operations on bigger arrays just for getting the crossovers is quite resource-expensive. Gladly, vectorbt deploys its own Numba-compiled function crossed_above_nb for finding the crossovers in an iterative manner, which is the second, native way. To use this function, we can use the methods GenericAccessor.crossed_above and GenericAccessor.crossed_below, accessible via the attribute vbt on any Pandas object:
Info
If the time series crosses back during the confirmation period wait, the signal won't be set. To set the signal anyway, use forward shifting.
As with other comparison methods, each indicator has the helper methods {name}_crossed_above and {name}_crossed_below for generating the crossover masks:
Once we've generated two or more masks (conditions), we can combine them into a single mask using logical operators. Common logical operators include 
Note
Do not use and, or, or not on arrays - they only work on single boolean values! For example, instead of mask1 and mask2 use mask1 & mask2, instead of mask1 or mask2 use mask1 | mask2, and instead of not mask use ~mask.
For example, let's combine four conditions for a signal: the low price dips below the lower band AND the bandwidth is above some threshold (= a downward breakout while expanding), OR, the high price rises above the upper band AND the bandwidth is below some threshold (= an upward breakout while squeezing):
To test multiple thresholds and to broadcast exclusively using vectorbt:
Combining two or more arrays using a Cartesian product is a bit more complex since every array has the column level symbol that shouldn't be combined with itself. But here's the trick. First, convert the columns of each array into their integer positions. Then, split each position array into "blocks" (smaller arrays). Blocks will be combined with each other, but the positions within each block won't; that is, each block acts as a parameter combination. Combine then all blocks using a combinatorial function of choice (see itertools for various options, or generate_param_combs), and finally, flatten each array with blocks and use it for column selection. Sounds complex? Yes. Difficult to implement? No!
In newer versions of VBT the same effect can be achieved with a single call of BaseAccessor.cross:
But probably an easier and less error-prone approach would be to build an indicator that would handle parameter combinations for us  
For this, we will write an indicator expression similar to the code we wrote for a single parameter combination, and use IndicatorFactory.from_expr to auto-build an indicator by parsing that expression. The entire logic including the specification of all inputs, parameters, and outputs is encapsulated in the expression itself. We'll use the annotation @res_talib_bbands to resolve the specification of the inputs and parameters expected by the TA-Lib's BBANDS indicator and "copy" them over to our indicator by also prepending the prefix talib to the parameter names. Then, we will perform our usual signal generation logic by substituting the custom parameters cond2_th and cond4_th with their single values, and return the whole thing as an output mask annotated accordingly.
Info
Even though the indicator factory has "indicator" in its name, we can use it to generate signals just as well. This is because signals are just boolean arrays that also guarantee to be of the input shape.
To compare the current value to any previous (not future!) value, we can use forward shifting. Also, we can use it to shift the final mask to postpone the order execution. For example, let's generate a signal whenever the low price dips below the lower band AND the bandwidth change (i.e., the difference between the current and the previous bandwidth) is positive:
Important
Never attempt to shift backwards to avoid the look-ahead bias! Use either a positive number in DataFrame.shift, or the vectorbt's accessor method GenericAccessor.fshift.
Another way to shift observations is by selecting the first observation in a rolling window. This is particularly useful when the rolling window has a variable size, for example, based on a frequency. Let's do the same as above but determine the change in the bandwidth in relation to one week ago instead of yesterday:
Hint
Using variable windows instead of fixed ones should be preferred if your data has gaps.
The approach above is a move in the right direction, but it introduces two potential issues: all windows will be either 6 days long or less, while the performance of rolling and applying such a custom Python function using Pandas is not satisfactory, to say the least. The first issue can be solved by rolling a window of 8 days, and checking the timestamp of the first observation being exactly 7 days behind the current timestamp:
The second issue can be solved by looping with Numba. However, the main challenge lies in solving those two issues simultaneously because we want to access the timestamp of the first observation, which requires us to work on a Pandas Series instead of a NumPy array, and Numba cannot work on Pandas Series 
Thus, we will use the vectorbt's accessor method GenericAccessor.rolling_apply, which offers two modes: regular and meta. The regular mode rolls over the data of a Pandas object just like Pandas does it, and does not give us any information about the current window  The meta mode rolls over the metadata of a Pandas object, so we can easily select the data from any array corresponding to the current window 
And if this approach (rightfully) intimidates you, there is a dead simple method GenericAccessor.ago, which is capable of forward-shifting the array using any delta:
Hint
This method returns exact matches. In a case where the is no exact match, the value will be NaN. To return the previous index value instead, pass method="ffill". The method also accepts a sequence of deltas that will be applied on the per-element basis.
But what if we want to test whether a certain condition was met during a certain period of time in the past? For this, we need to create an expanding or a rolling window, and do truth value testing using numpy.any or numpy.all within this window. But since Pandas doesn't implement the rolling aggregation using any and all, we need to be more creative and treat booleans as integers: use max for a logical OR and min for a logical AND. Also, don't forget to cast the resulting array to a boolean data type to generate a valid mask.
Let's place a signal whenever the low price goes below the lower band AND there was a downward crossover of the close price with the middle band in the past 5 candles:
Note
Be cautious when setting min_periods to a higher number and converting to a boolean data type: each NaN will become True. Thus, at least replace NaNs with zeros before casting.
If the window size is fixed, we can also use GenericAccessor.rolling_any and GenericAccessor.rolling_all, which are tailored for computing rolling truth testing operations:
Another way of doing the same rolling operations is by using the accessor method GenericAccessor.rolling_apply and specifying reduce_func_nb as "any" or "all" string. We should use the argument wrap_kwargs to instruct vectorbt to fill NaNs with False and change the data type. This method allows flexible windows to be passed. Again, let's roll a window of 5 days:
Let's do something more complex: check whether the bandwidth contracted to 10% or less at any point during a month using an expanding window, and reset the window at the beginning of the next month; this way, we make the first timestamp of the month a time anchor for our condition. For this, we'll overload the vectorbt's resampling logic, which allows aggregating values by mapping any source index (anchor points in our example) to any target index (our index).

We can observe how the signal for the bandwidth touching the 10% mark propagates through each month, and then the calculation gets reset and repeated.
To set signals periodically, such as at 18:00 of each Tuesday, we have multiple options. The first approach involves comparing various attributes of the source and target datetime. For example, to get the timestamps that correspond to each Tuesday, we can compare pandas.DatetimeIndex.weekday to 1 (Monday is 0 and Sunday is 6):
Now, we need to select only those timestamps that happen at one specific time:
Since each attribute comparison produces a mask, we can get our signals by pure logical operations. Let's get the timestamps that correspond to each Tuesday 17:30 by comparing the weekday of each timestamp to Tuesday AND the hour of each timestamp to 17 AND the minute of each timestamp to 30:
As we see, both conditions combined produced no exact matches because our index is hourly. But what if we wanted to get the previous or next timestamp if there was no exact match? Clearly, the approach above wouldn't work. Instead, we'll use the function pandas.Index.get_indexer, which takes an array with index labels, and searches for their corresponding positions in the index. For example, let's get the position of August 7th in our index:
But looking for an index that doesn't exist will return -1:
Warning
Do not pass the result for indexing if there is a possibility of no match. For example, if any of the returned positions is -1 and it's used in timestamp selection, the position will be replaced by the latest timestamp in the index.
To get either the exact match or the previous one, we can pass method='ffill'. Conversely, to get the next one, we can pass method='bfill':
Returning to our example, we need first to generate the target index for our query, which we're about to search in the source index: use the function pandas.date_range to get the timestamp of each Tuesday midnight, and then add a timedelta of 17 hours and 30 minutes. Next, transform the target index into positions (row indices) at which our signals will be placed. Then, we extract the Pandas symbol wrapper from our data instance and use it to fill a new mask that has the same number of columns as we have symbols. Finally, set True at the generated positions of that mask:
Let's make sure that all signals match 18:00 on Tuesday, which is the first date after the requested 17:30 on Tuesday in an hourly index:
The above solution is only required when only a single time boundary is known. For example, if we want 17:30 on Tuesday or later, we know only the left boundary while the right boundary is infinity (or we might get no data point after this datetime at all). When both time boundaries are known, we can easily use the first approach and combine it with the vectorbt's signal selection mechanism. For example, let's place a signal at 17:00 on Tuesday or later, but not later than 17:00 on Wednesday. This would require us placing signals from the left boundary all the way to the right boundary, and then selecting the first signal out of that partition:
The third and final approach is the vectorbt's one 
It's relying on the two accessor methods BaseAccessor.set and BaseAccessor.set_between, which allow us to conditionally set elements of an array in a more intuitive manner.
Place a signal at 17:30 each Tuesday or later:
Place a signal after 18:00 each Tuesday (exclusive):
Fill signals between 12:00 each Monday and 17:00 each Tuesday:
Place a signal exactly at the midnight of January 7th, 2021:
Fill signals between 12:00 on January 1st/7th and 12:00 on January 2nd/8th, 2021:
Fill signals in the first 2 hours of each week:
See the API documentation for more examples.
With Numba, we can write an iterative logic that performs just as well as its vectorized counterparts. But which approach is better? There is no clear winner, although using vectors is an overall more effective and user-friendlier approach because it abstracts away looping over data and automates various mechanisms associated with index and columns. Just think about how powerful the concept of broadcasting is, and how many more lines of code it would require implementing something similar iteratively. Numba also doesn't allow us to work with labels and complex data types, only with numeric data, which requires skills and creativity in designing (efficient!) algorithms. 
Moreover, most vectorized and also non-vectorized but compiled functions are specifically tailored at one specific task and perform it reliably, while writing an own loop makes you responsible to implement every bit of the logic correctly. Vectors are like Lego bricks that require almost zero effort to construct even the most breathtaking castles, while custom loops require learning how to design each Lego brick first 
Nevertheless, the most functionality in vectorbt is powered by loops, not vectors - we should rename vectorbt to loopbt, really  The main reason is plain and simple: most of the operations cannot be realized through vectors because they either introduce path dependencies, require complex data structures, use intermediate calculations or data buffers, periodically need to call a third-party function, or all of these together. Another reason is certainly efficiency: we can design algorithms that loop of the data only once, while performing the same logic using vectors would read the whole data sometimes a dozen of times. The same goes for memory consumption! Finally, defining and running a strategy at each time step is exactly how we would proceed in the real world (and in any other backtesting framework too), and we as traders should strive to mimic the real world as closely as possible.
Enough talking! Let's implement the first example from Logical operators using a custom loop. Unless our signals are based on multiple assets or some other column grouping, we should always start with one column only:
We've got the same number of signals as previously - magic!
To make the function work on multiple columns, we can then write another Numba-compiled function that iterates over columns and calls generate_mask_1d_nb on each:
Probably a more "vectorbtonic" way is to create a stand-alone indicator where we can specify the function and what data it expects and returns, and the indicator factory will take care of everything else for us!
But what about shifting and truth value testing? Simple use cases such as fixed shifts and windows can be implemented quite easily. Below, we're comparing the current value to the value some number of ticks before:
Important
Don't forget to check whether the element you query is within the bounds of the array. Unless you turned on the NUMBA_BOUNDSCHECK mode, Numba won't raise an error if you accessed an element that does not exist. Instead, it will quietly proceed with the calculation, and at some point your kernel will probably die. In such a case, just restart the kernel, disable Numba or enable the bounds check, and re-run the function to identify the bug.
And here's how to test if any condition was true inside a fixed window (= variable time interval):
As soon as dates and time are involved, such as to compare the current value to the value exactly 5 days ago, a better approach is to pre-calculate as many intermediate steps as possible. But there is also a possibility to work with a datetime-like index in Numba directly. Here's how to test if any condition was true inside a variable window (= fixed time interval):
Hint
Generally, it's easier to design iterative functions using regular Python and only compile them with Numba if they were sufficiently tested, because it's easier to debug things in Python than in Numba.
Remember that Numba (and thus vectorbt) has far more features for processing numeric data than datetime/timedelta data. But gladly, datetimee/timedelta data can be safely converted into integer data outside Numba, and many functions will continue to work just as before:
Why so? By converting a datetime/timedelta into an integer, we're extracting the total number of nanoseconds representing that object. For a datetime, the integer value becomes the number of nanoseconds after the Unix Epoch, which is 00:00:00 UTC on 1 January 1970:
Writing own loops is powerful and makes fun, but even here vectorbt has functions that may make our life easier, especially for generating signals. The most flexible out of all helper functions is the Numba-compiled function generate_nb and its accessor class method SignalsAccessor.generate, which takes a target shape, initializes a boolean output array of that shape and fills it with Falsevalues, then iterates over the columns, and for each column, it calls a so-called "placement function" - a regular UDF that changes the mask in place. After the change, the placement function should return either the position of the last placed signal or -1 for no signal.
All the information about the current iteration is being passed via a context of the type GenEnContext, which contains the current segment of the output mask that can be modified in place, the range start (inclusive) that corresponds to that segment, the range end (exclusive), column, but also the full output mask for the user to be able to make patches wherever they want. This way, vectorbt abstracts away both preparing the array and looping over the columns, and assists the user in selecting the right subset of the output data to modify.
Let's place a signal at 17:00 (UTC) of each Tuesday:
Info
Segments in generate_nb are always the entire columns.
But our index is a daily index, thus there can't be any signal. Instead, let's place a signal at the next possible timestamp:
The most fascinating part about the snippet above is that the entire datetime logic is being performed using just regular integers!
Important
When being converted into the integer format, the timezone of each datetime object is effectively converted to UTC, thus make sure that any value compared to the UTC timestamp is also in UTC.
But what about multiple parameter combinations? We cannot pass the function above to the indicator factory because it doesn't look like an apply function. But vectorbt's got our back! There is an entire subclass of the indicator factory tailed at signal generation - SignalFactory. This class supports multiple generation modes that can be specified using the argument mode of the type FactoryMode. In our case, the mode is FactoryMode.Entries because our function generates signals based on the target shape only, and not based on other signal arrays. Furthermore, the signal factory accepts any additional inputs, parameters, and in-outputs to build the skeleton of our future indicator class.
The signal factory has the class method SignalFactory.with_place_func comparable to from_apply_func we've got used to. In fact, it takes a placement function and generates a custom function that does all the pre- and post-processing around generate_nb (note that other modes have other generation functions). This custom function, for example, prepares the arguments and assigns them to their correct positions in the placement function call. It's then forwarded down to IndicatorFactory.with_custom_func. As a result, we receive an indicator class with a run method that can be applied on any user-defined shape and any grid of parameter combinations. Sounds handy, right?
Let's parametrize our exact-match placement function with two parameters: weekday and hour.
Note
The mode FactoryMode.Entries doesn't mean that we are forced to generate signals that must strictly act as entries during the simulation - we can generate any mask, also exits if they don't depend on entries.
The indicator function was able to match all midnight times but none afternoon times, which makes sense because our index is daily and thus contains midnight times only. We can easily plot the indicator using the attached plot method, which knows how to visualize each mode:

After populating the position entry mask, we should decide on the position exit mask. When exits do not rely on entries, we can use the generator introduced above. In other cases though, we might have a logic that makes an exit signal fully depend on the entry signal. For example, an exit signal representing a stop loss exists solely because of the entry signal that defined that stop loss condition. There is also no guarantee that an exit can be found for an entry at all. Thus, this mode should only be used for cases where entries do not depend on exits, but exits depend on entries. The generation is then done using the Numba-compiled function generate_ex_nb and its accessor instance method SignalsAccessor.generate_exits. The passed context is now of the type GenExContext and also includes the input mask and various generator-related arguments.
The generator takes an entry mask array, and in each column, it visits each entry signal and calls a UDF to place one or more exit signals succeeding it. Do you recall how we had to accept from_i and to_i in the placement functions above? The previous mode always passed 0 as from_i and len(index) as to_i because we had all the freedom to define our signals across the entire column. Here, the passed from_i will usually be the next index after the previous entry, while the passed to_i will usually be the index of the next entry, thus effectively limiting our decision field to the space between each pair of entries.
Warning
Beware that knowing the position of the next entry signal may introduce the look-ahead bias. Thus, use it only for iteration purposes, and never set data based on to_i!
Let's generate an entry each quarter and an exit at the next date:
We can control the distance to the entry signal using wait, which defaults to 1. Let's instruct vectorbt to start each segment at the same timestamp as the entry:
And below is how to implement a variable waiting time based on a frequency. Let's wait exactly 7 days before placing an exit:
But what happens with the exit condition for the previous entry if the next entry is less than 7 days away? Will the exit still be placed? No!
By default, each segment is limited by the two entries surrounding it. To make it infinite, we can disable until_next:
Note
In such a case, we might be unable to identify which exit belongs to which entry. Moreover, two or more entries may generate an exit at the same timestamp, so beware!
In the case above, the generated signals follow the following schema: entry1, entry2, exit1, entry3, exit2, and so on. Whenever those signals are passed to the simulator, it will execute entry1 and ignore entry2 because there was no exit prior to it - we're still in the market. It will then rightfully execute exit1. But then, it will open a new position with entry3 and close it with exit2 right after, which was originally designed for entry2 (that has been ignored). To avoid this mistake, we should enable skip_until_exit to avoid processing any future entry signal that comes before an exit for any past entry signal. This would match the simulation order.
Note
Make sure to use skip_until_exit always in conjunction with disabled until_next.
Finally, to make the thing parametrizable, we should use the mode FactoryMode.Exits and provide any supporting information with the prefix exit:
We can then remove redundant entries if wanted:
After that, each exit is guaranteed to come after the entry it was generated for.
Instead of dividing the entry and exit signal generation parts, we can merge them. This is particularly well-suited for a scenario where an exit depends on an entry but also an entry depends on an exit. This kind of logic can be realized through the Numba-compiled function generate_enex_nb and its accessor class method SignalsAccessor.generate_both. The generation proceeds as follows. First, two empty output masks are created: entries and exits. Then, for each column, the entry placement function is called to place one or more entry signals. The generator then searches for the position of the last generated entry signal, and calls the exit placement function on the segment right after that entry signal. Then, it's the entry placement function's turn again. This process repeats until the column has been traversed completely. The passed context is of the type GenEnExContext and contains all the interesting information related to the current turn and iteration.
Let's demonstrate the full power of this method by placing an entry once the price dips below one threshold, and an exit once the price tops another threshold. The signals will be generated strictly one after another, and the entry/exit price will be the close price.

To parametrize this logic, we need to use the mode FactoryMode.Both. And because our functions require input arrays that broadcast against the input shape, vectorbt won't ask us to provide the input shape but rather determine it from the input arrays automatically:

A chain in the vectorbt's vocabulary is a special ordering of entry and exit signals where each exit comes after exactly one entry and each entry (apart from the first one) comes after exactly one exit. Thus, we can easily identify which exit belongs to which entry and vice versa. The example above is actually a perfect example of a chain because each signal from crossing a threshold is based solely on the latest opposite signal. Now, imagine that we have already generated an array with entries, and each of those entries should exist only if there was an exit before, otherwise it should be ignored. This use case is very similar to FactoryMode.Exits with enabled skip_until_exit and disabled until_next.
But what the mode FactoryMode.Chain proposes is the following: use the generator generate_enex_nb with the entry placement function first_place_nb to select only the first entry signal after each exit, and any user-defined exit placement function. In the end, we will get two arrays: cleaned entries (often new_entries) and exits (exits).
What we should always keep in mind is that entries and exits during the generation phase aren't forced to be used as entries and exits respectively during the simulation. Let's generate entry signals from a moving average crossover each mimicing a limit order, and use an exit placement function to generate signals for executing those limit orders. As a result, we can use those newly generated signals as actual entries during the simulation! If any new "entry" signal comes before the previous "exit" signal, it will be ignored. We'll also track the fill price with another array.
For example, the first limit order for BTCUSDT was placed on 2021-02-04 and filled on 2021-05-19. The first limit order for ETHUSDT was placed on 2021-03-11 and filled on 2021-03-24. To simulate this data, we can pass fill_mask as entries/order size and fill_mask as order price.
Hint
If you want to replace any pending limit order with a new one instead of ignoring it, use FactoryMode.Exits and then select the last input signal before each output signal.
There is an entire range of preset signal generators - here - that are using the modes we discussed above. Preset indicators were set up for one particular task and are ready to be used without having to provide any custom placement function. The naming of those indicators follows a well-defined schema:
You hate randomness in trading? Well, there is one particular use case where randomness is heartily welcomed: trading strategy benchmarking. For instance, comparing one configuration of RSI to another one isn't representative at all since both strategy instances may be inherently bad, and deciding for one is like picking a lesser evil. Random signals, on the other hand, give us an entire new universe of strategies yet to be discovered. Generating a sufficient number of such random signal permutations on a market can reveal the underlying structure and behavior of the market and may answer whether our trading strategy is driven by an edge or pure randomness.
There are two types of random signal generation: count-based and probability-based. The former takes a target number of signals n to place during a certain period of time, and guarantees to fulfill this number unless the time period is too small. The latter takes a probability prob of placing a signal at each timestamp; if the probability is too high, it may place a signal at each single timestamp; if the probability is too low, it may place nothing. Both types can be run using the same accessor method: SignalsAccessor.generate_random to spread entry signals across the entire column, SignalsAccessor.generate_random_exits to spread exit signals after each entry and before the next entry, and SignalsAccessor.generate_random_both to spread entry and exit signals one after another in a chain.
Warning
Generating a specific number of signals may introduce the look-ahead bias because it incorporates the knowledge about the next opposite signal or the column end. Use it with caution, and only when the position of the last to-be-placed signal is known in advance, such as when trading on the per-month basis.
Let's generate a signal once in 10 timestamps on average:
Note
The more signals we generate, the closer is the average neighbor distance to the target average.
Now, let's generate exactly one signal each week. To achieve that, we'll generate an "entry" signal on each Monday, and an "exit" signal acting as our target signal. This won't cause the look-ahead bias because we have defined the bounds of the generation space in advance.
To parametrize the number of signals and the probability, we have at our disposal the indicators starting with the prefix RAND and RPROB respectively. A powerful feature of those indicators is their ability to take both parameters as array-like objects! In particular, we can provide n per column, and prob per column, row, or even element in the target shape. 
Let's gradually generate more signals with time using RPROB! We'll start with the probability of 0% and end with the probability of 100% of placing a signal at each timestamp:

To test multiple values, we can provide them as a list. Let's prove that the fixed probability of 50% yields the same number of signals on average as the one ranging from 0% to 100% (but both are still totally different distributions!):
Stop signals are an essential part of signal development because they allow us to propagate a stop condition throughout time. There are two main stop signal generators offered by vectorbt: a basic one that compares a single time series against any stop condition, and a specialized one that compares candlestick data against stop order conditions common in trading.
The first type can be run using the Numba-compiled function stop_place_nb and its accessor instance method SignalsAccessor.generate_stop_exits. Additionally, there are indicator classes STX and STCX that make the stop parametrizable. Let's use the accessor method to generate take profit (TP) signals. For this, we need four inputs: entry signals (entries), the entry price to apply the stop on (entry_ts), the high price (ts), and the actual stop(s) in % to compare the high price against (stop). We'll use the crossover entries generated previously. We'll also run the method in the chained exits mode to force vectorbt to wait for an exit and remove any entry signals that appear before.
But how do we determine the stop price? Gladly, the Numba-compiled function also accepts a (required) in-output array stop_ts that is being written with the stop price of each exit. By default, vectorbt assumes that we're not interested in this array, and to avoid consuming much memory, it creates an empty (uninitialized) array, passes it to Numba, and deletes it afterwards. To make it return the array, we need to pass an empty dictionary out_dict where the accessor method can put the array. Whenever the out_dict is detected, vectorbt will create a full (initialized) array with np.nan, pass it to Numba, and put it back into the dictionary:
Hint
We could have also passed our own (already created) stop_ts inside out_dict and vectorbt would override only those elements that correspond to exits!
The same can be done with the corresponding indicator class. But let's do something completely different: test two trailing stop loss (TSL) parameters instead, where the condition is following the high price upwards and is met once the low price crosses the stop value downwards. The high price can be specified with the argument follow_ts. The entry price will be the open price (even though we generated them using the close price, let's assume this scenario for a second), and thus we'll also allow placing the first signal at the entry bar by making wait zero:

Note
Waiting time cannot be higher than 1. If waiting time is 0, entry_ts should be the first value in the bar. If waiting time is 1, entry_ts should be the last value in the bar, otherwise the stop could have also been hit at the first bar.
Also, by making the waiting time zero, you may get an entry and an exit at the same bar. Multiple orders at the same bar can only be implemented using a flexible order function or by converting the signals directly into order records. When passed directly to Portfolio.from_signals, any conflicting signals will be ignored.
If we're looking into placing solely SL, TSL, TP, and TTP orders, a more complete approach would be using the full OHLC information, which is utilized by the Numba-compiled function ohlc_stop_place_nb, the accessor instance method SignalsAccessor.generate_ohlc_stop_exits, and the corresponding indicator classes OHLCSTX and OHLCSTCX. The key advantage of this approach is the ability to check for all stop order conditions simultaneously!
Let's generate signals based on a stop loss and trailing stop loss combo of 10% and 15% respectively:

Keep in mind that we don't have intra-candle data. If there was a huge price fluctuation in both directions, we wouldn't be able to determine whether SL was triggered before TP and vice versa. So some assumptions need to be made:
A common tricky situation is when the entry price is the open price and we're waiting one bar. For instance, what would happen if the condition was met during the waiting time? We cannot place an exit signal at the entry bar. Instead, the function waits until the next bar and checks whether the condition is still valid for the open price. If yes, the signal is placed with the stop price being the open price. If not, the function simply waits until the next opportunity arrives. If the stop is trailing, the target price will update just as usual at the entry timestamp. To avoid any logical bugs, it's advised to use the close price as the entry price when wait is one bar (default).
When working with multiple stop types at the same time, we often want to know which exact type was triggered. This information is stored in the array stop_type (machine-readable) and stop_type_readable (human-readable):
All the stop types are listed in the enumerated type StopType.
Both stop signal generation modes are very flexible towards inputs. For example, if any element in the arrays ts and follow_ts in the first mode is NaN (default), it will be substituted by the element in entry_ts. If only an element in follow_ts is NaN, it will be substituted by the minimum or maximum (depending on the sign of the stop value) of the element in both other arrays. Similarly, in the second mode, we can provide only entry_price and vectorbt will auto-populate the open price if is_entry_open is enabled and the close price otherwise. Without high, vectorbt will take the maximum out of open and close. Generally, we're not forced to provide every bit of information apart from the entry price, but it's in our best interest to provide as much information as we can to make best decisions and to closely mimic the real world.
For example, let's run the same as above but specify the entry price only:

The same flexibility goes for parameters: similarly to the behavior of the probability parameter in random signal generators, we can pass each parameter as an array, such as one element per row, column, or even element. Let's treat each second entry as a short entry and thus reverse the trailing take profit (TTP) logic for it:

We can then split both final arrays into four direction-aware arrays for simulation:

Seems like all trades are winning, thanks to a range-bound but still volatile market 
 Python code  Notebook
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
Instead of building our custom simulator from scratch, we can make use of one of the preset simulation methods offered by vectorbt. There are three predominantly used methods: Portfolio.from_orders, Portfolio.from_signals, and Portfolio.from_order_func. Each one has its own benefits and drawbacks, but all of them follow the same iteration schema that we discussed previously: iterate over the rows and columns, and at each step, convert the current element of all the input data passed by the user into an order request, and process it by updating the current simulation state and by appending the filled order record to an array. This array along with other information can later be used during the reconstruction phase to analyze the simulation.
Portfolio.from_orders is the most basic method out of three: it doesn't take any UDFs, and allows us to provide every bit of information on orders as separate, broadcastable arrays. Literally each element across all the passed arrays will be converted into an instance of Order and processed as usual. Since the number of orders is limited by the number of elements in the passed arrays, we can issue and execute only one order per timestamp and asset.
For example, passing [0.1, -0.1, np.nan] as the order size array and [11, 10, 12] as the order price array will generate three orders at three subsequent timestamps:
Hint
This method should be used when you know exactly what to order at each timestamp. And, as practice shows, many types of signals and other inputs can be successfully converted into an order format for a nice speedup. For example, if your entries and exits are cleaned (i.e., one exit comes exactly after one entry and vice versa), you can convert them to order size using entries.astype(int) - exits.astype(int), which will order 1 share once a signal is encountered, in both directions.
The backbone of this method is the Numba-compiled function from_orders_nb, which is the fastest simulation function out of three, but also the only one that can be (and is) safely cached in Numba because it doesn't depend on any complex or global data. It's also using flexible indexing, it's registered as a chunkable function, and it can be parallelized by Numba with a single command. 
The arguments of this simulation function include some arguments we've encountered in the documentation on simulation: target shape, group lengths, initial cash, and some others. The arguments we haven't seen before include the call sequence array, initial state arrays (such as init_position), continuous state change arrays (such as cash_deposits), order information arrays (such as size), and various flags for controlling the simulation process (such as save_returns). Also, most arguments have sensitive default values that are consistent across most Numba-compiled functions defined in portfolio.nb.
Every single field in the named tuple Order is present as an argument in the signature of this simulation function, and can be provided in a format suitable for flexible indexing.
Let's simulate the three orders we mentioned above:
The result of the simulation is an instance of the named tuple SimulationOutput, which consists of the filled order and log records, and other supporting information for post-analysis:
Info
Despite the fact that we didn't instruct vectorbt to create an array with log records, cash deposits, and cash earnings, we still see those arrays in the simulation output. Because Numba has difficulties in processing optional writable arrays, we cannot make those outputs None, but instead, we create empty arrays with ultra-small shapes to denote that they should be ignored during post-analysis.
Here's a basic helper function to pretty-print order records:
To apply any information on every element, we can provide a scalar. This works because most simulation methods convert any scalar into a two-dimensional array suitable for flexible indexing automatically! We can also provide any information that should be applied per row, such as price, as a one-dimensional array, like this:
Important
Default broadcasting rules of vectorbt are slightly different from the NumPy broadcasting rules: in NumPy, (3,) will broadcast along columns in the shape (3, 1), but vectorbt assumes that flexible two-dimensional arrays are always time series and thus are provided per row.
If we want to provide any information per column instead, we always need wrap it into a two-dimensional array. We can additionally make this array have only one row to apply information to all rows. Let's test multiple size values by providing them per column, while the price is provided per row:
This has the same effect as if we broadcasted all the arrays prior to passing.
Notice how all the arrays have the same shape (3, 3), which becomes our target shape.
Important
We cannot use np.broadcast_arrays since one-dimensional arrays will broadcast along columns.
Info
Visit the API documentation of Order to learn more about different arguments and their meanings.
If any element in the array group_lens is greater than 1, it's assumed that the columns are grouped and cash sharing is enabled by default. Let's simulate a portfolio with two assets where we try to order as many shares as possible in both assets:
We see that the first asset used up the entire cash and left the second one without funds.
One practical feature of from_orders_nb is that we can provide our own call sequence to change the order in which columns are processed inside their groups. Without providing a call sequence, the default processing order is from left to right. For example, let's perform two rebalancing steps with the default call sequence: allocate 100% to the second asset at the first timestamp, and close out the position and allocate 100% to the first asset at the second timestamp.
We see that only the column 1 has been processed. This is because we attempted to allocate 100% to the first asset without first closing out the position of the second asset at the second timestamp. To account for this order, we can pass our own call sequence:
Info
Order records are partitioned by column and are guaranteed to come in the order they were filled in each column. Also, order ids (the first field) are generated per column, not globally!
The first asset at the second timestamp now has the required funds to go long.
To avoid providing the call sequence manually, we can leave it at None and enable auto_call_seq instead. In such a case, vectorbt will automatically sort the columns by their approximated order value:
Sometimes though, we would like to throw a look at the sequence in which orders were processed, which isn't reflected in the order records. For this, we can provide our own call sequence for vectorbt to change it in-place and return it as a field of the SimulationOutput named tuple. The call sequence array must be pre-filled with indices in the strict order from 0 to n, where n is the length of the particular group of columns. We can easily build such an array using the function build_call_seq, which takes the target shape, the group lengths, and the call sequence type from CallSeqType:
The generated and then sorted call sequence exactly follows the correct one we constructed manually before.
Hint
There is usually no big reason in providing your own call sequence. Setting it to None (default) and turning on the flag auto_call_seq will derive the best call sequence in the most resource-friendly manner.
In addition to returning order records, we can instruct vectorbt to also fill the return based on the portfolio value at the end of each bar. The constructed series will be available under the field returns of SimulationOutput.in_outputs, and can be used to calculate a range of metrics, such as the Sharpe ratio. Let's do the following: pull the BTC price and calculate the return of a simple buy-and-hold strategy. We'll see that the filled returns will closely match the returns calculated from the price directly:
Returns are calculated from the value of each group, thus the number of columns in the returned time series matches the number of groups in group_lens:
Initial state defines the starting conditions of the entire trading environment. It mainly consists of three variables: the initial cash, the initial position, and the average entry price of the initial position. Each variable has at most one dimension and is either defined per column or per group. 
The initial cash is the most important variable out of three, and can be provided as a flexible array init_cash with values per column or per group with cash sharing. As a rule of thumb: it must be either a scalar or one-dimensional array with the same number of elements as there are in group_lens.
Let's create a group of two columns and allocate 50% to each column, and another two groups with one column and 100% allocation each. This way, we can perform two independent backtests: with grouping and without. By supplying the initial cash as a single number ($100 by default), the first group will split it among two assets, and thus the starting condition of the grouped columns will differ from that of the columns without grouping, which isn't ideal if you want to set up fair statistical experiments.
Let's fix this and provide the first group with twice as much capital:
Apart from the initial cash, we can also specify the initial position of each asset to start with. Let's start the simulation with 1 BTC and 1 ETH, and calculate the returns:
The first data point is NaN because vectorbt cannot calculate the initial value of each portfolio instance without knowing the entry price of each initial position. Let's fix this by setting the entry price to the first open price:
Important
Make sure to distinguish between a column and a group!
Columns represent individual assets, and most information in vectorbt must be supplied at this (lowest) level of granularity. Such arrays must broadcast against target_shape. Groups, on the other hand, represent collections of assets that share the same cash, and only some variables related to cash must be provided per group. Such arrays must broadcast against group_lens.shape.
Hint
If you're not sure whether an argument is expected as a one-dimensional or two-dimensional array, take a look into the source code of the function: one-dimensional arrays are annotated with FlexArray1dLike and two-dimensional with FlexArray2dLike. If an argument is expected as strictly one-dimensional or two-dimensional array (i.e., a scalar is not allowed), it's annotated with FlexArray1d and FlexArray2d respectively. If an argument is not flexible at all (i.e., it must have the same shape as target_shape), it will be just Array1d and Array2d respectively.
In addition to providing the initial cash, we can also deposit or withdraw arbitrary cash amounts as the simulation progresses. Similar to init_cash, the argument cash_deposits must be specified per group. The only difference is that the array now should broadcast along rows and columns, and specify the deposited or withdrawn amount at each timestamp. Thanks to flexible indexing, we can specify the amount to be applied per each element, row, group, or to the entire frame. The actual operation then takes place at the beginning of each bar.
Let's simulate a simple DCA strategy where we deposit $100 at the beginning of each year and spend it right away:
Below, we're doing the same but on a group with two assets and equal allocations. Since the array cash_deposits changes the cash balance, it must have the same number of columns as we have groups (with cash sharing).
To withdraw cash, we need to provide a negative amount. If the amount of cash to be withdrawn exceeds the amount of cash we have in our account, only the available cash will be withdrawn. Let's start with 1 BTC, sell 10% each year, and continuously withdraw the entire cash balance:
As we see, on the first date of each year, we sold 10% of our position, and since any changes related to cash are only applied at the beginning of each bar, the cash resulting from the transaction can only be withdrawn on the date that follows. Also, whenever we specify cash deposits, vectorbt will create a full-scale array SimulationOutput.cash_deposits to account for values that couldn't be satisfied, such as -np.inf, which was replaced by the cash balance at each timestamp. Without having this array, we wouldn't be able to properly reconstruct the simulation during the post-analysis phase.
In contrast to cash deposits, cash earnings (cash_earnings) mark cash that is either inflowing to or outflowing away from the user, and so they have a direct effect on the profitability. Also, they are applied at the end of each bar. For example, we can use (negative) cash earnings to charge some fixed commission during a pre-determined period of time, or to simulate profit taking from stacking cryptocurrency. One of the most useful applications of cash earnings are cash dividends, which are expressed by a separate argument cash_dividends. Cash dividends are multiplied by the current position size and added to the cash earnings. Similar to cash deposits, vectorbt creates a separate array for cash earnings once it registers any non-zero value in either cash earnings or cash dividends provided by the user, and writes final operations to this array, available under SimulationOutput.cash_earnings.
Let's simulate investing in Apple and keeping the dividends:
By default, since vectorbt doesn't know the number of order records to be filled in advance, it creates an empty array of the same shape as target_shape and gradually "appends" new records. As soon as there are hundreds of thousands or even millions of elements in the target shape, we may run out of memory by trying to create such a huge empty array with such a complex data type. To avoid stressing our RAM too much, we can specify the maximum number of potential records to be filled in each column using max_order_records for order records and max_log_records for log records. 
For instance, if we have tick data with one million of data points, and we want to simulate a simple strategy where we buy at the first timestamp and then sell at the latest timestamp, it makes sense to limit the number of potential orders in each column to just 2:
Exceeding this limit will throw an error:
We can also completely disable filling order records by setting max_order_records to zero:
Note
max_order_records and max_log_records are effective for each column. If one column expects to generate 2 records and another one to generate 1000 records, you must use the value of 1000. Also, don't reduce the maximum number of log records (apart from setting it to zero) since logs are generated at each single timestamp.
In some simple use cases, there is no point in processing each timestamp if all elements in the size array at that timestamp are NaN. This wouldn't make sense in cases where there is a need to continuously update the current state of the simulation, such as when having to fill returns or to apply user-defined cash deposits and earnings. In other cases though, we can create a "shortcut" that may bring a huge improvement in performance when processing large input arrays. This is possible by enabling skipna, just make sure to disable the procedure of forward-filling the valuation price:
2 milliseconds for one million data points - not bad! 
Every simulation function, including from_orders_nb, is registered as a jittable function in JITRegistry once vectorbt is imported. The so-called "jitable setup" resulting from the registration contains various information on compilation and decoration, such as what arguments were passed to the Numba's @njit decorator. At any point in time, we can instruct the registry to redecorate the function by keeping other decoration arguments at their defaults. For example, to disable Numba and run the simulator as a regular Python function:
To disable caching:
To enable automatic parallelization:
Hint
All the returned functions can be used exactly the same way as from_orders_nb. The two latter functions can also be called from within Numba.
The same goes for chunking: each simulation function is registered as a chunkable function in ChunkableRegistry and all arguments in from_orders_nb are perfectly chunkable across groups (not rows or columns!). But since Numba-compiled functions are meant to be used by other Numba-compiled functions and Numba can't take regular Python functions, they haven't been decorated with the chunked decorator just yet. To decorate, we need to explicitly tell the registry to do so:
Hint
The returned function can be used the same way as from_orders_nb, but not within Numba since it's now wrapped with a regular Python function that takes care of chunking.
Let's go all into BTC and ETH while utilizing chunking, which will produce two totally-isolated simulations. Internally, this will split group_lens into two arrays ([1] and [1]), then split each argument value into chunks such that each chunk contains information only about one of the groups, execute the same function but on different chunks using Dask, and finally, merge the results of both simulations as if they were produced by a single, monolithic simulation 
Chunking even knows how to split flexible arrays!
Often, using the Numba-compiled simulation function directly is quite cumbersome, given that many of our inputs are either scalars or Pandas objects that would have to be converted to NumPy arrays. Nevertheless, knowing how inputs are processed at the most fundamental level is very important for understanding how vectorbt works under the hood. To add another level of abstraction and to assist the user, vectorbt enhances each simulation function by wrapping it with a class method, such as Portfolio.from_orders for from_orders_nb, that automates some input pre-processing and output post-processing.
For example, here's how easy it is to test the three orders we mentioned at the beginning:
But probably the simplest example involves just a single order (for $10, buy 1 share):
Each class method is basically a small pipeline that pulls default argument values from global settings, broadcasts arguments, checks whether arguments have correct data types, redecorates the simulation function by resolving jitting and chunking options, runs the simulation, and, finally, creates a new Portfolio instance based on the result of the simulation and ready to be analyzed. But the most important: it makes use of Pandas, including datetime indexes and column hierarchies - nobody wants to work with NumPy arrays alone!
In contrast to their Numba-compiled functions, the class methods require the close price to be provided. This requirement is associated with the post-analysis phase: many metrics and time series such as the equity curve can only be reliably calculated using the latest price at each bar since they use information that happens somewhere during that bar, and we should avoid looking into the future by using the open price or any other intermediate data.
If we look at the signature of Portfolio.from_orders, we would notice an exceptionally high amount of None values. The value None has a special meaning and usually instructs vectorbt to replace it with the respective default value from the global settings.
The global settings for Portfolio are stored in the config settings.portfolio. For example, by default, vectorbt uses the close price as the order price (remember how the negative infinity means the open price and the positive infinity means the close price?):
To change a default, we can override it directly in the settings. For example, let's introduce a fixed commission of $1 to every order:
The settings can be reset as any other config:
Generally, global defaults follow the same schema as keyword arguments across most simulation functions. For example, price defaults to np.array(np.inf) just about everywhere in Numba. If you cannot find an argument in the global settings, then there is no default value for that argument and None is a legitimate value.
In the vectorbt's ecosystem, we can find a lot of arguments of an enumerated type. Enums are regular integers that act as categorical variables. Similarly to contexts, they are also represented by a named tuple, but this tuple is already initialized with values usually ranging from 0 to the total number of categories in the enum. Let's take the SizeType as an example:
Instead of requiring the user to pass any value as an integer, we can take the name of its field instead and convert it to an integer using map_enum_fields, which takes the field name and the enumerated type, and returns the value of that field in that type. In addition, it allows us to convert entire collections of fields, such as lists and Pandas objects.
Internally, this function uses apply_mapping, which enables many useful options, such as ignoring the input if it's already of an integer data type:
Also, by default, it ignores the case and removes all non-alphanumeric characters:
And that's all the magic behind Portfolio.from_orders and other simulation methods knowing how to handle string options! Let's enter a position of one share at one bar and then sell 50% of it at the next bar:
Note
Conversion isn't vectorized - it's advisable to provide actual integers when bigger arrays are involved to avoid performance penalties.
Once the argument values have been resolved, vectorbt takes all the arguments that should broadcast against the target shape, and passes them to the function broadcast.
Broadcasting is one of the most essential building blocks in vectorbt because it allows us to provide arrays with various shapes and types, including NumPy arrays, Pandas Series and DataFrames, and even regular lists. Whenever we iterate over rows (timestamps) and columns (assets), the simulator needs to know which element in each array currently to pick. Instead of providing huge arrays where each element is set, we can pass some arrays with elements per row, some with elements per column, and some as scalars, and the vectorbt's broadcaster will make sure that they perfectly align with the target shape over which the simulator iterates.
Let's broadcast some arrays manually. Here, we have one column in the price array and want to test multiple combinations of order size by making it a DataFrame with one row and multiple columns:
But thanks to flexible indexing, we don't have to bring each argument to the full shape and materialize it. That's why, to avoid high memory consumption, each simulation method also passes keep_flex=True to the broadcaster to keep all arguments in their original form suitable for flexible indexing. For them, the broadcaster will only check whether they can broadcast to the final shape. Since we not only broadcast NumPy arrays but also Pandas objects as well, we need to return the wrapper resulting from the broadcasting operation, which will contain the final shape and Pandas metadata including the index and columns:
Hint
Even though we passed fees as a scalar, the broadcaster automatically wrapped it with a NumPy array and expanded to two dimensions for Numba. For the same reason, it also converted Pandas to NumPy.
Some arrays, such as init_cash and cash_deposits, cannot broadcast together with close because their final shape depends on the number of groups, or they are one-dimensional by nature. Thus, after all the regular arrays were aligned, the wrapper was created, and the target shape was established, vectorbt will first create the group lengths array, and then individually broadcast all the arrays that aren't broadcastable against target_shape. For example, to broadcast the initial position array:
All of this is conveniently done by Portfolio.from_orders!
To control the broadcasting process, we can pass additional arguments to broadcast via broadcast_kwargs. For example, let's replace the final columns:
We can also wrap any argument with the class BCO to provide broadcasting-related keyword arguments just for that particular object, or with the class Param to mark the object as a parameter. Below, we are testing the Cartesian product of two parameters: size and fees.
Furthermore, we can broadcast differently-shaped Pandas DataFrames if their column levels overlap. Let's say we have two assets, and we want to test the open and close price as the order price. For this, we need to create a Pandas DataFrame for the order price that has 4 columns, each price type per each asset:
Even though both shapes (1524, 2) and (1524, 4) are not broadcastable (!) in NumPy, vectorbt identifies that both DataFrames share the same column level symbol, and aligns them based on that level using align_pd_arrays, which then yields a successful simulation:
Info
See the API documentation of broadcast for more examples on broadcasting.
Even though Portfolio.from_orders is the most basic simulation method, it already takes dozens of broadcastable array-like arguments. So, how do we know exactly which argument is broadcastable?
To see which arguments can broadcast, take a look either at the API documentation of the argument, or at the annotation of the argument in the source code, which has the type ArrayLike when it can be provided both as a scalar and as an array. You can also look at the argument annotations of the Numba-compiled simulation function (here from_orders_nb) and search for the prefix FlexArray. Finally, the last and probably the least underrated method is to look at the argument taking specification of the chunking decorator: arguments that are chunked using FlexArraySlicer or have flex in the specification name are broadcastable by nature. The specification also reveals against what shape the argument should broadcast.
Here, the argument close is expected as a flexible array that is chunked along the column axis using the group lengths mapper. Since the simulation function is always chunked by its groups and columns of this argument are mapped to those groups, it should broadcast along columns in target_shape as opposed to groups in group_lens. Arguments that have no mapper, such as cash_deposits, always broadcast along groups:
Hint
As a rule of thumb: if you take a look at the source code of from_orders_nb, only the arguments that have portfolio_ch.flex_array_gl_slicer as their specification broadcast together and build the target shape. All other flexible arguments broadcast individually once the target shape has been built.
Since the target shape is now being generated from broadcasting instead of being passed manually by the user, there is no possibility for the user to provide the group lengths either. Instead, vectorbt will take a grouping instruction and create this array for us. Grouping is performed by constructing a Grouper instance, which takes the broadcasted columns and a group-by object (group_by). It then uses the group-by object to distribute the columns into groups and generate the group lengths array. See the API documentation of Grouper to learn about various group-by options. For example, we can pass group_by=True to put all columns into a single group, or specify the column level by which the columns should be grouped. 
And even though from_orders_nb automatically turns on cash sharing whenever it discovers multiple columns in a group, we must explicitly enable cash sharing with cash_sharing in the class method, otherwise no grouping during the simulation will be performed! This is because a Portfolio instance can also group its columns during the post-analysis phase, and cash_sharing is a special flag that tells vectorbt to perform the grouping during the simulation as well.
Let's demonstrate what it's like to invest all the initial cash into two assets without grouping, with grouping, and with grouping and cash sharing:
Passing group_by=True only works when all columns are different assets and there are no parameter combinations. But how about multiple assets and parameter combinations?
Let's play with different group-by instructions on the mult_close and mult_price arrays that we constructed earlier. The final broadcasted shape has 4 columns: each price type per each asset. What we want to do is to create two groups by putting the assets of each parameter combination into a single basket. We cannot pass group_by=True because it will combine all 4 columns. We also cannot pass the column level symbol as group_by because it will group by asset, that is, it will put the columns with BTC-USD, such as (Open, BTC-USD) and (Close, BTC-USD), into one group and the columns with ETH-USD into another. What we need though is to put (Open, BTC-USD) and (Open, ETH-USD) into one group and (Close, BTC-USD) and (Close, ETH-USD) into another; that is, we need to pass all the column levels apart from symbols as group_by, which can be done in multiple ways:
Important
To make sure that the grouping operation on assets was successful, the final column hierarchy should include all columns levels except the one with asset symbols. For example, passing group_by=True will hide all columns levels, while passing group_by='symbol' will show only the column level with asset symbols 
Similarly to grouping, the class method also simplifies handling of call sequences. There is an argument call_seq that not only accepts a (broadcastable) array, but can also be provided as a value of the enum CallSeqType. For example, we can pass "auto" to sort the call sequence automatically and to first execute the assets that should be sold before executing the assets that should be bought, which is an important requirement for rebalancing. Let's create an equally-weighted portfolio that is being rebalanced every month:

Hint
By the way, this is exactly the way Portfolio.from_optimizer uses Portfolio.from_orders for rebalancing!
To access the sorted call sequence after the simulation, we can pass attach_call_seq and then read the property Portfolio.call_seq:
When testing many trading strategies and parameter combinations, cash may quickly become a bottleneck. Also, determining the right amount of starting capital is often a challenge by itself. Gladly, there is a range of options that help us in backtesting a trading strategy without having to think about cash limits. One common approach is to pass np.inf as init_cash to simulate an unlimited cash balance. Welcome to the billionaires club 
But this would drive the post-analysis havoc because the portfolio value at each timestamp would be infinite as well. To account for this, we can instruct vectorbt to simulate an unlimited cash balance during the simulation, and then post-analyze the expenditures to determine the optimal starting capital. If wanted, we can then re-run the simulation but with the calculated optimal amount. To make this possible, we can pass one of the options from InitCashMode as init_cash. As opposed to other enums, this enum contains only negative values such that they cannot be confused with zero or positive amounts.
Important
During the simulation, each group value will be infinity. Thus, we cannot use the size of (+/-) infinity, but also percentages, target percentages, and other size types that depend on the group value. We also cannot fill returns during the simulation.
Let's DCA into BTC and ETH by buying one unit each year, and then, retrospectively, find out how much capital this activity would require:
We can then pass these amounts to a new simulation if desired:
We can see how each investment gradually reduces the cash balance and how the final investment drains it completely, while still enabling us to order exactly one unit of each cryptocurrency:
All the arrays that are returned as part of SimulationOutput, such as cash_deposits, cash_earnings, and in_outputs.returns, can be accessed as an attribute with the same name of a Portfolio instance. Let's do the same example as we did in Cash deposits:
Another automation brought by Portfolio.from_orders is touching the maximum number of order and log records. Whenever we pass None, from_orders_nb will pick the maximum possible number of records. In the class method though, vectorbt will check how many non-NaN values there are in the size array, and will pick the highest number across all columns. The same goes for logs, where it determines the number of True values. This has almost no impact on performance because vectorbt doesn't need to fully broadcast both arrays to get these numbers. Thus, we don't have to provide max_order_records and max_log_records if we decide to represent inactive data points as NaN in a huge close array.
Numba is the closest thing to a statically-typed code in vectorbt, and directly passing wrong data types to Numba usually leads to an ugly-formatted exception. To provide us with a bit of help in debugging, vectorbt checks all data types prior to calling from_orders_nb.
Here, the argument close seems to require a numeric data type 
Note
In vectorbt, almost no function will change the data type of an array in a hidden manner because casting may be quite expensive in performance terms, especially when huge arrays are involved. It's the responsibility of the user to supply properly typed and sized data!
Remember how we resolved various jitting options to redecorate from_orders_nb? Just like everywhere in vectorbt, in the class method a jitting option can be provided using the argument jitted. Let's test a random simulation without and with automatic parallelization with Numba:
Info
Preset simulators like from_orders_nb can only be parallelized along groups and columns, thus it would make no difference in enabling the parallel mode when there is only one group or column present. But also, it's usually a better idea to use chunking for parallelization as Numba may yield no performance benefit at all. Only certain user-crafted pipelines that make heavy use of math can be parallelized well with Numba.
Similarly to a jitting option, chunking can be enabled by providing an option via the argument chunked. Below, we do the same benchmark as above but now using Dask:
Info
Multithreading with Dask is better suited for this job than multiprocessing with Ray because, by default, all Numba-compiled functions in vectorbt release the GIL, and there is a much smaller overhead when starting multiple threads than processes. Consider using multiprocessing when the function takes a considerable amount of time to run.
This method is best suited for backtesting jobs where order information is known beforehand and there is no order that changes its parameters depending on changes in the environment. That is, we cannot implement SL, TP, limit, or any other complex order type using this method. It's really just a smart way of representing multiple instances of Order in a vectorized and resource-cheap way - imagine how difficult it would be to supply a list of named tuples instead of arrays! There are two particular use cases where Portfolio.from_orders becomes interesting: portfolio rebalancing with predefined weights, and "what-if" analysis. While we've already touched rebalancing more than enough times, the latter use case is about simulating and analyzing various hypothetical scenarios of a real-world trading activity.
Let's say we have made 3 trades on SOL/BTC on Binance and want to analyze them in depth. Even if Binance has made some improvements in its trade analysis capabilities, doing it with vectorbt opens an entirely new dimension. First, we need to pull the close price of a granularity with which we would like the trades to be analyzed. Then, we need to convert trade information into orders. Finally, we can use Portfolio to see how the portfolio evolved over time!

We can now change the size, price, and fixed_fees arrays to our liking and re-run the simulation to see how the performance of our trading strategy has been affected 
As we just learned, vectorbt deploys a range of preset simulators, each consisting of a Numba-compiled core, and a class method on top of Portfolio that wraps that core and supports it with diverse enhancements that make it easier and user-friendlier to operate. In particular, we looked at the most primitive simulator - "from orders", which takes a shape (timestamps x assets + parameter combinations), and at each single element of that shape, puts different information pieces like puzzles together to create an order instance. For visual thinkers: you can imagine it taking all arrays together, broadcasting them on the fly against a common shape, and overlaying them on top of each other to form a cube where each element viewed from top is a vector with order information from Order. 
Because of flexible indexing, we don't have to actually broadcast and materialize all those arrays - vectorbt is smart enough to project (i.e., extrapolate) smaller arrays to bigger shapes, such that we can supply incomplete information per timestamp, per asset, or per the entire matrix, as if we supplied that information per each element. This way, there is almost no additional memory footprint, which allows us to conveniently work on big data and perform hyperparameter optimization without leaving Numba, as long as all input arrays fit into RAM, of course 
Finally, this documentation piece gave us a glimpse into how vectorbt gradually stacks various levels of abstraction to automate tasks. We started with just two commands - buy and sell, added tons of features on the way, and ended up with a Python method that makes it almost ridiculously easy to backtest things. And still, this method lies at the bottom of the food chain: it cannot backtest trading strategies where orders depend on the current simulation state, that is, we have to know all the order information prior to starting the simulation. But get ready, that's where signals and order functions come into play!
 Python code
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
Portfolio refers to any combination of financial assets held by a trader. In the world of vectorbt, "portfolio" is a multidimensional structure capable of simulating and tracking multiple independent but also dependent portfolio instances. The main function of a portfolio is to apply a trading logic on a set of inputs to simulate a realistic trading environment, also referred to as "simulation". The outputs of such a simulation are orders and other information that can be used by the user in assessing the portfolio's performance, also referred to as "reconstruction" or "post-analysis". Both phases are isolated in nature, which enables various interesting use cases for quantitative analysis and data science.
The main class concerned with simulating and analyzing portfolios (i.e., with the actual backtesting) is Portfolio, which is a regular Python class subclassing Analyzable and having a range of Numba-compiled functions at its disposal. It's built similarly to other analyzable classes in the way that it has diverse class methods for instantiation from a range of inputs (such as Portfolio.from_signals taking signals), it's a state-full class capable of wrapping and indexing any Pandas-like objects contained inside it, and it can compute metrics and display (sub-)plots for quick introspection of the stored data.
So, what's a simulation? It's just a sophisticated loop!  
A typical simulation in vectorbt takes some inputs (such as signals), gradually iterates over their rows (time steps in the real world) using a for-loop, and at each row, runs the trading logic by issuing and executing orders, and updating the current state of the trading environment such as the cash balance and position size. If we think about it, it's the exact same way we would approach algorithmic trading in reality: at each minute/hour/day (= row), decide what to do (= trading logic), and place an order if you decided to change your position in the market.
Now, let's talk about execution. The core of the vectorbt's backtesting engine is fully Numba-compiled for best performance. The functionality of the engine is distributed across many functions in an entire sub-package - portfolio.nb, ranging from core order execution commands to calculation of P&L in trade records. Remember that those functions aren't meant to be used directly (unless specifically desired) but are used by Python functions higher in the stack that know how to properly pre-process their input data and post-process the output data.
In the following parts, we'll discuss order execution and processing, and gradually implement a collection of simple pipelines to better illustrate various simulation concepts.
Remember that vectorbt is an exceptionally raw backtester: it's primary commands are "buy"  and "sell"  This means that any strategy that can be translated into a set of those commands is also supported out of the box. This also means that more complex orders such as limit and SL orders must be implemented manually. In contrast to other backtesting frameworks where processing is monolothic and functionality is written in an object-oriented manner, Numba forces vectorbt to implement most of the functionality in a procedural manner.
Info
Even though Numba supports OOP by compiling Python classes with @jitclass, they are treated as functions, must be statically typed, and have performance drawbacks that don't allow us to jump on the wagon just yet.
Functions related to order execution are primarily located in portfolio.nb.core. The functions implementing our primary two commands are buy_nb and sell_nb. Among the requested size and price of an order, the primary input of each of these functions is the current account state of type AccountState, which contains the current cash balance, position size, and other information about the current environment. Whenever we buy or sell something, the function creates and returns a new state of the same type. Furthermore, it returns an order result of type OrderResult, which contains the filled size, price adjusted with slippage, transaction fee, order side, status information on whether the order succeeded or failed, and valuable information about why it failed.
The buy operation consists of two distinct operations: "long-buy" implemented by long_buy_nb and "short-buy" implemented by short_buy_nb. The first one opens or increases a long position, while the second one decreases a short position. By chaining these two operations, we can reverse a short position, which is done automatically by buy_nb: it checks the position we're in (if any), and calls the responsible function.
Let's say we have $100 available and want to buy 1 share at the price of $15:
The returned state indicates that we spent $15 and increased our position by 1 share. The order result contains details about the executed order: we bought 1 share for $15, with no transaction fees. Since order side and status are of named tuple type OrderSide and OrderStatus respectively, we can query the meaning behind those numbers as follows:
Info
If any value is -1 and cannot be found in the named tuple, the information is unavailable.
Now, based on the new state, let's execute a transaction that uses up the remaining cash:
Since vectorbt was originally tailored to cryptocurrency and fractional shares, the default behavior is buying as much as possible (here 5.67), even if the amount is below of that requested. But what happens if we wanted to have the entire share instead? Let's specify the size granularity of 1, indicating that only integer amounts should be allowed:
This has bought exactly 6 shares. Given the new account state, let's run the same transaction again:
The account state remains unchanged, while so many NaNs in the order result hint at a failed order. Let's query the meaning behind the status and status information numbers using OrderStatus and OrderStatusInfo named tuple respectively:
Here, the status "Size is zero" means that by considering our cash balance and after applying the size granularity, the (potentially) filled order size is zero, thus the order should be ignored. Ignored orders have no effect on the trading environment and are simply, well, ignored. But sometimes, when the user has specific requirements and vectorbt cannot execute them, the status will become "Rejected", indicating that the request could not be fulfilled and an error can be thrown if wanted.
For example, let's try to buy more than possible:
There are many other parameters to control the execution. Let's use 50% of cash, and apply 1% in fees and slippage:
The final fees and the price adjusted with the slippage are reflected in the order result.
Whenever we place an order, we can specify any price. Thus, it may sometimes happen that the provided price is (by mistake of the user) higher than the highest price of that bar or lower than the lowest price of that bar. Also, if the user wanted the price to be closing, and he specified a slippage, this would also be quite unrealistic. To avoid such mistakes, the function performs an OHLC check. For this, we need to specify the price_area of type PriceArea: with the price boundaries, and what should be done if a boundary violation happens via price_area_vio_mode of type PriceAreaVioMode:
The sell operation consists of two distinct operations: "long-sell" implemented by long_sell_nb and "short-sell" implemented by short_sell_nb. The first one decreases a long position, while the second one opens or increases a short position. By chaining these two operations, we can reverse a long position, which is done automatically by sell_nb: it checks the position we're in (if any), and calls the responsible function.
The function for selling takes the same arguments but uses them in the opposite direction. Let's remove 2 shares from a position of 10 shares:
The size in the order result remains positive but the side has changed from 0 to 1:
Shorting is a regular sell operation with sell_nb, but with one exception: it now involves the debt as well as the locked cash balance. Whenever we short, we are borrowing shares and selling them to buyers willing to pay the market price. This operation increases the cash balance and turns the position size negative. It also registers the received cash amount as a debt, and subtracts it from the free cash balance. Whenever we buy some shares back, the debt decreases proportionally to the value of the shares bought back, while the free cash might increase depending upon whether the price was higher or lower than the average short-selling price. Whenever we cover the short position entirely, the debt becomes zero and the free cash balance returns to the same level as the regular cash balance.
Note
You shouldn't treat debt as an absolute amount of cash you owe since you owe shares, not cash; it's used for calculating the average leverage and entry price of the short position, which is then used to calculate the change in the free cash balance with each trade.
To borrow any shares, we need a positive free cash balance to be used as a collateral. The exact amount of free cash needed for a shorting operation depends on margin; by default, we need the same amount of funds available in our margin account as the value of to-be-borrowed shares. For example, if we short a stock and the new position had a value of $100, we would be required to have the $100 that came from the short sale plus an additional $100 in cash, for a total of $200, which depending on the definition is either a 100% (before sale) or 200% (after sale) initial margin requirement. Maintenance margin and liquidation checks are the responsibility of the user (for now). 
Info
Infinity is a special value in vectorbt and usually means "go as far as you can".
Here's what happened. First, we've converted all the available free cash ($100) into the locked one such that it becomes the collateral of our shorting operation. Because the default leverage is 1, we've borrowed the same value in shares ($100), which has been added to the regular cash balance as well registered as a debt. This value corresponds to (minus = borrowed) 6.67 shares. But here's the problem: since we've doubled the regular cash balance, it may be used by other assets. To avoid that, all operations are done strictly on the free cash balance! But it's zero right now, so how do we buy back the borrowed shares? Remember that the debt and locked cash represent the total amount of cash that we used in the first place; by adding both to the free cash, we get our cash limit for the current buy operation, which nicely matches the regular cash when dealing with only one asset.
To change the margin, we can use the argument leverage. For example, setting it to 2 will allow us to borrow twice as many shares as can be covered by the current free cash:
The debt-to-locked-cash ratio is now 2, which corresponds to the leverage that we specified.
Info
We can specify a different leverage for each short-sell order, even in the same position.
Let's try to run the same operation again, but now on the new account state:
We see that vectorbt prevents the free cash balance to become negative.
To order any quantity possible, we can use the unlimited leverage:
What's the effective leverage of this operation?
If we had to calculate the current portfolio value, it would still default to the initial cash since no transaction costs were involved and no additional trades were made:
As we see, the positive cash balance and the negative position size keep the total value in balance. Now, let's illustrate buying back some shares using buy_nb. First, we'll borrow 10 shares with 2x leverage and sell them for the price of $10 per share:
Let's buy back 5 shares for the price of $30 per share (my condolences):
We executed the order for $150, which have been deducted from the regular cash balance. The position has been reduced by half, to 5 borrowed shares. Along with the position, the debt and locked cash have also been reduced by half. Given the absolute amount of released debt ($75), we can then compute the P&L, which is the total spent cash subtracted from the total released debt, or -$75. But this operation has also released some locked cash - $37.5, such that the amount of cash added back to our free cash balance is -$37.5, which makes it negative. A negative free cash means that we won't be able to buy any other assets apart from reducing any short positions and thus potentially releasing additional funds (i.e., profits as well as losses are shared among all assets within the same group with cash sharing). Even though we've got a negative free cash, we can still buy back more shares because the sum of debt, locked_cash, and free_cash is greater than zero.
Not only the debt and locked cash can be used to compute the effective leverage, but we can also derive the average entry price of the entire position:
Let's say instead of jumping, the price has dipped to $10 per share (my congratulations!):
We see that the debt and locked cash have decreased to the same level as previously (because we've bought back the same relative amount of shares), but the free cash balance is now $200, netting $25 in profit! Again, the calculation is simple: we just take the total amount of spent cash ($5 * 10 = $50) and subtract it from the total released debt (0.5 * $150 = $75) to get the P&L. By adding the P&L, the released locked cash (0.5 * $75 = $37.5), and the current free cash ($25) together, we get the new free cash of $87.5 immediately available for all other assets to use.
Let's compute the equity to validate the profit:
Let's close out the open short position using the same price:
The free cash balance equals to the regular cash balance, and we are debt-free! Additionally, the last two operations have brought us $50 in profit, or (15 - 10) * 10 = $50.
Finally, let's try to close out the position using an astronomically high price!
We could buy back only 2 shares out of the remaining 5. If we try the same operation again, we would get the "Not enough cash" message because debt + locked_cash + free_cash is below or equal to zero. We also witness the regular cash balance going to zero, which means that we've exhausted all of our capital; but we shouldn't rely on it to make trading decisions! If any other asset buys shares with leverage, regular cash may go negative, which doesn't necessarily mean that we have no cash left - only free cash (with debt and locked cash when covering shorts) provide us with the right information.
Even though vectorbt allows setting an arbitrary (even an infinite) cash and ordering as many shares as required by the user, this constellation is associated with some drawbacks: an infinite cash leads to an infinite portfolio value, which makes certain operations on that value impossible, such as the conversion from a target percentage into a target amount of shares; but also, the more cash we have, the smaller are potential contributions of the positions to the portfolio value, thus lowering the magnitude of the portfolio returns. What we need though is to multiply those contributions without inflating the cash balance, which can be effectively done using leverage.
Leverage involves borrowing additional funds to buy shares. In main contrast to shorting, leverage is applied to long positions and borrows cash instead of shares. The underlying mechanism is quite similar though. First, we multiply the available free cash by leverage. Then, we derive the order value and the fraction of it to be borrowed. Finally, we move the borrowed cash to debt while declaring a part of the free cash as the collateral of the operation and moving it to locked_cash. But since the locked cash must be spent to buy a part of the shares, it changes the way the effective leverage can be calculated from debt / locked_cash to debt / locked_cash + 1.
Let's say we have $100 in our margin account and want to buy $200 worth of shares. As we learned previously, we can specify an infinite leverage and vectorbt will derive the effective leverage internally for us:
We can see that $100 were deducted from our free cash balance and additional $100 were borrowed, thus bringing the effective leverage to 2:
Buying 10 shares instead would use no leverage since the transaction can be entirely covered by the free cash, even if the leverage is infinity:
Is there a way to use only a subset of our own free cash while borrowing the rest? Yes! The command buy_nb takes the argument leverage_mode of the type LeverageMode, which supports two modes: "lazy" and "eager" leveraging. The first mode is the default one and enables leverage only if the quantity to be bought cannot be fulfilled using own resources. The second mode enables leverage for any quantity and requires the leverage to be set explicitly, that is, an infinite leverage would raise an error. 
Note
Shorting supports "lazy" leveraging only.
Let's buy 10 shares with a 3x leverage:
We've used only $33.3 from our free cash balance as the collateral to borrow additional $66.6, making a total of $100 that were spent to buy the desired quantity.
Now, how do we repay the debt? When selling, the debt and locked cash balances decrease proportionally to the number of shares sold, which is exactly the same procedure as during (partially-) closing a short position. The main difference is in the calculation of the P&L: we take the total of the released debt and locked cash and subtract them from the cash retrieved from the operation.
First, we'll use a 2x leverage to buy 10 shares for the price of $20 per share:
Let's sell 5 shares for the price of $5 per share (my condolences):
We've retrieved $25 from this operation, which have been added to the regular cash balance. The debt and locked cash have been cut in half because the half of the leveraged position has been closed out. The P&L of this operation is the gained cash ($25) minus the released debt ($50) and locked cash ($50), which makes it a loss of $75. By adding this number to the released locked cash, we arrive at the new free cash of -$25 - a change that is propagated to all other assets using the same cash balance and thus preventing them to open or increase their positions.
Another way of calculating the P&L using the equity:
Now, let's say instead of dipping, the price has jumped to $40 per share (my congratulations!):
We've retrieved $200 from this operation, which have been added to the regular cash balance. The debt and locked cash have been cut in half because the half of the leveraged position has been closed out. The P&L of this operation is the gained cash ($200) minus the released debt ($50) and locked cash ($50), which makes it a profit of $100. By adding this number to the released locked cash, we arrive at the new free cash of $150, which can be now used by all other assets sharing the same balance.
Let's close out the remaining position using the same price:
We've made a profit of $200, which is just the same as we had used our own cash:
Long and short positions behave symmetrically. For example, let's open two opposite positions using an infinite size and 10x leverage and close them out with a price difference of $5 per share that favors the current position:
Positions in vectorbt can be reversed with a single order. To reverse a position, the direction argument should stay at its default - Direction.Both. Let's start with a position of 10 shares, reverse it to the maximum extent in the short direction, and then reverse it to the maximum extent again but in the opposite (long) direction:
Both operations are symmetric in nature and cancel each other out by a repetitive call, thus ultimately we've arrived at our initial state of the account.
To close out a position and to avoid its reversal, we can either specify the exact size, or the size of infinity and the current direction via the direction argument of the type Direction. For example, if we're in a long position and specified the long-only direction, the position won't be reversed:
Note
Using the buy_nb and sell_nb commands guarantees to execute the order in the long and short direction respectively.
We can also use the commands that are guaranteed to execute within the current position and not open an opposite one: long_sell_nb for long positions and short_buy_nb for short positions. They don't require the argument direction at all, just the size of infinity:
Even by using just these two essential commands, we can already build our own backtesting pipeline of arbitrary complexity and flexibility. As said before, a simulation is just a loop that iterates over timestamps. Let's create a simplified pipeline that puts $1 into Bitcoin each time it discovers a Golden Cross entry signal, and sells $1 otherwise. What we want is one number: the final value of the portfolio.
Hint
Adding a suffix _nb to indicate a Numba-compiled function is not necessary but still a good convention in vectorbt.
We can validate the pipeline using one of the preset simulation methods:
Using the primitive commands is fun as long as we exactly know the direction of the order and are sure that the provided arguments make sense. But very often, we have to deal with more complex requirements such as target percentages, which change the order direction depending on the current value. In addition, the commands do not validate their arguments; for example, there won't be any error thrown in a case when the user accidentally passes a negative order price. But also, we need a better representation of an order - it's a bad practice of passing all the parameters such as slippage as keyword arguments. 
All the checks and other pre-processing procedures are happening in the function execute_order_nb. The first input to this function is an order execution state of type ExecState, which contains the same information as an account state we saw above, but with additional information on the current valuation. The second input is a named tuple of type Order representing an order. The third argument is the price area, which we are also already familiar with.
An order in vectorbt is represented by a named tuple. Named tuples are alternatives to data classes in both the Python and Numba world; they are very efficient and lightweight data structures that can be easily constructed and processed. Let's create an instance of an order:
The tuple allows for attribute access through the dot notation:
Other than this, it behaves just like any other tuple in Python:
One issue that we still have to address when working with Numba are default arguments: although we can construct a new tuple solely with default arguments in Numba as we did above, if we want to override some values, they must be located strictly on the left in that tuple's definition. Otherwise, we must explicitly provide all the default arguments located before them:
Another issue are data types. In the example above where we provided integer size and price, Numba had no issues processing them. But as soon as we create such as order in a loop and one of the arguments is a float instead of an integer provided previously, Numba will throw an error because it cannot unify data types anymore. Thus, we should cast all arguments to their target data types before constructing an order.
Both issues are solved by using the function order_nb:
Notice how the size and price arguments were automatically cast to floats.
Hint
Use order_nb instead of Order whenever possible.
To create an order that closes out the current position, we can conveniently use close_position_nb:
Having constructed the order, execute_order_nb will check the arguments of that order for correct data types and values. For example, let's try passing a negative price:
After validating the inputs, execute_order_nb uses them to decide which command to run: buy or sell. But first, it has to do some preprocessing.
Even though vectorbt isn't associated with any particular data schema and can run on tick data just as well as on bar data, it still gives us an option to provide the current candle (price_area) for validation and resolution reasons. In such a case, it will consider the passed order price as a price point located within four price bounds: the opening, high, low, and closing price. Since order execution must happen strictly within those bounds, setting order price to -np.inf and np.inf will replace it by the opening and closing price respectively. Hence, next time, when you see any default price being np.inf, just know that it means the close price 
Our primitive commands accept only a size in the number of shares, thus we have to convert any size type defined in SizeType to Amount. Different size types require different information for conversion; for example, TargetAmount requires to know the current position size, while Value also requires to know the current valuation price.
Let's execute an order such that the new position has 3 shares:
Since we're not in the market, vectorbt used buy_nb to buy 3 shares. If we were in the market with 10 shares, it would have used sell_nb to sell 7 shares.
Function order_nb takes the argument direction for two reasons: resolve the direction of the order based on the sign of the argument size, and decide on whether to reverse the position or just to close out it. When the direction is LongOnly or Both, a positive size means buying and a negative size means selling. When the direction is ShortOnly though, the exact opposite happens: a positive size means selling and a negative size means buying. This is because a positive size is associated with increasing a position, which means buying to increase a long position and selling to increase a short position. For example, if the direction was ShortOnly and the size was a negative infinity, any short position would be closed out and any long position would be enlarged.
Speaking about the valuation price, it's the latest available price at the time of decision-making, or the price used to calculate the portfolio value. In many simulation methods, valuation price defaults to the order price, but sometimes it makes more sense to use the open or previous close price for the conversion step. The separation of the valuation and order price enables us to introduce a time gap between order placement and its execution. This is important because, in reality, not always an order can be executed right away.
Let's order 100% of the portfolio value:
Why haven't we spent the entire cash? Because to convert the target percentage into the target amount of shares, vectorbt used the provided order execution state with val_price of $15 and value of $100, which produced 100 / 15 = 6.67. The closer the valuation price is to the order price, the closer the calculation result would be to the target requirement.
By default, if we want to place multiple orders within the same bar (for example, in pairs trading), vectorbt wouldn't adjust the portfolio value after each order. This is because it assumes that we made our trading decisions way before order execution and adjusting the value would affect those decisions. But also, an order has only a marginal immediate effect on the value, for example, because of a commission. To force vectorbt to update the valuation price and value itself, we can enable update_value:
Notice how the new valuation price has been set to the close price adjusted with the slippage while the value has decreased by the fixed commission. Any new order placed after this one would use the updated value and thus probably produce a different outcome.
Note
Use this feature only if you can control the order in which orders appear within a bar and when you have intra-bar data.
Let's create another simplified pipeline that orders given a target percentage array. In particular, we'll keep 50% of the portfolio value in shares, and rebalance monthly. We'll calculate the portfolio value based on the open price at the beginning of each bar, and order at the end of each bar (to keep things realistic). Also, we'll fill asset value and portfolio value arrays to later plot the allocation at each bar.
Let's run the pipeline on our Bitcoin data:

Hint
Each point represents a revaluation at the end of each bar.
We see that allocations are being regularly pulled back to the target level of 50%.
Let's validate the pipeline using Portfolio.from_orders:

One of the biggest advantages of using vectorbt is that you can run your minimalistic trading environment in any Python function, even inside objective functions of machine learning models! There is no need to trigger the entire backtesting pipeline as a script or any other complex process like most backtesting frameworks force us to do 
Order execution takes an order instruction and translates it into a buy or sell operation. The responsibility of the user is to do something with the returned order execution state and result; mostly, we want to post-process and append each successful order to some list for later analysis - that's where order and log records come into play. Furthermore, we may want to raise an error if an order has been rejected and a certain flag in the requirements is present. All of this is ensured by process_order_nb.
Order records is a structured NumPy array of the data type order_dt containing information on each successful order. Each order in this array is assumed to be completed, that is, you should view an order as a trade in the vectorbt's world. Since we're dealing with Numba, we cannot and should not use lists and other inefficient data structures for storing such complex information. Given that orders have fields with variable data types, the best data structure is a record array, which is a regular NumPy array with a complex data type and that behaves similarly to a Pandas DataFrame.
Since any NumPy array is a non-appendable structure, we should initialize an empty array of a sufficient size, and gradually fill it with new information. For this, we need a counter - a simple integer - that points to an index of the next record to be written.
Info
Actually, you can append to a NumPy array, but it will create a new array. Don't try this at home 
Let's create an array with two order records and a counter:
We shouldn't access this array just yet because it contains memory garbage, thus it requires the user to manually set all the values in the array, and should be used with caution.
Let's execute an order using execute_order_nb at the 678th bar, and fill the first record in the array:
Note
When writing to an element of a record field, first select the field, and then the index.
At the next bar, we'll reverse the position and fill the second record:
Here are the order records that we've populated:
But instead of setting each of these records manually, we can use process_order_nb to do it for us! We just need to do one little adjustment: both the order records and the counter must be provided per column since vectorbt primarily works on multi-columnar data. This means that the order records array must become a two-dimensional array and the counter constant must become a one-dimensional array (both with only one column in our example):
Such filled order records will become the backbone of the post-analysis phase.
Log records have the data type log_dt and are similar to order records, but with a few key differences: they are saved irrespective of whether the order has been filled, and they also contain information on the current execution state, the order request, and the new execution state. This way, we can completely and post-factum track down issues related to order processing.
Note
Logging costs performance and memory. Use only when really needed.
Let's extend the last pipeline to independently process an arbitrary number of columns, and gradually fill order records. This way, we can backtest multiple parameter combinations by taking advantage of multidimensionality!
Info
We are flattening (repartitioning) order records because most records are left unfilled, thus unnecessarily taking memory. By flattening, we're effectively compressing them without losing any information because each record already tracks the column it's supposed to be in.
Our pipeline now expects all arrays to be two-dimensional. Let's test three value combinations of the parameter every, which controls the re-allocation periodicity. For this, we need to expand all arrays to have the same number of columns as the parameter combinations.
This output is exactly what Portfolio requires as input: order records with a couple of other arguments can be used to reconstruct the simulation state, including the regular cash balance and the position size at each time step. The reconstructed state can be used to model the equity curve, then returns, and then the accompanying metrics such as the old but gold Sharpe ratio. So, what how do we construct a portfolio? Instead of using any class method, we'll pass the data directly to the class. For this, only three arguments are required: a wrapper, a close series, and order records. Ideally, we should also supply arguments that were used during the simulation, such as the initial cash:
Hint
Notice *? It means any argument after order_records must be provided as a keyword argument.
Let's do the wrapping part:
We can now use the portfolio the same way as if we simulated it with any preset method:
The issue of bringing all arrays to the same shape as we did above is that it unnecessarily consumes memory: even though the only array that has different data in each column is target_pct, we have almost tripled memory consumption by having to expand other arrays like close. Imagine how expensive would it be having to align dozens of such array-like arguments 
Flexible indexing allows us to overcome this alignment step and to access each element of an array solely based on its shape. For example, there is no need to tile close three times if each row stays the same for each column - we can simply return the same row element irrespective of the column being queried. The same goes for a one-dimensional array with elements per column - return the same column element for each row. The only requirement is that the array must have one dimension if it should broadcast against rows or columns, and two dimensions if it should broadcast against rows and columns. Any scalars should be transformed into one of the above formats, otherwise we'll be greeted with an ugly Numba error. 
For the actual indexing, we can use the following Numba-compiled functions:
Let's demonstrate their use in different scenarios:
One-dimensional indexing functions are suited only for arguments that are one-dimensional by design, such as initial capital, which makes only sense to be provided per column, not per element. But what if the user should also be able to pass per_row_arr or per_col_arr as fully-broadcast arrays? In this case, the user needs to expand both arrays to two dimensions according to the NumPy's broadcasting rules and use exclusively flex_select_nb. The reason for this is that Numba isn't flexible enough to permit doing operations on both one-dimensional and two-dimensional arrays, so we must decide on the indexing function beforehand.
This yields the same results as if we had aligned the arrays prior to indexing (= memory expensive):
Hint
If you're not sure whether a flexible array will be indexed correctly, try broadcasting it with NumPy!
But what happens if the index is out of bounds? Let's say we iterate over 6 columns but an array has data only for 3. In such a case, vectorbt can rotate the index and return the first element in the array for the fourth column, the second element for the fifth column, and so on:
If you think that this is crazy, and you would rather have an error shown: rotational indexing is very useful when it comes to testing multiple assets and parameter combinations. Without it (default), we would need to tile the asset DataFrame by the number of parameter combinations, but with it, we could have just passed the data without tiling and thus wasting memory. But also, in many places, vectorbt ensures that all arrays can broadcast against each other nicely anyway.
Let's adapt the previous pipeline for flexible indexing. Since usually we don't know which one of the passed arrays has the full shape, and sometimes there is no array with the full shape at all, we need to introduce another argument - target_shape - to provide the full shape for our loops to iterate over. We'll also experiment with rotational indexing, which isn't supported by any of the preset simulation methods.
Thanks to flexible indexing, we can now use all arrays without tiling:
This also allows us to provide target percentages as a constant to re-allocate at each bar! Since constants don't have any implications on the targe shape, we only need to broadcast the price shapes:
This operation has generated the same number of orders as we have elements in the data:
To demonstrate rotational indexing, let's pull multiple symbols and perform the simulation without having to tile or change them in any way:
 
Symbol 2/2

Without rotation, we would have got an "IndexError: index out of bounds" error as the number of columns in the target shape is bigger than that in the price arrays.
Using groups, we can put multiple columns to the same backtesting basket 
Generally, a group consists of a number of columns that are part of a single portfolio entity and should be backtested as a single whole. Very often, we use groups to share capital among multiple columns, but we can also use groups to bind columns on some logical level. During a simulation, it's our responsibility to make use of grouping. For example, even though process_order_nb requires a group index, it uses it just for filling log records and nothing else. But after the simulation, vectorbt has many tools at its disposal to enable us in aggregating and analyzing various information per group, such as portfolio value.
Groups can be constructed and provided in two ways: as group lengths and as a group map. The former is easier to handle, marginally faster, and requires the columns to be split into monolithic groups, while the latter allows the columns of a group to be distributed arbitrarily, and is generally a more flexible option. Group lengths is the format primarily used by simulation methods (since asset columns, in contrast to parameter columns, are usually located next to each other), while group maps are predominantly used by generic functions specialized in pre- and post-analysis. Both formats can be easily generated by a Grouper instance.
Let's create a custom column index with 5 assets, and put them into 2 groups. Since group lengths work on monolithic groups only, assets in each group must be next to each other:
The first element in the returned array is the number of columns with the label 0, and the second element is the number of columns with the label 1.
Hint
Grouper doesn't care if we pass a list of integers or a sequence of strings - it will convert everything into a Pandas Index and treat it as group labels. They don't have to be alphanumerically sorted.
If we create discrete groups, the generation will fail:
Now, how do we define logic per group? Here's a template:
Group map is a tuple of two arrays:
Thus, a group map makes distributed groups inherently monolithic, such that we can work with any possible group distribution:
In the second example, the first two (2) column indices in the first array belong to the first group, while the remaining three (3) column indices belong to the second group.
Here's a template for working with a group map:
When sharing capital between two or more assets, we sometimes want to process one column before the others. This makes most sense, for example, in cases where we need to exit positions before opening new ones to release funds for them. If we look at the templates for both grouping formats above, we can precisely identify where the column processing order should be changed: in the for-loop that iterates over columns. But how do we programmatically change this order? Here comes a call sequence into play.
Call sequence is an array of column indices in the order of their processing. For example, if the third column should be processed first, the first column second, and the second column third, the call sequence would be [2, 0, 1]. That is, we are always moving from left to right in the call sequence and pick the current column index. Such a design has one immense benefit: we can use another array, such as with potential order values, to (arg-)sort the call sequence.
The sorting is done by the function insert_argsort_nb, which takes an array with values to sort by and an array of indices, and sorts the indices in-place using insertion sort in the order the values appear in the first array. This sorting algorithm is best suited for smaller arrays and does not require any additional memory space - perfect for groups with assets!
Let's say we have three assets: one not in position, one in a short position, and one in a long position. We want to close all positions in the order such that assets that should be sold are processed first. Otherwise, we wouldn't have cash from exiting the long position to close out the short position. For this, we will first use approx_order_value_nb to approximate the order value of each operation:
We see that the second column would require approx. $50 in cash and the third column would bring approx. $150 in cash to close out the position. Let's create a call sequence and sort it by the order value:
Note
Both the order value and the call sequence are sorted in-place!
We can then modify the for-loop to iterate over the call sequence instead:
Hint
A good practice is to keep a consistent naming of variables. Here, we're using k to denote an index in the call sequence, c to denote a column index within a group, and col to denote a global column index.
Let's upgrade our previous pipeline to rebalance groups of assets. To better illustrate how important is sorting by order value when rebalancing multi-asset portfolios, we'll introduce another argument auto_call_seq to switch between sorting and not sorting. We will use group lengths as the grouping format of choice because of its simplicity. Also note that now we have to keep a lot of position-related information in arrays rather than constants since they exist in relation to columns rather than groups. In addition, as we already know how to fill order records, let's track the allocation at each bar instead.
Wow, this went complex really fast! 
But it's not that complex as it may appear. We took a bunch of columns and split them into groups. Then, for each group, we defined a mini-pipeline that applies our logic on the columns within this group only, acting as a single portfolio unit. At the beginning of each bar, we calculate the portfolio value, and build a call sequence that re-arranges columns by their order value. We then iterate over this sequence and execute an order in each column. Finally, at the end of each bar, we again re-calculate the portfolio value and write the real allocation of each asset to the output array. The best in this pipeline is that it closely mimics how preset simulation methods work in vectorbt, and it's one of the most flexible pieces of code you can actually write!
Let's allocate 70% to BTC and 30% to ETH, and rebalance on a monthly basis:

Info
As you might have noticed, some allocations do not quite sum to 100%. This is because we used the open price for group valuation and decision-making, while the actual orders were executed using the close price. By the way, it's a bad sign when everything aligns perfectly - this could mean that your simulation is too ideal for the real world.
And here's the same procedure but without sorting the call sequence array:

As we see, some rebalancing steps couldn't be completed at all because long operations were executed before short operations, leaving them without the required funds.
The biggest advantage of this pipeline is in its flexibility: we can turn off grouping via group_by=False to run the entire logic per column (each group will contain only one column). We can also test multiple weight combinations via multiple groups, without having to tile the pricing data thanks to rotational indexing. This, for example, cannot be done even with Portfolio.from_orders 
Sometimes, there is a need to create a simulation method that takes a user-defined function and calls it to make some trading decision. Such a UDF would require access to the simulation's state (such as the current position size and direction) and other information, which could quickly involve dozens of variables. Remember that we cannot do full-scale OOP in Numba, thus we have to pass data using primitive containers such as tuples. But usage of variable positional arguments or a regular tuple would be quite cumbersome for the user because accessing each field can only be done using an integer index or tuple unpacking. To ease this burden, we usually pass such information in form of a named tuple, often referred to as a (simulation) "context".
Let's create a very basic pipeline that iterates over rows and columns, and, at each element, calls a UDF to get an order and execute it!
First, we need to answer the following question: "What information would a UDF need?" Mostly, we just include everything we have:
And here's our pipeline that takes and calls an order function:
Let's write our own order function that generates orders based on signals:
We just created our own shallow Portfolio.from_order_func functionality, neat! 
Your homework is to extend this pipeline with flexible indexing 
In terms of performance, Numba code is often a roller coaster 
Numba is a just-in-time (JIT) compiler that analyzes and optimizes code, and finally uses the LLVM compiler library to generate a machine code version of a Python function to be compiled. But sometimes, even if the function looks efficient on paper, Numba may generate a suboptimal machine code because of some variables or their types not interacting optimally. In such a case, the code may still run very fast compared to a similar implementation with Python or even to another JIT compiler, but there is a lot of space for improvement that may be hard to discover, even for experienced users. There are even cases where switching the lines in which variables are defined suddenly and unexpectedly has a negative/positive effect on performance.
Apart from official tips, there are some of the best practices you should always keep in mind when designing and optimizing Numba-compiled functions:
Hint
As a rule of thumb: the simpler is the code, the easier it becomes for Numba to analyze and optimize it.
To benchmark a simulator, we can use the timeit module. If possible, create some sample data of a sufficient size, and prepare for the worst-case scenario where orders are issued and executed at each single time step to benchmark the full load. Also, make sure to run tests all the way during the simulator's development to track the evolution of its execution time and stability.
Note
Generation of sample data and preparation of other inputs must be done prior to benchmarking.
Let's generate 1-minute random OHLC data for one year using RandomOHLCData:

Then, we need to prepare all the data, which includes filling signals such that there is at least one order at each bar (our worst-case scenario for performance and memory):
Each of the arrays is 527,041 data points long. 
So, how is our simulator performing on this data?
80 milliseconds to generate half a million orders on Apple M1, not bad! 
To better illustrate how only a minor change can impact performance, we will create a new order function that also creates a zero-sized empty array:
As we see, creating an empty array at each bar has slowed down the execution by more than 50%. And this is a very important lesson to learn: create arrays outside of loops and only once!
Because of path dependencies (= the current state depends on the previous one), we cannot parallelize the loop that iterates over rows (= time). But here's the deal: since vectorbt allows us to define a multi-columnar backtesting logic, we can parallelize the loop that iterates over columns or groups of columns, given that those columns or groups of columns are independent of each other - all using Numba alone. By the way, this is one of the primary reasons why vectorbt loves two-dimensional data layouts so much.
Automatic parallelization with Numba cannot be simpler: just replace range that you want to parallelize with numba.prange, and instruct Numba to parallelize the function by passing parallel=True to the @njit decorator. This will (try to) execute the code in the loop simultaneously by multiple parallel threads. You can read more about automatic parallelization with Numba here and about the available threading layers here. On MacBook Air (M1, 2020), turning on parallelization reduces the processing time by 2-3 times on average. Usually, a simple arithmetic-heavy code without creating any arrays can be better parallelized than a complex vectorization-heavy code.
Important
You can modify the same array from multiple threads, as done by countless functions in vectorbt. Just make sure that multiple threads (columns, in our case) aren't modifying the same elements and data in general!
Here's a small example of a function that computes the expanding maximum on two-dimensional data, without and with automatic parallelization:
It's your turn: enable automatic parallelization of columns in the sixth pipeline and benchmark it! Just don't forget to reduce the number of rows and increase the number of columns.
Even if we had optimized the simulation pipeline for the best-possible performance, the actual compilation step would take a huge chunk of that time savings away. However, the good news is that Numba doesn't have to re-compile the function the second time it's executed, given that we passed the same argument types (not data!). This means that we need to wait only once if we want to test the same function on many parameter combinations, at the same Python runtime. Sadly, if only one argument differs in type, or we've restarted the Python runtime, Numba has to compile again. 
But luckily, Numba gives us a mechanism to avoid re-compilation even if we've restarted the runtime, called caching. To enable caching, just pass cache=True to the @njit decorator.
Important
Avoid turning on caching for functions that take complex, user-defined data, such as (named) tuples and other functions. This may lead to some hidden bugs and kernel crashes if the data changes during the next runtime. Also make sure that your function doesn't use global variables. For example, the fifth pipeline is perfectly cacheable, while the sixth pipeline is not cacheable, or maybe could be if order_func_nb was cacheable as well.
Make sure to define any cached function inside a Python file rather than in a notebook cell since it must have a clear filepath such that it can be introspected by Numba. To invalidate the cache, go to the directory where the function resides and remove the __pycache__ directory. You can do that by running rm -rf __pycache__ from your terminal.
Hint
A good practice is to invalidate the cache every time you change the code of a cached function to avoid potential side effects. Also, disable caching for the entire time of developing a function and turn it only on once the function has been fully implemented.
Using ahead-of-time compilation, we can compile a function only once and get no compilation overhead at runtime. Although this feature of Numba isn't widely used in vectorbt because it would restrict us from passing input data flexibly, we can make use of it in cases where we know the argument types in advance. Let's pre-compile our fifth pipeline!
For this, we have to specify the signature of a function explicitly. You can read more about it in the types reference.
This has generated an extension module named pipeline_5. On macOS, the actual filename is pipeline_5.cpython-37m-darwin.so. We can then import the module like a regular Python module and run the function pipeline_5_nb of that module:
That was lightning fast! 
Important
You should ensure that the provided arguments exactly match the registered signature, otherwise you may get errors that may be very difficult to debug. For example, while setting init_cash to 100 would yield an "index is out of bounds" error, casting the array to integer would make all allocations zero!
We've covered in detail a lot of components of a typical simulator in vectorbt. Simulation is the primary step in backtesting of a trading strategy, and by mastering it you'll gain some hard-core skills that can be applied just in any place of the vectorbt's rich Numba ecosystem. 
One of the most important takeaways from this documentation piece is that implementing a custom simulator is as easy (or as difficult) as any other Numba-compiled function, and there is no point in using the preset simulation methods such as Portfolio.from_signals if you can produce the same results, achieve a multifold performance gain, be able to use rotational indexing, caching, and AOT compilation, by designing your own pipeline from scratch. After all, it's just a bunch of loops that gradually move over the shape of a matrix, execute orders, update the state of the simulation, and write some output data. Everything else is up to your imagination 
 Python code
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
The method Portfolio.from_orders (FO), which was discussed previously, is the most primitive simulation method: it takes order information in form of multiple array-like arguments and broadcasts them to a single shape, such that we know exactly what has to be ordered of each asset at each bar. This method requires us to have that information in advance, regardless of any events during the simulation. But what if we wanted to create an order only given that we're not currently in the market, or in general, to make an order dependable on the current simulation state? Such a conditional logic cannot be represented using orders alone - we either need to use a callback, or define more arrays. The former and the latter are both implemented by Portfolio.from_signals (FS).
Before we dive deep into this method, make sure to learn more about signals here. In a nutshell: signals are an abstraction layer over orders. Each signal consists of four boolean values:  long entry,  long exit,  short entry, and  short exit. A combination of these values enables us to control the direction of an order relative to the current position. For example, a short entry flag will reverse the current long position, or open a new short one if we're not in the market. This way, position management can be abstracted away from order management, such that we can lean back and only express our decision of whether we're currently bullish or bearish - a perfect playground for ML models, by the way.
And there's another reason to love signals: statistically, in an entire universe of signal permutations, there is at least one permutation that beats the market all the time. This means that we can design a perfect trading algorithm using the above signal schema alone - we just need to guess the right timing and direction of each signal, which reduces the number of factors we need to care about to just two (in a perfect world, of course, because in the real world we also need to take into account the risk, execution modalities, etc.). For example, if the price of any security is $21 at day 1, $20 at day 2, and $22 at day 3, we can short entry at day 1 and long entry at day 2 to get positive-only returns. That's why trading systems and their backtesting components as well shouldn't necessarily scream from complexity in order to be profitable - they just need a robust signal generator as their algorithmic backbone and a trading infrastructure that closely matches the backtesting one.
Example
Here are all the signal permutations for a price series with 4 points and their total return:
Don't run this on longer price series since the number of permutations grows exponentially with the number of data points - 2^n. That is, a year of daily history would require checking 2^365 or 7.515336e+109 permutations.
Similarly to FO, this method is also a class method of Portfolio and has two Numba-compiled core functions: from_signals_nb and from_signal_func_nb. In fact, FS shares many arguments with FO, especially those used to set up the simulation, such as init_cash, and those carrying order information, such as size. For instance, if we look at the API documentation of the argument size under Portfolio.from_signals, we'll see "See Portfolio.from_orders". But also the simulation procedure of FS itself is very similar to that of FO: while looping over all columns and rows, at each iteration, it resolves the current order and executes it by appending information on the filled order to the order records and updating the current simulation state. But that's where the similarities end.
Here's an abstract visualization of the framework of FS run on three rows and two groups with two columns and one column respectively:

If you worked with vectorbt long enough, you have likely noticed that the framework of Portfolio.from_signals follows that of Portfolio.from_orders and Portfolio.from_order_func. Like most things in the vectorbt's universe, the simulation with FS is done by iterating over a so-called "target shape". This shape consists of two dimensions: rows representing the time axis and columns representing the asset axis (or, generally, configurations). Columns are further divided into groups: if multiple columns share the same cash, they are put into the same group (blue rectangle on the left above), while columns without cash sharing or grouping are isolated and appear as a group with exactly one column (blue rectangle on the right above). Groups are considered to be separate, atomic backtesting instances that aren't connected by any means, that is, splitting the shape by target groups shouldn't affect the final result. This, by the way, is why chunking is generally performed on groups rather than columns 
The actual iteration over the rows and groups happens in the column-major order: the simulator starts moving over the rows in the first group, and once finished, continues with the second group. Every time it hits a new row when processing a group, all the assets at this row are called a "segment" because they together compete for the same resources at the same time, or are connected by any user-defined means. For example, an account with BTC-USD and ETH-USD on the date 2020-01-01 is considered a segment because the value of both assets adds to the total value of the group at this date. Each asset within a segment is called an "element", which is the smallest simulation unit. An element in FS can host only one order, such that the number of filled orders is effectively capped by the number of rows times the number of columns. For example, a year of the daily BTC-USD and ETH-USD history can generate at most 365 * 2 = 730 orders, or one order per bar and asset.
Segment is where the major part of the simulation takes place:
FS first updates the current simulation state using the opening price. This is done to get the group value in case some order has the size defined as a (target) percentage, to be able to convert the size into an absolute amount of units. Then, it iterates through the columns in the current group and determines the four signals under each column. Those signals then get converted into an order specification that is quite similar to the one FO takes. 
After having resolved all the order specifications, and if the automatic call sequence is enabled, the simulator attempts to sort the orders by their potential value such that sell orders are executed first - which is needed for rebalancing. This is only possible if all the (limit, order, and user-defined) orders within the current group happen either at the beginning or at the end of the current bar. If some orders happen in the middle of the bar, or some happen at the beginning and some at the end of a bar, an error will be thrown because we can only sort orders if they are guaranteed to happen at the same time! If the dynamic call sequence is disabled and there are orders in multiple bar zones, the simulator will sort them by the bar zone. 
Finally, FS goes through the columns in a newly established call sequence to execute the orders, and updates the simulation state once again using the closing price. The final update is required to optionally generate the portfolio returns and for the segment post-processing function.
FS supports two signal generation modes: fixed (cached), and dynamic (non-cached). The first mode is implemented by the function from_signals_nb, which takes the signals as four pre-defined arrays and doesn't allow defining any callbacks, thus it's perfectly cacheable and doesn't have to be recompiled with each new runtime, unless it discovers a new set of data types. The second mode is implemented by the function from_signal_func_nb, which doesn't accept any signal arrays, but defines a signal function - a special callback meant to generate the four signals for each asset and at each bar dynamically. It's most suited for use cases where the signal depends on the current simulation state. Furthermore, it defines a callback that is getting called after processing the current segment, which can be used to pre-compute various metrics, such as the Sharpe ratio. The main downsize is that it cannot be cached (yet), thus it has to be re-compiled in each new runtime ( to those running vectorbt as a script).
The convenience of the method Portfolio.from_signals, which wraps those two modes, is in its ability to choose the mode automatically: whenever we override any default callback, it runs the second mode over the first one.
Remember how in FO we had to provide everything as arrays and could neither dynamically change the provided information nor affect the execution in any way? FS is much more flexible than that: it expects most information to be defined beforehand (acting as a facade), while signals can be generated both statically and dynamically. Let's play with the dynamic signal generation a bit.
The second mode is implemented by accepting a user-defined callback function, signal_func_nb. Whenever the main simulation loop hits a new row (bar), it asks each asset in the current group to generate signals using this callback function. For this, it packs all the information that might be useful to the user, such as the current cash balance and the group value, into a named tuple of the type SignalContext. In return, it expects the function to return four signals, which will be used to create an order for that asset.
Here's how a dead-simple signal function that orders nothing looks like:
Hint
To avoid waiting for the function to compile, remove the @njit decorator from signal_func_nb and pass jitted=False to from_signals in order to disable Numba for this method completely. Do this only if the amount of input data is small (< 1000).
To better understand when the function is called, let's expand our data to two assets and print out the current column and row:
We see that the function was called at each row, first in the column BTC-USD, then in the column ETH-USD. Here, both assets are acting as isolated tests, thus the simulator processes one column after another. But once we introduce a grouping with or without cash sharing, which binds columns semantically, the simulator will process the columns group-wise, that is, it will move over the groups, then over the rows, and finally over the columns in the current group and at the current row. Let's demonstrate this by introducing two groups with two assets sharing the same cash:
The context tuple passed to the signal function contains all the necessary information to identify the position of the call in the simulation. For example, we can use c.index[c.i] with SignalContext.index and SignalContext.i to get the timestamp of the current bar. We can also change the current state of any pending limit or stop order before it's processed since the signal function is conceptually executed right before the beginning of the bar.
Thanks to the strict processing of groups from left to right and storing the state of each group globally, we can access the order records and, generally, the latest simulation state of all the groups that were processed earlier. For example, SignalContext.last_cash has the same number of elements as there are groups. This is powerful and dangerous at the same time: we can introduce complex intergroup relationships if wanted, or accidentally access the wrong group if not paying enough attention.
Signals are just one additional level of abstraction over orders, meaning there needs to be some logic in place that translates them into order specifications. Indeed, whenever the simulator receives a new set of four signals at each row and column, it first resolves them into a single signal, which then gets converted into an order. The resolution step checks whether the provided signals have conflicts. Mostly, the signals are expected to have only one True value and three False values, but sometimes multiple signals are True, especially when the signal function is forwarding data from multiple boolean arrays. In such a case, the simulator goes through the following procedure consisting of multiple checks and fixes:
It first checks whether there are multiple True values within the same direction, for example, when the long entry and long exit are both set. To decide which one in the long direction to keep, it looks at the argument upon_long_conflict of the type ConflictMode provided by the user. For example, the option "adjacent" will pick the signal adjacent to the position we are currently in such that only the long entry will remain active if we're in a long position. This is done by calling the function resolve_signal_conflict_nb:
After deciding for at most one signal in both directions, the simulator checks whether both the long and short entry are active and uses the function resolve_dir_conflict_nb based on the argument upon_dir_conflict of the type DirectionConflictMode to select the winner. For example, we can choose to always go short when there is any uncertainty:
Finally, it calls the function resolve_opposite_entry_nb if there is an entry signal that is opposite to the direction of the current position. For example, if we're in a long position and the short entry signal is set, the simulator will use the argument upon_opposite_entry of the type OppositeEntryMode to decide whether to reduce, close, or completely reverse the current long position. Let's make the short entry signal behave like the long exit signal:
In the end, only one active signal out of four will remain 
We have our one signal, now what? It's time to convert it into an order! And this is the easiest step in the pipeline, done by the function signal_to_size_nb, which takes the four signals (three of which are now deactivated) and the size requirement under this row and column (i.e., for this element), and returns the order size, size type, and direction to be requested. For example, being in a position of 20 shares and receiving the long exit signal, the size becomes minus 20 shares, the size type becomes SizeType.Amount, and the direction becomes Direction.LongOnly:
Even though we provided the function with the default order specification for the current element, such as size, the function didn't use it because it isn't required for closing the current position. On the other hand, if we wanted to reverse the current position (that is, close it and then order using the default specification), those inputs would suddenly become effective:
The size is calculated as follows: reduce the number of shares by 20 to close out the long position, and, given that we're operating with a percentage of the current group value, open a new short position of size * value_now / val_price_now = 2.0 shares. The size type is SizeType.Amount while the direction is Direction.Both because the operation now involves two directions.
The simulator called the signal function, resolved the incoming signals, and converted them into an order specification. But this is not the only order that competes for the current bar: there may be also pending limit and stop orders. Since the FS simulation function can process at most one order at each bar, it has to decide for a winner, which should always be an order that executes first. But how do we know which one comes first if we don't have any intra-bar data? We can still divide each bar into three "zones": opening (first rectangle below), somewhere in-between (second rectangle below), and closing (third rectangle below). For example, if a stop order was hit at or before the opening of the current bar and the user order should execute at the close price, then clearly the stop order should be first on the line. Here's the full decision chain:
As we see, limit orders have priority over stop orders, while stop orders have priority over user orders, but only if they are triggered within the same zone of a bar.
While a market order is a transaction meant to execute as quickly as possible at the current market price, a limit order is an order to buy or sell an asset with a restriction on the maximum price to be paid or the minimum price to be received (the "limit price"). The price of a limit order is getting compared against a pre-defined price level. Until this level is reached, the order is marked as pending, and it will not be filled if the price does not reach this level.
Whenever a stop or user-defined order goes through and its order type provided via order_type is OrderType.Limit, the simulator first determines what kind of limit price the order should be executed at: open, close, or something else? And this is a very important concept to grasp: the argument price gives vectorbt hints on where in the bar the operation should take place. If the limit price is the open price (provided as either PriceType.Open or -np.inf), the simulator is allowed to use the entire candle for its checks and execute the order right upon detecting the price being hit, at the same bar. If the limit price is not the close price but something in-between, the simulator is allowed to use the close price only. If the limit price is the close price (provided as either PriceType.Close or np.inf), the simulator is not allowed to execute the limit order right away - it's forced to postpone its very first check to the next bar.
If the limit order couldn't be executed at the same bar as it was created, the order is marked as pending and all the relevant information becomes stored in a record array of the type limit_info_dt, which is structured by asset. This array can keep only one instance per asset, thus FS allows only one limit order to be active at a time. In a signal function, this array can be accessed via c.limit_info_dt, allowing us to change any information before the new bar. For example, to change the price: c.limit_info_dt["init_price"][c.col] = new_price.
Once the simulator hits the next bar, it first uses check_limit_expired_nb to check whether the pending limit order expired at the beginning or somewhere in the middle of the bar. If the former, the order is thrown away. If the latter, the simulator also checks whether the order was hit at the beginning of the bar (and execute it), and if not, throw it away because there is no guarantee that the order was hit before the deadline. For example, let's assume that the order can be at most 36 hours in force, it was issued at the day 2020-01-01, and now is the day 2020-01-02:
We see that the function marked the order as expired, but not at the beginning of the bar such that it can still be executed using the open price. But if the lifespan of the order was 24 hours, the function would also raise the first flag and disallow any execution:
Info
The lifespan is calculated by subtracting any time from the opening time of the creation bar, even if the order was placed at the very end of the creation bar.
Once we're sure that the order can be executed at this bar (i.e., it won't expire), the simulator uses the function check_limit_hit_nb to check whether the order should be executed by determining whether its target price has been hit. This is implemented through comparison of the price against the current candle. For example, if we have a pending buy limit order with a target price of 9.5, the function will check whether the low price went below the target price:
If the target price was 11, the function would notify us about the price being hit already at the beginning of the bar; in such a case, the order will be executed right away using the open price:
If the target price hasn't been hit, the limit order remains pending. It can still be cancelled manually in the signal function called before all the checks above, or in the post-segment function called after processing the entire segment. The pending order will also be cancelled automatically once any stop order gets executed since the latter may change the simulation state and potentially pull the resources required to execute the former in the future. 
Finally, the four signals returned by the signal function and resolved into a single signal also can affect the pending order, regardless of whether the final signal gets executed or not. Consider the example where we have a pending buy limit order and the user decides to issue the long exit or short entry signal; in this case, the most intuitive reaction would be cancelling the pending order since the user have changed their mind. Exactly this is happening by default. Such "pending conflicts" are resolved using the function resolve_pending_conflict_nb, which uses the arguments upon_adj_limit_conflict and upon_opp_limit_conflict, both of the type PendingConflictMode, to decide what to do if the direction of the pending order is adjacent and opposite respectively to the direction of the resolved user-defined signal.
Here, the function decided to cancel the limit order and to ignore the user-defined signal.
Stop orders help to ensure a higher probability of achieving a predetermined entry or exit price, limiting the investor's loss, or locking in a profit. They remain dormant until a certain price is passed, at which time they are activated as a market or limit order. Execution of a stop order usually closes out the position. 
There are four types of stop orders:
By using a stop-loss order, we're limiting our risk in the trade to a set amount in the event that the market moves against us. For instance, if a stop-loss sell order were placed at $45 per unit, the order would be inactive until the price reached or dropped below $45. The order would then be transformed into a market or limit order, and the units would be sold at the best available price. A take profit is pretty much the exact opposite. It expresses how much we're willing to make as a profit with one trade and close it once we're happy with the amount. A combination of an SL and TP order creates a certain risk-to-reward ratio, which can be further tuned to respect the odds of reaching each certain breakout scenario. 
A different bread are trailing orders: when the price increases, it drags the trailing stop along with it. Then, when the price finally stops rising, the new stop-loss price remains at the level it was dragged to, thus automatically protecting our downside, while locking in profits as the price reaches new highs. Also, TTP is a version of TSL that gets activated after a certain threshold. These two orders are usually viewed and represented as a single order.
In contrast to limit orders, stop orders are created after an entry order has been filled, and behave identically to user-defined exit signals that are triggered once a stop condition has been met. An entry order is any successfully-filled order that has opened a new position or increased an existing one. Once the simulator detects such an order, it first resolves the stop entry price provided via the argument stop_entry_price of the type StopEntryPrice. This is the initial price to which all the stop values and thresholds are applied.
Note
By default, the stop entry price is the closing price, not the order price, to avoid a scenario where the stop is getting hit at the very first bar but cannot be executed since we don't have intra-bar data and cannot execute two orders at the same bar. By using the order price, the earliest time the stop can execute at is the opening of the next bar.
Based on this price, the simulator can also determine where exactly in the bar the stop order should be triggered. Why is this important? Because the simulator needs to know whether it can already use the current candle to update the price of any TSL and TTP order. Internally, the information on stop orders is stored inside three arrays of the data types sl_info_dt, tsl_info_dt, and tp_info_dt, laid out per asset. Each data type has a similar schema: the initial row and price, the current stop value (either in absolute or percentage terms), and the limit delta and its format if the stop order should ultimately trigger a limit order. The trailing stops also include the newly updated price and the row of the update.
Stop orders cannot be activated at the same bar as they were issued, even if the entry price is the opening price. This is because FS cannot handle two orders at the same bar (if there is a need for this, use a flexible order function with Portfolio.from_order_func). After hitting a new bar, the price of any pending SL and TP order then gets checked against the low and high price of the current candle respectively (the other way round for short positions), using the function check_stop_hit_nb, which returns the stop price, whether it happened on open, and whether it happened at all. For example, let's imagine that the initial price is $10 per unit and the stop loss sits at 10%. For the stop to be marked as hit, the lowest price of this candle must be under 10 * (1 - 0.1) = 9 per unit:
But if the initial price was at $12 per unit, the stop would match already at open:
To check a TP order, we would need to set the argument hit_below to False.
As opposed to fixed stop orders, TSL and TTP orders additionally need to track the peak price the stop price is based upon. Since we have no information on whether the highest price of a candle comes before the lowest price or vice versa, we need to split the candle into distinct zones and update the peak price in a zone that precedes that of the stop check. First, the simulator uses the opening price to update the peak price. Then, it checks whether the stop has been hit during the entire bar. If not, it proceeds to update the peak price using the highest (for long positions) or lowest (for short positions) price of the candle, and performs the stop check again, but now using the closing price alone to avoid the ambiguity that's been mentioned before. This way, the simulator always pessimistically assumes that the worst event (where the stop is being hit) happens before the best event (where the peak price is being updated).
To make the matter even more complex, a TTP order additionally needs to check whether its threshold has been crossed to be properly activated. The check is performed using the function check_tsl_th_hit_nb: it takes the initial and the peak price, and checks whether the difference between them is greater than or equal to the threshold. If so, the order gets converted into a regular TSL order. But if the difference is smaller, the simulator proceeds to update the peak price using the current candle, and makes a second attempt to check for the threshold crossover. If the difference is finally greater, it uses check_stop_hit_nb, but it cannot use the current candle anymore since it's unknown whether the stop has been hit after or before the threshold has been crossed, thus only the closing price is used by disabling the argument can_use_ohlc. Here's a depiction of what's happening:
What happens if multiple stops have been hit? The simulator pessimistically assumes that SL comes first, TSL and TTP come second, and TP comes last. The pending stop that comes before others gets executed while the remaining pending stops get canceled. 
After that, the winner gets converted into the four signals using the function generate_stop_signal_nb, which is based on the current position and the default stop exit behavior defined using the argument stop_exit_type of the type StopExitType. For example, instead of closing out the position, we can make the function reverse the position by using StopExitType.Reverse:
As we can see, the short entry signal is True while other signals are False. The number after the signals is the selected accumulation mode of the type AccumulationMode for a case where we want to reduce the position instead of closing out or reversing it. What's left is the resolution of the stop exit price, done by the function resolve_stop_exit_price_nb. The logic is quite simple: if the argument stop_exit_price of the type StopExitPrice is StopExitPrice.Close, use the closing price, otherwise use the stop price that has been hit. The argument can also be provided as an actual price.
Finally, the order signal gets converted into a market or limit order specification the same way as a user-defined signal. See Signal conversion.
Apart from the possibility to update any stop in a callback, there is a possibility to update the stop automatically once the current position has increased. In such a case, the argument upon_stop_update of the type StopUpdateMode controls whether any current stop should remain or be reset. This decision is done by the function should_update_stop_nb. For example, our position has increased, and we want to know whether the current stop should be updated given the option StopUpdateMode.Override:
But if the new stop value is NaN (i.e., no stop), we shouldn't update:
Unless the option is StopUpdateMode.OverrideNaN, which will effectively disable all stops:
This won't apply to the case where the current position has decreased. But why should we care about updates if signals usually either open or close positions, and don't increase or decrease them? When using accumulation, signals can add to the position or remove from the position. In this case, we should ask ourselves: should this position change invalidate the stops we defined previously, or define new ones? The argument upon_stop_update controls this behavior.
Similarly to updating, cancellation of the currently pending stop orders happens when the position has been closed out. This will clear all the stops automatically. But also similarly to limit orders, there is a possibility of a conflict with an active user-defined signal, which is resolved using the function resolve_pending_conflict_nb. The arguments that are used to resolve any pending conflicts for stop orders are upon_adj_stop_conflict and upon_opp_stop_conflict, both of the type PendingConflictMode. For example, if the user has decided to reduce the position and clear all the pending stops along with it, they can set the argument upon_adj_stop_conflict (reducing the position and attempting to close it are considered adjacent signals) to the option PendingConflictMode.CancelExecute.
We've covered some theory on how this simulation method works. Let's take a break from reading and concern ourselves with signal arrays, which together with a signal function are the main input to this method. As we already know, signals come in two flavors:
The first flavor is a compressed form of the second flavor, so to say: we can always convert direction-unaware signals into direction-aware, but not the other way round since the first format defines a total of 2 * 2 * 3 = 12 combinations and the second format defines a total of 2 * 2 * 2 * 2 = 16 combinations. On the other hand, the first format is easier to use since we can set the direction globally and just operate with two arrays instead of four.
But first, let's fetch the entire history of BTCUSDT and ETHUSDT for our examples below:
As we won't need the entire history to illustrate most concepts, let's select the week of data between the 18th and 24th February 2021, where there is a substantial change in price in both directions:

Let's try passing the data without signals:
By default, all signals are set to False, thus no orders were generated. 
Now, let's assume that our ML model has correctly predicted the peak on the 21st February and ordered us to enter a position on the 18th February and close it on the 21st February. For this, we need to build our entry and exit arrays of the same shape as our data. But instead of specifying the same signals for each asset redundantly, we can provide a Series instead of a DataFrame, which will be applied to each asset thanks to broadcasting:
We can see that the first order in the column BTCUSDT is a buy market order that opened a new long position. The second order is of the same size but the opposite side, thus it was used to close out the long position. Reading orders isn't always straight-forward, especially when it comes to determining when positions are opened and closed. To get a better overview, let's calculate and print the position for each symbol at each bar:
The returned array represents the position at the end of each bar, hence we're still in the market on the 20th February but went out of the market on the 21st February.
We provided the same array for each symbol, but what if our ML model told us that the peak for ETHUSDT is one day ahead of that for BTCUSDT? As soon as our signal specification starts varying with columns, we need to build the respective signal array as a DataFrame with values defined per element. Let's keep the entry array the same for both symbols (entry signals do not vary with columns in our case) and only expand the exit array:
We can now observe that the long position in the column ETHUSDT was cleared one day before that of BTCUSDT, as our imaginary model wished. To make array creation a bit easier and to avoid setting each element manually, we can use the symbol wrapper of the data instance to create empty boolean arrays of the same shape as the data, and fill them on specific dates:
For readers who appreciate hot features, here's how to let the vectorbt's broadcaster create both arrays and fill them dynamically using index dictionaries:
The best in the approach above is that the broadcaster won't create arrays bigger than necessary: it will notice that the entry specification is the same for both symbols and create an array with one column instead of two, saving us some memory.
In all the examples above, we provided only two arrays: entries and exits. Whenever this happens, the method treats the provided signals as direction-unaware, meaning that an additional argument direction is used to control their direction. By default, the direction is Direction.LongOnly (see signal_direction in portfolio settings). To change the direction, override direction with any option available in Direction. An option can be provided either as an integer or a field name. For example, let's allow both directions to reverse the position on exit instead of just closing it out:
Zero values have changed to negative values, which means that positions are now being reversed. Similarly to the arguments for signals, the argument for direction can also be provided as an array. This makes possible defining different directions for different dates and symbols. For example, let's long BTCUSDT and short ETHUSDT. Thanks to broadcasting, to provide any information per column, we can represent it as a two-dimensional array with just one row:
The position values under the column ETHUSDT have turned negative indicating that we're in a short position. The following example illustrates the usage of direction defined per element by entering a long position at the first bar, exiting it at the peak, then entering a short one at the next bar, and finally exiting it at the last bar:
Note
The numeric format should be preferred over the string format for bigger arrays since strings must be converted to integers prior to the simulation, which is a relatively slow operation.
Direction-aware signals are a more flexible form of signals as they allow for more signal combinations. To switch to this mode, we need to provide the arguments short_entries and short_exits acting as short signals along with the arguments entries and exits acting as long signals. This will disable the direction argument completely as the direction is now expressed by the signals themselves. Let's adapt the example above:
So, when do we use which signal mode? Use direction-unaware signals when working with a single direction throughout the entire column, and direction-aware signals for more granular decisions, especially when positions need to be closed when both directions are allowed, for example, to close out any position at the end of a day.
Passing signals as pre-defined arrays has one major advantage: caching. Even after we restart the runtime, there won't any recompilation when we pass signal arrays of the same format again. But sometimes there is a need to trade in a bit of performance for more flexibility. Such use cases include path-dependency scenarios where signals depend on the previous or current simulation state such that they cannot be generated in advance. Another use case is to reduce RAM usage: the entire indicator and signal generation logic can be encapsulated into a single signal function such that there is no need for intermediate arrays anymore. This is useful when a huge number of parameters needs to be tested, or when the user wants to choose a number of assets to trade out of a big universe of assets, which usually involves keeping very wide arrays in RAM but using a signal function would make them redundant.
Let's implement the latest example above but without any arrays!
We've switched a vectorized logic in favor of an iterative logic, which is usually more verbose but also much more flexible and similar to the format used in most open-source backtesting frameworks. But that doesn't mean that we have to define everything iteratively, we can still pass one or more arrays and make decisions based on them. All we have to do is to make the signal function accept arrays as positional arguments and select one element out of them at each time step to generate the four signals, and then pass the actual arrays to FS as a tuple using signal_args. Just note that any array-like object must be a NumPy array since Numba doesn't understand Pandas.
But what if we wanted to expand our data to multiple assets? The example above would only work if each of the arrays is kept as one-dimensional since we only select rows in the signal function. To make our logic shape-agnostic though, we need to make each of the arrays two-dimensional and additionally select the current column in the signal function. But that's not enough: we also need to take care of broadcasting, which can be flexibly done in the signal function either manually with flex_select_nb or automatically with select_nb:
Our strategy can now be applied on an arbitrary number of columns, great! But even this isn't the most flexible design  What if the user provides a signal array that doesn't have the same number of rows as the data? If bound checking is enabled, we would get the "index is out of bounds" error since the signal function would attempt to select an element that simply doesn't exist. To make any array broadcast against the data prior to the simulation, we can define it in the dictionary broadcast_named_args, and then use a template to substitute its name by the broadcasted array in signal_args:
Our setup now behaves the same way as the built-in arguments entries, exits, short_entries, and short_exits  We don't even have to convert them to NumPy arrays anymore since this is being taken care of automatically by the broadcaster. It allows us also to use index dictionaries and other hot broadcasting features:
But we're rarely interested in backtesting signals on some fixed dates. Let's create a signal function that yields a long entry signal when there is an above-crossover and a short entry signal when there is a below-crossover of two moving average arrays. In addition, we'll parametrize this strategy by introducing a flexible parameter wait that controls the number of bars after which a signal should be placed after a crossover has been detected; if an opposite crossover happens during that time, the signal gets canceled, thus wait also acts as a confirmation period. The parameter will broadcast together with data to be able to define it per row, column, and element.
To confirm that our strategy has generated a correct number of orders, let's get the total number of crossover signals manually:
To demonstrate the full power of the vectorbt's broadcaster, let's test the confirmation period of 0, 1, 7, and 30 bars by wrapping the parameter with Param:
We can observe how the number of orders gradually decreases with a longer confirmation period. If you love vectorbt for its performance, you might argue that even though the number of crossovers is low, having the second loop is not exactly optimal performance-wise: we can rewrite the logic to iterate over the data just once. For this, we need to introduce a temporary array that stores the index of the latest crossover that has been confirmed so far, and once the confirmation period is fully over, we can issue a signal. This use case is a perfect example on how to temporarily store and then share data between multiple signal function calls!
The temporary array that we will create will be a one-dimensional NumPy array where the latest crossover index is stored per column. We could have also used a regular typed list but remember that NumPy arrays enjoy superiority in Numba. Why per column and not just an array with one value? This would work for ungrouped portfolios where columns are processed strictly one after another, but in a case where the portfolio is grouped, columns are processed in a zigzag manner inside their groups, thus we should always structure our temporary data per column to be on a safe side. Another challenge is the creation of such an array: how do we know the number of columns in advance? Thankfully, we can use a template!
The code above isn't even complex: it would take the same amount of code to define this logic in a conventional backtesting software. The only difference is that vectorbt relies on functional programming whereas other frameworks rely on object-oriented programming where functions such as crossed_above_nb are methods of the current backtesting instance (self.crossed_above()) and variables such as temp_coi are attributes of that instance (self.temp_coi).
When signals are generated in an automated way, it often happens that multiple signals of the same type come one after another, or multiple signals of different types appear at the same bar. The first case is effectively solved by the FS method by taking into consideration only the first signal while ignoring others (unless accumulation is turned on), as demonstrated in the following example where we're repeatedly issuing a long entry signal:
But what if we start issuing a long exit signal at the same time?
We can see that the simulator simply ignored conflicting signals. But sometimes, we may want to prefer one signal type over another. In the example above, we have a so-called "long signal conflict", the resolution of which is controlled by the argument upon_long_conflict of the type ConflictMode. For example, if long entries are more important than long exits:
What about use cases where we allow both directions for an exit signal to become a short entry signal? If so, we would have a so-called "signal direction conflict", the resolution of which is controlled by the argument upon_dir_conflict of the type DirectionConflictMode. Let's prefer short signals over long signals, in any direction:
We can see that both orders have become sell orders. Let's combine both use cases and apply our knowledge to the scenario where all four signals are set! We will open a long position at the first bar, while at other bars only the one signal that's opposite to the current position will win and reverse the position:
Great, we forced vectorbt to reverse the position at each bar 
As we've learned, signals are just another level of abstraction over orders; they control the timing and direction of orders. But how do we specify the parameters of a typical order a signal should be converted into? Identically to Portfolio.from_orders, the class method Portfolio.from_signals takes a variety of order-related arguments; in fact, it takes all the arguments that can be found as a field in the class Order. If any argument remains None, the method takes the default value defined in the portfolio settings. For example, the default size is np.inf, meaning that each signal instructs the simulator to use up the entire capital. Also, each order-related argument is array-like and broadcasts together with signals. This way, by setting an argument to a single value, we can enable that value for each signal. Let's make each entry signal order $1 worth of each asset by tweaking the size and size type arguments:
As we can see, the simulator ordered 1 / 51552.60 = 0.000019 units of BTCUSDT and then closed out the position. To have a more granular control over any order information, we can specify the information as an array. For example, let's enter a position with 50% of the available cash, close the position, then open a new position of the opposite direction with 25% of the available cash, and close the position again:
Thanks to the vectorbt's powerful broadcasting mechanism, we can backtest arbitrary configurations with a couple lines of code. Below, we're testing three different mutual configurations of the size and size type arguments:
Each configuration is getting applied on the entire set of signal arrays.
If we wanted to sell 1$ worth of each asset instead of closing out the entire position whenever an exit signal is encountered, we need to turn on accumulation:
There is a leftover in the column BTCUSDT since we made a profit, while the position has been closed entirely in the column ETHUSDT since we made a loss ($1 is worth less than at the beginning of the simulation). But there is another implication: whenever the accumulation is turned on, the method starts behaving effectively like Portfolio.from_orders. For instance, it treats each signal as an order, irrespective of whether we're in the market or not. This is best illustrated by the following example, where we're issuing a long entry signal at each single bar, without and with the accumulation:
We can see that without the accumulation, only the first signal is getting executed while all the signals that follow are getting ignored since we're already in a position. With the accumulation though, each signal gets executed irrespective of the current position. This allows for pyramiding and other trading schemes that require binary position requirements to be lifted.
There is a variety of size types supported. To get the full list, refer to the enumerated type SizeType. Each size type can be provided either as a (case-insensitive) string representing a field name, or an integer representing a value, such that the arguments size_type="value" and size_type=SizeType.Value will behave identically. Most size types will be internally converted into the size type Amount, which is the absolute amount of units to order. The conversion is mostly done using the valuation price val_price, which defaults to the order price and is meant to be the latest price at the point of decision-making.
Note
When working with Numba-compiled functions directly, only the integer format is supported.
But not all size types are supported in Portfolio.from_signals: any target size that defines a target, such as TargetAmount, TargetValue, and TargetPercent(100), cannot be safely used since the final order size may contradict the signal. For example: if we're in a position of 10 units, and we've issued an entry signal with the size of 3 units and the size type of TargetAmount, the actual order will be a sell order of the size 7, which is opposite to the direction of the signal that we've issued. Additionally, the size type Percent cannot be used in certain circumstances when both directions are allowed, such as when reversing a position, because such a percentage cannot be simply "flipped".
Size is always represented as a (usually 64-bit) floating-point number and the entire simulation logic of vectorbt is also based on this number format. But, as you might have heard, floating numbers aren't exactly ideal for monetary calculations because the floating-point arithmetic often causes a loss of precision. Since Numba doesn't allow us to use fixed-point numbers, vectorbt is forced to use floating-point numbers and apply a range of tricks to compare them with a certain confidence, such as by using numpy.isclose and numpy.round_.
But what about use cases where the size needs to be an integer, such as when trading stocks? For this, we can use the argument size_granularity, which will round off the final absolute size to some number of decimal places. As a rule of thumb: use 1 for whole shares, 0.001 for fractional shares, and some custom value for crypto. For example, Binance provides the step size of each trading pair, which can be directly used as size_granularity.
Info
Even though the traded size appears as an integer, it's still represented by a float.
By default, vectorbt executes an order right away using the current closing price. This behavior can be changed by tweaking the argument price, which can either take a price array, or a price option of the type PriceType. If we take a look into the portfolio settings, we'll find that price is set to the string "close", which upon running is getting translated into the option PriceType.Close. This option is just an alias for the value np.inf (positive infinity). What does an infinity do here? Since an order price must be defined within the price bounds of a bar, the negative and positive infinity represent the opening and closing price respectively (see Price resolution); both can be used inside arrays. The other two options NextOpen and NextClose are standalone options and cannot be used inside arrays because they involve other arguments.
Important
Which option to choose depends on which price you used to generate your signals. Mostly, signals are generated and executed using the same closing price. To account for potential time gaps between signal generation and execution, use the next open or close, or shift the signals manually. If you generated the signals using the opening price or any other price preceding it, you can also use the option "open" and a bit of slippage.
Let's execute entries using open and exits using close. Remember that each order-related argument is defined for all signals types: long entries and exits, and short entries and exits. That means that there isn't any argument that defines price specifically for exits, otherwise, the number of arguments would skyrocket. To make any argument value valid only for a subset of signal types, we should set it by using the signal types a mask:
Let's order using the opening price of the next bar instead:
As we can see, the simulator waited one bar and then executed each signal using the opening price. This is one of the safest approaches when it comes to backtesting because now we have the freedom of running indicators on any price, and we don't have to worry about the look-ahead bias during the execution anymore. The most bulletproof approach would be using the next close because the difference between the previous close and the next open is usually negligible.
We could have done the same as above by shifting the signal arrays by one bar manually:
And this is one of the most underrated features of vectorbt: since we're working with array data, we can shift the data to make any current bar use information from the past. In the example above, after we've forward-shifted the signal arrays, the long entry signal on the 18th February has moved to the 19th February while the index has stayed the same. Thus, the surrounding price information in form of OHLC (the arguments open, high, low, and close) must be fixed and should never be shifted! That also means that we not only have to shift the signals, but any information those signals link to, for example, the order direction and price:
Hint
As a rule of thumb: if an argument is signal-anchored, it should be shifted as well. If an argument is date-anchored though, it should stay the same.
To reduce the burden of shifting manually, vectorbt can do it for us automatically! For this, it requires from us providing the argument from_ago, which represents how far ago was the bar from which all the signal and order information should be taken. For example, when from_ago=1, the related information is taken from the previous bar:
This argument can also be provided as an array.
By default, an order is executed as a market order. This can be seen in the portfolio settings, under the key order_type. Market orders are transactions meant to execute as quickly as possible at the current market price, which is always price (the closing price by default). In reality though, the price at which an order is executed often does not match the price at which it was requested. To take into account this discrepancy, we need to introduce a slippage. Assuming the trading volume is high, we should see less slippage. And when the trading volume is slow, we should expect to see more slippage. An optimal slippage can be calculated from order book data (see this blog).
But let for a second assume that the average slippage is 0.5%. Using it together with the default price is generally a bad idea since the closing price is meant to be the latest price seen at each bar. To make our simulation more realistic, we need to apply it on the next open instead:
We can see that the slippage increased the price by 0.5% when buying and decreased the price by 0.5% when selling. Introducing a slippage will always result in a fixed price penalty, thus the slippage should always reflect the average penalty that is being recorded in the market for transactions of this magnitude. Since slippage isn't static but depending on various factors, we can provide it as an array.
To help eliminate or reduce slippage, traders use limit orders instead of market orders. A limit order only fills at the price we want, or better. Unlike a market order, it won't fill at a worse price. But there is a catch: while the price is guaranteed, the filling of the order is not, and limit orders will not be executed unless the asset price meets the order qualifications. If the asset does not reach the specified price, the order is not filled, and we may miss out on the trading opportunity. So, what happens when we execute our signals using limit orders? Let's switch the default order type by setting the argument order_type to "limit":
Since the default order price is the closing price, the simulator registered it as the target limit price and skipped the entry bar. At the next bar, the simulator checked whether the limit price has been hit by comparing it against the full candle. As we can see above, each of the target prices could be satisfied already at the next bar, which is quite similar to using the next opening price as the order execution price. But such a rapid match isn't always the case: what if we wanted to enter a short trade on the 22nd February using the previous high as the limit price?
Suddenly, no limit order could be filled as the same price or higher (we're selling) couldn't be found at any time during or after the 22nd February. We have now an order that can potentially remain pending forever. How do we limit its lifetime? There are various possibilities.
Time-in-force orders can be created using the argument limit_tif, which expects a time delta, in any format that can be translated into np.timedelta64 and then to a 64-bit integer representing nanoseconds, that is, a string, pd.Timedelta, datetime.timedelta, np.timedelta64, or an integer. The time-in-force specification starts counting at the beginning of the entry bar (even if the limit order has been issued at the end of the bar) and will be checked at the beginning of each bar starting from the second one. Let's create a buy limit order on the 20th February using the previous low price:
We can see that the limit order in the column BTCUSDT was executed in two bars while the limit order in the column ETHUSDT was executed in just one bar. Below, we're testing a TIF option passed as a frequency string, which works only outside of arrays:
Why is there is no order in BTCUSDT? Because 2 days from 2021-02-20 00:00:00 is 2021-02-22 00:00:00, which is the timestamp at which the order gets cancelled, and since each timestamp represents the opening time of a bar, the pending order doesn't exist during the fifth bar anymore. To provide a TIF specification inside an array or to test multiple configurations, instead of a string, we must use either a Pandas or NumPy format, or a total duration in nanoseconds. Let's test all formats at once!
Note
NumPy and integer formats are preferred when building large arrays.
There is also a way to specify a number of rows as opposed to a time delta by tweaking the time delta format (time_delta_format), which is represented by the enumerated type TimeDeltaFormat. This is required in cases where the index is not datetime-like, such as after splitting the data for cross-validation. Let's change the time delta format to Rows and do the same as above but now waiting for a specific number of rows to pass:
If we're confident that our index has a fixed frequency (for example, days instead of business days) and doesn't have gaps, we can also determine the number of rows by dividing time deltas:
Another way to let limit orders expire based on dates and times is by setting an expiration date with limit_expiry. We've got two meaningful options here: setting a frequency at which limit orders should expire, or providing a datetime-like array that can incorporate pd.Timestamp, datetime.datetime, and np.datetime64 objects. In the end, the argument will be converted into the 64-bit integer format representing a Unix time (total nanoseconds since 1970). Let's play with the first option first by making limit orders behave like day orders! By receiving a frequency, the simulation method will determine the right bound of each timestamp based by using the method ArrayWrapper.get_period_ns_index:
But since our data is already daily, let's make a pending limit order expire at the end of the week in which it was created:
Both orders could be executed because the dates 2021-02-20, 2021-02-21, and 2021-02-22 belong to the same week 2021-02-16/2021-02-22. As soon as we had changed the week layout to start on a Sunday though, only the first two dates would belong to the same week, thus resulting in the expiration of the pending order in the column BTCUSDT:
We can also build our own limit_expiry array. For instance, let's simulate a TIF of 2 days by relying on the expiration dates alone - the same as passing limit_tif="2d":
Note
Don't attempt to pass a pd.Index directly, it will be converted into parameters. Use pd.Series instead.
What happens if the user issues a signal when there is a limit order pending? Since FS can track only one limit order per column at a time, we need to choose a winner. There are two arguments that are relevant: upon_adj_limit_conflict and upon_opp_limit_conflict, both of the type PendingConflictMode. The first controls the decision process if both orders attempt to go in the same direction, the second if their directions are opposite. Let's introduce a sell market order on the 21st February and test various conflict resolution options:
As you might have noticed, the first word in the option's name means what to do with the pending limit order, and the second word means what to do with the user-defined order. By default, the limit order wins when the user-defined signal is of the same direction (buy and buy, sell and sell, option KeepIgnore) to avoid repeatedly executing similar signals, and the user-defined signal wins when both orders are of the opposite direction (buy and sell, sell and buy, option CancelExecute) to account for a change in the market regime.
The target price of a limit order is taken from the argument price. But what if we wanted to place a limit order at a price some distance (a.k.a. "delta") above or below some another price, such as 10% from the current closing price? For this, we would need to know whether a signal is a buy or sell order beforehand: if it's a buy order, the price should be lower than the closing price, and if it's a sell order, the price should be higher than the closing price. To avoid determining such information in advance and building a price array manually, there is a handy argument limit_delta that specifies how far away from price the target limit price should be. The format of this argument is controlled by another argument delta_format of the type DeltaFormat. The default format is a percentage (Percent), such as passing limit_delta=0.1 will be understood as 10%. Let's try out different deltas for a buy limit order at the very first bar:
As we can see, without a delta, the target limit price is essentially the current closing price. With a limit delta of 10%, the target limit price becomes close * (1 - limit_delta), which in our example have only been matched after a few bars. The final limit delta of 50% haven't been matched at all since the price window that we chose doesn't have dips of such a magnitude. If our limit order was a sell order, the calculation would be close * (1 + limit_delta).
We've covered regular buy/sell limit orders that get matched whenever the price is found to be the same or lower/higher than the requested price. Now, we can reverse the matching logic with the argument limit_reverse to simulate buy/sell stop orders. For instance, for a buy stop order with an absolute delta of $100, it would search for a price that is $100 higher (as opposed to lower) than the current closing price and execute the limit order using that price:
Info
Why does the same delta of zero result in a different fill price without and with reversing? Because upon hitting the second bar, the limit price is getting compared against the opening price. In the first example, the opening price is higher than the limit price, thus the limit order is executed using the limit price. In the second example, because the opening price is higher, the limit order is executed using the opening price.
In the theoretical section we discussed how it's possible to change any limit order from a callback. There are three callbacks that we can use: a signal function (signal_func_nb), an adjustment function (adjust_func_nb), or a post-segment function (adjust_func_nb). The signal function is called at the beginning of a bar and should only be used if we need to define the four user-defined signals dynamically. The adjustment function is called at the beginning of the default signal function dir_signal_func_nb for direction-unaware and ls_signal_func_nb for direction-aware signals, which are provided via boolean arrays (statically). The post-segment function is called at the end of a bar for pre-calculating metrics such as portfolio returns. 
Note
The adjustment function gets called only if the signals are provided statically, that is, signal_func_nb hasn't been overridden. Also note that overriding any of the functions above will disable caching of Portfolio.from_signals!
Each of those functions accepts a named tuple with the current simulation context, which includes a one-dimensional record array SignalContext.last_limit_info with the latest limit order information defined per column. By changing this array, we can change the limit order that is yet to be processed at this bar (or the next one for the post-segment function). We will focus on the adjustment function since it's meant to work together with signal arrays. This function should be Numba-compiled, take the current context variable (usually denoted with c) and other custom arguments (adjust_args), and return nothing (None). Why nothing? Because we can change all the relevant information in the context. Let's create a function that cancels any pending limit order (that is, it hasn't been executed at the entry bar). Limit orders will be created using the opening price and a parametrized custom delta:
We can see that without a delta, each order could be filled already at the entry bar. With a delta of 10% though, the orders couldn't be filled at the entry bar, and thus they were cancelled at the beginning of the second bar (19th February). Let's do the opposite: if a limit order is still pending, override its price with the valuation price and delta with 1% to execute at this bar:
The same way as we cancelled an order, we can also create a new order without the need of user-defined signals - basically out of nothing!
We just created limit orders dynamically, without any signals 
Just like limit orders, stop orders are executed only once a price condition is met. Their information is also stored per column inside a one-dimensional record array. But in contrast to limit orders, they are just deterred user-defined signals that result in a market or limit order to mainly close out an existing position. They are created upon a successful entry order that either opens, increases, or reverses a position (i.e., closes out and then enters a new position in the opposite direction). There are three stop order types: SL (sl_stop), TSL and TTP (tsl_stop and tsl_th, acting as one order), and TP (tp_stop). Each one of them is stored and tracked separately, but they all act as a one-cancels-all (OCA) order where one order is triggered in full while the others are automatically canceled. Any pending stop orders get also canceled if the entered position has been closed out by other means.
Let's place a stop loss order of 10%:
Important
When passing an instance of Data as the first argument (as we did with sub_data), the method will extract OHLC from it automatically. If you're passing the closing price as the first argument instead, make sure to pass other price features (OHL) as well, using the arguments open, high, and low. Without them, candles will be incomplete and vectorbt will make decisions solely based on close!
We can see that the entered long position has been exited at the price 51552.60 * 0.9 = 46397.34. To test multiple stop values, we can wrap it with the class Param:
Since each stop argument and its accompanying information broadcast together with the data, we can provide them as fully-fledged arrays. Below, we're defining a stop loss based on the ATR:
What about absolute stop values? They can be specified the same way as for limit orders:
We can also provide a target stop price directly:
Since the simulator can keep track of multiple stop types at the same time, we can simulate a specific risk-to-reward (R/R) ratio by setting a stop loss and a take profit. For instance, let's simulate the risk-to-reward ratio of 1/2 based on the ATR:
The TP order won in the first column, the SL order won in the second column. But is there a way to see the order type and preferably the date of the signal that initiated it? Sure, vectorbt can fulfill all of your wishes! 
As we can read form the first DataFrame, the first order is a buy market order (Side and Type) that was filled on the 18th February (Fill Index). The second order in the same column is a sell market order resulting from a TP order (Stop Type) that was issued on the 18th February (Signal Index) but actually filled on the 21st February (Fill Index). The column Creation Index is the same as the column Fill Index since all orders are market orders. The second DataFrame is a common representation of a single piece of information in relation to the time and asset; it's best used when the information should be analyzed as a time series with Pandas.
Finally, let's talk about multiple stop configurations. Here's how to test each stop type independently, that is, SL of 10%, TSL of 10%, TTP of 10% with a 10%-threshold, and TP of 10%:
To build a product of various stop types, we can specify unique (non-repeating) values and omit the level argument:
Warning
The number of columns generated from a Cartesian product of multiple parameter combinations grows exponentially with the number of parameter combinations - show mercy to your RAM!
But this would include a combination where tsl_th is not NaN but tsl_stop is NaN, which doesn't make any sense. What we need though are three combinations: no TSL/TTP stop, TSL stop, and TTP stop. Thus, we need to combine those two arguments manually and link them by the same level:
Note
If some arguments require a level, it must be defined for all arguments.
We can then call some metric and analyze it using Pandas. For example, let's calculate the average return of using 1) either SL or TP, or 2) SL and TP together:
Seems like combining SL and TP works better on our "huge" time series with 7 points 
The examples above work only if we're testing arguments provided as scalars (single values). How can we backtest various combinations of stop values provided as arrays though, such as those computed from the ATR? Performing that manually would be highly difficult because we would need to tile each stop type array by the number of stop values and then combine all stop type arrays using the Cartesian product. An easier approach is to build a basic indicator that does the preparation of arrays for us:
The entry price of stop orders can be configured using the argument stop_entry_price: it can take either the price itself, or any option from the enumerated type StopEntryPrice. To be able to distinguish between real prices and options, the options have a negative sign. By default, the entry price is the closing price, even if the entry order itself was filled before the closing price, such as at the opening price; this is because we cannot fill a stop order at the same bar as the entry order, that's why we're postponing the first check to the next bar. Let's see what happens if we define a stop order that can be executed already at the first bar:
As we can see, even though the target price 52378.26 could theoretically be filled at the first bar as it's lower than the highest price of 52530.00 of that bar, the check was still postponed to the next bar because of the FS's limitation of one order per bar and asset.
The number of options to define a stop exit price is relatively small: the stop price and the closing price. If the stop has been matched at the beginning of a bar, the stop price will become the opening price. The same for the closing price. By default, a stop order is executed using the stop price, but if the entire simulation should be done on the closing prices only, the second option might be more appropriate.
This way, we're effectively delaying the execution of a stop order until the end of a bar. Not only we can change the default stop exit price, but also the default stop exit behavior: using the argument stop_exit_type of the type StopExitType we can reduce/reverse/reverse+reduce the position instead of closing it. By default, the position is being closed out by issuing an exit signal and disabling accumulation, but we can also reverse it by issuing an opposite entry signal and/or reduce it by keeping the default accumulation:
Take a look at both columns under the parameter value reverse: they now have five orders instead of two, how so? By reversing, we're closing the existing position and opening a new one but in the opposite direction, using a single order. Opening a new position will create a new stop order, thus we've created a never-ending cycle of conditional reversals  If such a cycle is unwanted, we can define stop_exit_type as an array where only a particular stop is closed or reversed. Each value in this array should be located at the entry point rather than exit point because this information (along with stop_exit_price) is order-anchored and not date-anchored! For example, let's reverse the first order only:
The first position was reversed while the second position only exited.
What about order type? Upon exit, the order type becomes either a market or limit order. This behavior is controlled by the argument stop_order_type of the type OrderType. There is also an argument stop_limit_delta to specify a delta for the resulting limit order! Let's test a 5% TP order as a stop market order vs. stop limit order with a delta of 1%:
The latest stop limit order couldn't be filled because the highest price in the entire column ETHUSDT is 2042.34 while the requested limit price is 1939.61 * 1.05 * 1.01 = 2056.96. But even if there was a price higher on the 20th February: unless the stop price was triggered at open, the simulator wouldn't be able to use the entire candle to match the limit price because there is no intra-bar data to prove that the limit price can be hit strictly after the stop price. The only information the simulator could have used in this case is the closing price: only if it was higher than the limit price, there would be a guarantee that the limit price has actually been hit.
Conflicts between stops orders and user-defined signals enjoy the same resolution logic as limit orders, the only difference is in the argument naming: upon_adj_stop_conflict and upon_opp_stop_conflict. By default, whenever a user-defined signal in any direction is encountered, nothing happens: any stop order remains pending and the signal gets executed as usual; that's because pending stop orders are automatically cancelled whenever the position gets closed out. Let's cancel any signal when there is a stop order pending:
Similarly to limit orders, stop orders are also stored in one-dimensional record arrays where information is laid out per column: SignalContext.last_sl_info for SL, SignalContext.last_tsl_info for TSL and TTP, and SignalContext.last_tp_info for TP. For instance, create an adjustment function that creates one TP and one SL order based on a maximum allowed, absolute, parameterizable profit and loss (PnL):
Hint
To avoid re-compiling the entire simulation function with each new run, define the adjustment function in a different cell or even file. Caching won't help though since Numba doesn't allow for caching callbacks.
Our losses and profits are now capped by $10! Here's what happened. Whenever we provide a custom adjustment function or any other callback, the simulator switches to the flexible mode and enables stop orders. Then, whenever a new position is opened or an existing one is increased, the simulator initializes each stop type with the provided arguments. But since we haven't passed anything related to SL and TP, their default stop value is NaN, thus they are inactive by default. But regardless of that, they still contain valuable information, such as the initial price. Hence, what's left for us is just to check whether the respective stop is inactive and override the stop value to activate it. We could have also used the functions set_sl_info_nb and set_tp_info_nb to set the entire information, but there was no need for that.
We know how to define a single stop to close out an entire position, but what about moving out of a position gradually? For this, we need a way to provide multiple stop values. But there is a problem: the stop arguments should broadcast together with other broadcastable arguments, that is, when providing (for example) tp_stop as an array, the first axis should represent time and the second axis should represent columns. But thanks to a new single-axis broadcasting feature, we can notify the broadcaster that the array's rows don't represent time but something completely different.
On practice, Portfolio.from_signals has an argument stop_ladder, which when enabled can switch to single-axis broadcasting such that providing a list (or one-dimensional array) of values will be considered as a single ladder. We can also provide a two-dimensional array to specify the ladder for each single column.
Note
Enabling this argument makes all stop arguments behave like ladders - there's no way we can make one argument to be a ladder series and another argument to be a time series! Also, the argument stop_ladder is affective on all columns and cannot be provided per column.
In a ladder, each value represents a step with a stop value. Just like in real-world ladders, steps must be ordered from low to high such that a step is executed only after the previous one has been executed first. There's no limit on the number of steps. Let's enable laddering through the argument stop_ladder and build our first TP ladder with two values: 1% and 5%.
Both ladder steps were successfully executed and removed an equal chunk of the position. By default, the exit size distribution of the steps is uniform - the exit size depends only on the number of steps. But what if we wanted remove an amount from the position that is proportional to the step size? Step size in this scenario is the difference of the current stop price and the previous stop price (step range), relative to the difference of the last stop price and the entry price (full range). For this, we can provide the argument stop_ladder with the value "weighted", which corresponds to the mode StopLadderMode.Weighted (for possible ladder modes see StopLadderMode):
The first step removed (0.01 - 0) / (0.05 - 0) = 20% of the initial position while the second stop removed the remaining (0.05 - 0.01) / (0.05 - 0) = 80%.
Let's say we want to have two parallel ladders: SL and TP. What happens to the exit size distribution if both ladders are executed partially?
We see that the first TP step in the column BTCUSDT removed the half of the position because there are only two steps in the TP ladder. But then an SL step was hit and removed 33.3% of the initial position because there are three steps in the SL ladder. The final step then removed the remainder of the position. But what if our intention is to move out of the position respective to the current position and not the initial one? If the position suddenly changed, the exit sizes would be redistributed based on this change. This can be done by providing StopLadderMode.AdaptUniform or StopLadderMode.AdaptWeighted mode:
The both SL steps in the first column now remove exactly 1 / 3 of the new position and act as if they were created right after the position change by the first TP step.
Now, let's illustrate how to specify a different ladder for a different column. For this, we need to construct a two-dimensional array. Since some ladders may have a smaller number of steps than the others, we need to pad them with NaN:
The padding step wouldn't be necessary if we provided the ladders as parameters:
Finally, let's discuss how to specify our own exit size for each ladder step. Since there are no arguments for specifying the exit size, we need change it inside an adjustment or signal function. Remember to use an adjustment function if you already have signal arrays, and a signal function if you generate your signals dynamically. Inside the callback, we need to 1) retrieve the current information record of the stop type we've defined the ladder for, 2) select the exit size corresponding to the current step (available via the record field step), and 3) write the exit size to the record. Once the step is hit, the user-defined exit size will used instead of the default one.
Note
The default size and size type in any record are NaN and -1 respectively. Only once the step is hit, they will be internally replaced by the calculated values. Thus, you can check whether the ladder uses the default exit size by testing these fields against these values.
In the following example we go into a trade with 6 units, and then test two TP ladders: 1) remove 3 units at the level of 10% and the remaining 3 units at the level of 20%, and 2) remove 1 unit at the level of 1%, another 2 units at the level of 2%, and the remaining 3 units at the level of 3%.
We've learned that order-related arguments such as size act as a facade of the backtesting process with FS: they are meant to be anchored by time rather than signals, so we cannot change them dynamically. This is proved by the fact that any signal function (signal_func_nb) returns only signals and nothing else. But what if a signal needs to have a size or other order information different from the default one? 
Remember that vectorbt takes order information as arrays and processes them iteratively: it goes through each row and column, reads the current element, and uses this element along with the returned signals. Theoretically, if we override the current element in the signal function, vectorbt should pick it and use it as new order information! The only question is how to get access to the respective array if context (c) doesn't list it? Use templates! For example, by passing vbt.Rep("size") in signal_args, the method will search for a variable called size in its template context and pass it to the signal function. Conveniently for us, the template context contains all the information passed to the method, including the order-related arguments.
But there is a catch: to avoid consuming too much memory, vectorbt keeps most array-like arguments as two-dimensional arrays with only one element and uses flexible indexing (see this and this) to select that one element at each single row and column. That is, we either have to tell the broadcaster to build the full array and then override each element (arr[c.i, c.col] = ...), which is wasteful but still may be wanted if there is also a requirement to keep track of all the values used previously. Or, we can make an array with only one element per column and override that one element (arr[0, c.col] = ...) to make vectorbt use flexible indexing.
Let's demonstrate both approaches by buying for $10 when a long signal is encountered and selling for $5 when a short signal is encountered. Here's the first approach using a full-sized array:
Example
Can we somehow get access to the filled array after the simulation? Yes! The trick is to create an empty dictionary, pass it to the template, and save the created array to the dictionary inside the template expression:
Here's the second approach using a one-element array:
To demonstrate the full power of this trick, we'll create a custom entry ladder! Whenever a user-defined entry signal is discovered, it will be split into multiple smaller entry signals, each executed after a predefined number of bars and adding a predefined percentage of resources to the current position. Also, we'll show how to group arguments using named tuples and define custom record arrays that hold complex temporary information. Finally, we'll show how to parameterize the ladder and test two completely different ladders with a single simulation.
The strategy has correctly split the entry signal into a set of smaller entry orders distributed over time. The code above is a great template for defining dynamic signal strategies of any complexity.
Until now, we processed the columns BTCUSDT and ETHUSDT separately, that is, the second column is processed strictly after the first column. But what if there is a need to put both columns into the same portfolio for analysis? By introducing a grouping, we can make a group of columns to be treated as a single column (read more here). Here, we need to distinguish between grouping after and before the simulation.
When the main scenario is to simulate columns separately and then subsequently analyze them as a whole, we can group-by the columns after the simulation, either when querying a specific metric of interest:
Or, by quick-fixing the entire portfolio:
What the portfolio above did was aggregate various metrics such as the initial capital and time series such as the cash flows along the column axis. There are some time series that cannot be aggregated though, such as the asset flows:
We can still disable the grouping for individual metrics by passing group_by=False. What we cannot do after the simulation though is to introduce cash sharing because it can be only applied during the simulation where it has real consequences on the trading logic.
Adding a group-by instruction before the simulation may or may not have consequences on the simulation. For instance, grouping with cash sharing will always have consequences, while grouping without cash sharing will only have consequences when FS is run in the flexible mode (that is, when any of the default callbacks are overridden); in this case, columns are grouped for the user to take advantage of the grouping in the callbacks, while all the calculations are still done on the per-column basis as if there was no grouping at all. Let's make use of this fact and limit the number of active positions to just one in each group:
The first signal in the column ETHUSDT couldn't execute because there was already a position in the column BTCUSDT, but once it was closed, the second signal could finally go through.
Orders are sorted only in groups with cash sharing, and only in two cases: when orders in different columns should be executed in different bar zones (i.e., at different times) such that they need to be sorted by time, and when orders are instructed to be sorted by order value as part of the automatic call sequence enabled by call_seq="auto". Both cases cannot be combined! Let's take a look at the first case where the first column is executed using the closing price and the second column using the opening price:
As we can see, the second column was executed first and has ended up using the half of the capital. If both assets were executed in the same bar zone though, they wouldn't be sorted and would be just normally executed from the left column to the right:
Even though the second column requested $50 of assets, the simulator couldn't fulfill that request because the default initial capital is $100, which has been all used up by the first column. To sort by order value, use the automatic call sequence:
This has made the second column execute first. But if we attempted to sort by order value when the columns had to execute their orders at different times, the simulation would fail:
And it makes sense if we think more about it: if the simulator sorted both columns by value, the second column would come first, but how it's possible if it should execute at the beginning of the bar? Remember that each bar is divided into three zones: open, middle, and close. The only possibility to sort orders by value is to use either the opening or the closing point of the bar. We cannot use the middle of the bar because we don't really know what happens there. Since we give vectorbt clues on where in the bar an order should execute by using -np.inf (bar open) and np.inf (bar close) as order price, passing any custom prices would be considered happening in the middle of a bar and thus fail as well:
Even if we know that the provided array represents the closing price, vectorbt doesn't know about it. Thus, make sure to use exclusively any option from PriceType when the automatic call sequence should be enabled. If you're using stop orders, make sure to use PriceType.Close and StopExitPrice.Close to execute all orders at the closing price, for instance, for rebalancing. Limit orders, on the other hand, are never executed at the closing price, thus using them together with the automatic call sequence is hardly possible.
When the portfolio has been simulated, and we've got our new shiny portfolio instance, the first thing we usually do is run various statistics. Since most statistics are based on returns, the portfolio instance may sometimes need to repeatedly re-calculate the returns, which is a slow process because it has to translate order records into asset flows, cash flows, assets, cash, asset value, portfolio value, and finally, returns. Such operations are known as "reconstructions" because they aim to derive a certain property of the simulation post factum. Hence, performance analysis is usually the main performance bottleneck in a typical backtesting pipeline. Gladly, there is an argument save_returns that, when enabled, pre-calculates the returns during the simulation and makes them available for all the metrics that need them (read more here):
To check whether they make sense, compare them to the reconstructed returns:
Under the hood, FS creates a named tuple with an uninitialized returns array and gradually fills it at the end of each bar. Once the simulation is over, this tuple gets attached to the created portfolio instance under the attribute Portfolio.in_outputs:
Since most things returned by the simulator are in NumPy format, we can use the method Portfolio.get_in_output to wrap any array with Pandas:
But there is one catch: the argument save_returns is only available in the fixed simulation mode, that is, when none of the default callbacks have been overridden. As soon as we enter the flexible mode, we need to define our own in-output tuple under the argument in_outputs and fill it with any information we want, usually in the segment post-processing function post_segment_func. This callback takes the context SignalSegmentContext, which is the same as SignalContext but without the information on the current column since a segment encompasses multiple columns in a particular group at a particular row, hence, we need to go over the columns in the current segment manually. 
For demonstration, let's develop a post-segment callback that fills the portfolio returns the same way as done by the fixed FS mode! In addition, we will store the total portfolio return per column to avoid reconstructing it during the post-analysis phase:
The most impressive in the approach above is that any pre-computed metric that can be found among the attributes of the Portfolio class is automatically picked by any built-in property and method that requires it, such as Portfolio.stats. The only limitation is the inability to change the grouping after the array has been created; when this happens, the reconstructed version of the metric will be returned instead.
Backtesting shouldn't be complex! One of the most crucial factors in successful trading is a proper entry and exit timing. By parametrizing the timing and direction of our trades, we can theoretically achieve the worst and best-possible returns in any market, thus the representation of a trading strategy as a permutation of buy, sell, short-sell, and short-cover signals is almost "Turing-complete" in this regard. Reducing complex strategies into such a limited signal set has another advantage: we can enable and standardize backtesting of many trading strategies using the same piece of code. The method Portfolio.from_signals not only makes all of that possible, but it also encourages us to hack into the simulation to define our custom trading logic iteratively, or to change the default simulation behavior, making it the best of both vectorized and event-driven worlds. But not everything is appropriate to be represented using signals: strategies that heavily rely upon variations in order information, such as rebalancing strategies, are best implemented using other means. Gladly, vectorbt offers solutions to such problems as well 
 Python code
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
Once we develop a rule-based or ML-based strategy, it's time to backtest it. The first time around we obtain a low Sharpe ratio we're unhappy with, we decide to tweak our strategy. Eventually, after multiple iterations of tweaking parameters, we end up with a "flawless" combination of parameters and a strategy with an exceptional Sharpe ratio. However, in live trading the performance took a different turn: we essentially tanked and lost money. What went wrong?
Markets inherently have noise - small and frequent idiosyncrasies in the price data. When modelling a strategy, we want to avoid optimizing for one specific period because there is a chance the model adapts so closely to historical data that it becomes ineffective in predicting the future. It'd be like tuning a car specifically for one racetrack, while expecting it to perform well everywhere. Especially with vectorbt, which enables us to search extensive databases of historical market data for patterns, it is often possible to develop elaborate rules that appear to predict price development with close accuracy (see p-hacking) but make random guesses when applied to data outside the sample the model was constructed from.
Overfitting (aka curve fitting) usually occurs for one or more of the following reasons: mistaking noise for signal, and overly tweaking too many parameters. To curb overfitting, we should use cross-validation (CV), which involves partitioning a sample of data into complementary subsets, performing the analysis on one subset of data called the training or in-sample (IS) set, and validating the analysis on the other subset of data called the validation or out-of-sample (OOS) set. This procedure is repeated until we have multiple OOS periods and can draw statistics from these results combined. The ultimate questions we need to ask ourselves: is our choice of parameters robust in the IS periods? Is our performance robust on the OOS periods? Because if not, we're shooting in the dark, and as a quant investor we should not leave room for second-guessing when real money is at stake.
Consider a simple strategy around a moving average crossover. 
First, we'll pull some data:
Let's construct a parameterized mini-pipeline that takes data and the parameters, and returns the Sharpe ratio that should reflect the performance of our strategy on that test period:
Let's test a grid of fast_window and slow_window combinations on one year of that data:
 
Combination 990/990

It took 30 seconds to test 990 parameter combinations, or 30 milliseconds per run. Below we're sorting the Sharpe ratios in descending order to unveil the best parameter combinations:
Looks like fast_window=15 and slow_window=20 can make us millionaires! But before we bet our entire life savings on that configuration, let's test it on the next year:
The result is discouraging, but maybe we still performed well compared to a baseline? Let's compute the Sharpe ratio of the buy-and-hold strategy applied to that year:
Seems like our strategy failed miserably 
But this was just one optimization test, what if this period was an outlier and our strategy does perform well on average? Let's try answering this question by conducting the test above on each consecutive 180 days in the data:
 
Period 9/9

We've gathered information on 9 splits and 10 periods, it's time to evaluate the results! The index of each Series makes it almost too easy to connect information and analyze the entire thing as a whole: we can use the split level to connect elements that are part of the same split, the period level to connect elements that are part of the same time period, and fast_window and slow_window to connect elements by parameter combination. For starters, let's compare their distributions:
Even though the OOS results are far away from the best IS results, our strategy actually performs better (on average) than the baseline! More than 50% of periods have a Sharpe ratio of 0.96 or better, while for the baseline it's only -0.03. Another way of analyzing such information is by plotting it. Since all of those Series can be connected by period, we will use the period level as X-axis and the performance (Sharpe in our case) as Y-axis. Most Series can be plotted as lines, but since the IS sets capture multiple parameter combinations each, we should plot their distributions as boxes instead:

Here's how to interpret the plot above. 
The green line follows the performance of the best parameter combination in each IS set; the fact that it touches the top-most point in each box proves that our best-parameter selection algorithm is correct. The dashed orange line follows the performance of the "buy-and-hold" strategy during each period, which acts as our baseline. The red line follows the test performance; it starts at the second range and corresponds to the parameter combination that yielded the best result during the previous period (i.e., the previous green dot).
The semi-opaque blue boxes represent the distribution of Sharpe ratios during IS (training) periods, that is, each box describes 990 parameter combinations that were tested during each period of optimization. There's no box on the far right because the last period is a OOS (test) period. For example, the period 6 (which is the seventh period because the counting starts from 0) incorporates all the Sharpe ratios ranging from 1.07 to 4.64, which most likely means that the price had an upward trend during that time. Here's the proof:

No matter which parameter combination we choose during that period of time, the Sharpe ratio will stay relatively high and will likely delude us and make our strategy appear to be performing well. To make sure that this isn't the case, we need to analyze the test performance in relation to other points, which is the main reason why drew the lines over the box plot. For instance, we can see that during the period 6 both the baseline and the test performance are located below the first quartile (or 25th percentile) - they are worse than at least 75% of the parameter combinations tested in that time range:
The picture gives us mixed feelings: on the one hand, the picked parameter combination does better than most parameter combinations tested during 5 different time periods; on the other hand, it even fails to beat the lowest-performing 25% of parameter combinations during other 3 time periods. In defence of our strategy, the number of splits is relatively low: most statisticians agree that the minimum sample size to get any kind of meaningful result is 100, hence the analysis above gives us just a tiny glimpse into the true performance of a SMA crossover.
So, how can we simplify all of that?
 Python code  Notebook
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          




            Prevent this user from interacting with your repositories and sending you notifications.
          Learn more about blocking users.
        

              You must be logged in to block users.
            

        Contact GitHub support about this user’s behavior.
        Learn more about reporting abuse.
      

        Find your trading edge, using the fastest engine for backtesting, algorithmic trading, and research. 
      



Python





            3.4k
          




            533
          


        Your new Telegram buddy powered by transformers
      



Jupyter Notebook





            415
          




            105
          


        Documentation for data enthusiasts
      



JavaScript





            92
          




            10
          


        Some of my ML projects and Kaggle competitions
      



Jupyter Notebook





            15
          




            2
          


        Visualizing the Global Terrorism Database (GTD) with D3.js
      



Jupyter Notebook





            18
          




            7
          


        Applications using state-of-the-art in NLP
      



Jupyter Notebook





            5
          




            3
          


    Seeing something unexpected? Take a look at the
    GitHub profile guide.
  
