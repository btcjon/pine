>>> import numpy as np
>>> import pandas as pd
>>> from numba import njit
>>> import vectorbtpro as vbt

vectorbtpro
vectorbtpro
vectorbtpro
vectorbtpro
>>> import numpy as np
>>> import pandas as pd
>>> from numba import njit
>>> import vectorbtpro as vbt

vectorbtpro
vectorbtpro
vectorbtpro
vectorbtpro
repo
conda create --name vectorbtpro python=3.10

conda activate vectorbtpro

vectorbtpro
*
conda info --envs

pip
pip uninstall vectorbt

git+https
pip install -U "vectorbtpro[base] @ git+https://github.com/polakowo/vectorbt.pro.git"

GH_TOKEN
pip install -U "vectorbtpro[base] @ git+https://${GH_TOKEN}@github.com/polakowo/vectorbt.pro.git"

git+ssh
pip install -U "vectorbtpro[base] @ git+ssh://git@github.com/polakowo/vectorbt.pro.git"

pip install -U git+https://github.com/polakowo/vectorbt.pro.git

develop
pip install -U git+https://github.com/polakowo/vectorbt.pro.git@develop

%env
%env GH_USER=abcdef...
%env GH_TOKEN=abcdef...

!conda install -c conda-forge ta-lib --yes

!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
!tar -xzvf ta-lib-0.4.0-src.tar.gz
%cd ta-lib
!./configure --prefix=/usr
!make
!make install

!pip install -U "vectorbtpro[base] @ git+https://${GH_USER}:${GH_TOKEN}@github.com/polakowo/vectorbt.pro.git"

# setup.py
setup(
    # ...
    install_requires=[
        "vectorbtpro @ git+https://github.com/polakowo/vectorbt.pro.git"
    ]
    # ...
)

git
git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro

pip install -e vectorbtpro

git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro --depth=1

git pull --unshallow

git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro --depth=1

cd vectorbtpro

docker build . -t vectorbtpro

mkdir work

docker run -it --rm -p 8888:8888 -v "$PWD/work":/home/jovyan/work vectorbtpro

-v
{PWD/work}
/home/jovyan/work
~/work
docker run -it --rm -p 10000:8888 -v "$PWD/work":/home/jovyan/work vectorbtpro

http://127.0.0.1:8888/lab?token=9e85949d9901633d1de9dad7a963b43257e29fb232883908
work
y
git pull

docker build . -t vectorbtpro

Code
pip install ".[base]"

.whl
pip install wheel
pip install "filename.whl[base]"

filename
(1)
(1)
pybind11
pip install pybind11

llvmlite
pip install --ignore-installed 'llvmlite'

show_svg()
pip install kaleido==0.1.0post1

"imageio"
benchmark
arg_take_spec
size
chunker_cls
arg_take_spec
minp
span
random_subset
use_parser=True
status
status_overview
stats
data_format="datatable"
settings.plotting.auto_rangebreaks
fig.show
fig.show_png
fig.show_svg
fit_ranges
latest_at_index
realign
resample_opening
realign_opening
resample_closing
realign_closing
HardStop
cash_earnings
dateparser
settings.importing.clear_pycache
technical
"14:30":"15:45"
setup.py
pyproject.toml
signal_func_nb
vbt.pf_nb.get_sl_target_price_nb(c)
post_signal_func_nb
post_order_func_nb
max_orders
max_order_records
max_logs
max_log_records
ignore_errors
length="optimize"
optimize_anchor_set
skip_minus_one
skip_not_found
pre_execute_func
pre_chunk_func
post_chunk_func
post_execute_func
vbt.NoResult
from_talib
from_expr
split_columns
skipna
split_columns
return_raw="outputs"
high
low
close
high
low
close
*args
place_args
keys
keys_are_features=True
features
Data.fetch
Data.pull
Data.fetch
Data.pull
column_config
feature_config
single_symbol
single_key
symbol_classes
classes
token
auth_token
column_map
feature_map
vbt.settings.plotting.pre_show_func
obj1.equals(obj2, debug=True)
cash_sharing
MNTargetValue
MNTargetPercent
MNTargetPercent100
grouped_index
from_signals
class_name:symbol
numba.generated_jit
peak_idx
start_idx
peak_val
start_val
drawdown_ranges
decline_ranges
group_by=True
xloc
fps
duration
td_stop
dt_stop
vbt.Param(["daily", "weekly", "monthly"])
fig.auto_rangebreaks()
pct_scale
pct_scale
bm_close
argmin
argmax
nanargmin
nanargmax
level_name
dateutil.parser
vbt.autoidx(...) & vbt.autoidx(...)
obj.vbt.iloc[...] = ...
rescale_to
warmup
path_or_buf
key
run_arg_dict
run_func_dict
EXCHANGE:SYMBOL
capture
capture_ratio
user_agent
stop_ladder
hide
map_template
MNTargetValue
MNTargetPercent
MNTargetPercent100
point_wise
wrapper.replace(grouper=wrapper.grouper.replace(group_by=True))
nested_=True
wrapper.replace(grouper=dict(group_by=True), nested_=True)
pivots
modes
delisted
splitter.take(vbt.RepEval("x.iloc[range_]", context=dict(x=x))
get_indices
vbt.Idxr(0)
vbt.Idxr(0, "BTC-USD")
RowIdx
rowidx
ColIdx
colidx
RowPoints
pointidx
RowRanges
rangeidx
ElemIdx
idx
ArrayWrapper.fill_using_index_dict
fill_using_index_dict
records
long_entries
long_exits
entries
exits
size
layout
date_parser
get_indicators
list_indicators
get_symbols
list_symbols
vbt.indicator
tz
pf.orders.bar_low
call_seq
attach_cal_seq=False
engine_kwargs=dict(show_progress=True)
run_pipeline
plot_
_returns
plot_
_returns
td_stop
dt_stop
limit_tif
limit_expiry
dt_stop
limit_expiry
simulate_from_orders_nb
from_orders_nb
simulate_from_signals_nb
from_signals_nb
simulate_from_signal_func_nb
from_signal_func_nb
simulate_nb
from_order_func_nb
simulate_row_wise_nb
from_order_func_rw_nb
flex_simulate_nb
from_flex_order_func_nb
flex_simulate_row_wise_nb
from_flex_order_func_rw_nb
order_func_nb
order_args
flexible
flex_order_func_nb
flex_order_args
order_mode=True
shape
fill_pos_info
exit_size
exit_size_type
ladder=True
step
step_idx
market
pd.Index
pd.Index([0.1, 0.2])
vbt.Param([0.1, 0.2])
staticized=True
adjustment
find_earliest_date=True
start
end
log_returns
daily_returns
daily_log_returns
use_gl
vbt.settings.plotting
sim_out
random_subset
zoneinfo
use_class_ids
AlphaVantageData
AVData
pf_method
init_position
init_price
SequenceEngine
SerialEngine
vbt.[object]
__all__
vbt.PF
Portfolio
vbt.PFO
PortfolioOptimizer
stop_price
init_price
check_stop_hit_nb
OLS
OLSS
OLS
dateparser
n
n
n
out
only_once
wait
delta_format="target"
tz
start
end
fill_state
save_state
fill_returns
save_returns
save_value
entry_place_func
entry_place_func_nb
exit_place_func
exit_place_func_nb
distribute="chunks"
funcs_args
n_calls
jitted_loop
jitted_warmup
pd.Timestamp
pd.to_datetime
phelp
pprint
dropna
blosc
.pickle.blosc
start
end
tz_convert
deep_substitute
substitute_templates
execute_kwargs=dict(cooldown=1)
Lazy
Eager
leverage=np.inf
order_result, new_account_state
new_account_state, order_result
Percent
Percent100
fill_state
fill_state
pct_change
flex_select_auto_nb
flex_2d
FlexArray1dLike
FlexArray2dLike
FlexArray1d
FlexArray2d
["longonly", "shortonly", "both"]
[["longonly", "shortonly", "both"]]
skip_not_found=True
to_py_timezone
to_fixed_offset
pip
print
help
limit=1
order_records
close
reduce_func_nb="sum"
asyncio
python-telegram-bot
options_
pickle
dill
.config
.cfg
.ini
configparser
"True"
"100.0"
[a.b]
[a]
&a.b
b
a
yf_data = !vbt.YFData.pull("BTC-USD")
yf_data
[plotting]
default_theme = dark

splits_arr
object
high
low
_all
dynamic_mode=True
np.nan
np.inf
max_error_interp_mode
columns_are_symbols
stretch
extend
param1 > param2
i
col
flex_2d
skip_until_exit
split
clear_cache
collect_garbage
tvdatafeed>=2.1.0
1.7.0
min_size
max_size
ZIGZAG
apply
custom
ConflictMode.Opposite
DataClass.set_settings(path_id="custom", ...)
DataClass.set_custom_settings(...)
random_subset
vbt.technical(...)
vbt.techcon(...)
prec_
min_size
max_size
size_type
min_size
size_type
SizeType.TargetPercent
rel_
product_idx
level
classes
list_symbols
zero_offset
force_missed_run
np.int_
np.int64
alpaca-trade-api-python
alpaca-py
_settings_key
_default
_def
signed_size
value
get_elem_nb
select_nb
wtype
StopExitMode
upon_stop_exit
StopExitType
stop_exit_type
i
idx
boundscheck=True
@njit
fill_returns
limit_tif
limit_expiry
stop_exit_price
vbt.clear_pycache()
adjust_func_nb
signal_func_nb
post_segment_func_nb
klines_type
ewm
wtype
MSTD
MSD
close
volume
recovery_ranges
plot_close
limit_reverse
from_ago
from_ago=1
price
nextopen
nextclose
from_ago
signal_func_nb
adjust_func_nb
sl_stop
tsl_stop
tsl_th
delta_format
price
limit_tif
limit_expiry
time_delta_format
upon_opp_limit_conflict="cancelexecute"
signal_idx
creation_idx
type
stop_type
trades_cls
ValuePercent
Percent
ValuePercent
TargetPercent
-np.inf
np.inf
price="open"
column
float32
pf.get_returns(log_returns=True)
log_returns=True
pf.get_returns_acc(log_returns=True)
settings
minp
polygon==1.0.0
reindex_kwargs
size
cash_deposits=vbt.index_dict({vbt.pointidx(every="MS"): 100})
size=vbt.RepEval("size = wrapper.fill(); size.iloc[0] = np.inf; size")
np.inf
init_cash
init_position
init_price
close
close
indexer_tolerance
indexer_method
plot_as_entry_markers
plot_as_exit_markers
7d
post_index_func
param_settings
entry_ts
follow_ts
stop_ts_out
entry_price
open
after_reset
reset_wait
prepare_func
n
parse_index
force_first
keep_conflicts
reverse_order
call_seq
auto
flex_2d
incl_doc
indexer_method
skipna
order_counts
log_counts
last_oidx
last_lidx
inplace
ndim
ExecuteOrderState
ProcessOrderState
ExecState
interval
timeframe
group_by
skipna
_pcg
_pcgs
_cs
returns_pcgs
returns
init_price
init_position
cash_deposits
get_klines
concat
chunk_func
alpha_vantage
execute_kwargs
apply_func
jit_select_params=True
nogil=True
fetch_kwargs[symbol]
ohlc
ohlcv
symbol
*
match_regex
parse_paths
match_paths
path
paths
None
raise_on_error
help
help
convert_dicts_
convert_children_
per_column
custom_func
apply_func
apply_func
takes_1d=True
@talib
@talib_1d
@talib
import
kwargs_to_args
kwargs_as_args
as_attrs_
fill_returns=True
@
from_custom_func
with_custom_func
from_apply_func
with_apply_func
minp
None
select_params
keep_pd=True
custom_output_props
lazy_outputs
select_one
select_one_from_obj
select_col
select_col_from_obj
np.double
mapping
context
"rolling_mean((low + high) / 2, 10)"
low
"rolling_mean(abs(in_ts), p_window)"
rolling_mean
vbt.wqa101
np.int_
np.integer
np.issubdtype
bm_close=False
direction='both'
size_type='value'
bm_close
ca_registry
ca_reg
glob.glob
myinout_2d_pc
myinout_pc_2d
portfolio:2
vbt
in_outputs
in_outputs
window
rolling_period
exit_idx
entry_idx
ArrayWrapper(**Config({
    "index": "<RangeIndex at 0x1045815e8> of shape (3,)",
    "columns": "<Int64Index at 0x1045815e8> of shape (1,)",
    "ndim": 1,
    "freq": null,
    "column_only_select": null,
    "group_select": null,
    "grouped_ndim": null,
    "group_by": null,
    "allow_enable": true,
    "allow_disable": true,
    "allow_modify": true
}))

ArrayWrapper(
    index=<RangeIndex at 0x1045815e8 of shape (3,)>,
    columns=<Int64Index at 0x1045815e8 of shape (1,)>,
    ndim=1,
    freq=None,
    column_only_select=None,
    group_select=None,
    grouped_ndim=None,
    grouper=Grouper(
        index=<Int64Index at 0x1045815e8 of shape (1,)>,
        group_by=None,
        allow_enable=True,
        allow_disable=True,
        allow_modify=True
    )
)

arg_take_spec
merge_func
jitter
apply_func_nb(window, *args)
apply_func_nb(from_i, to_i, col, *args)
pd.eval
BCO
pd.Index
download
fetch
pf.trades.winning.pnl.count()
trades
winning
open
high
low
-np.inf
np.inf
cash_deposits
cash_earnings
cash_dividends
init_position
stop_signal_priority
0
max_order_records=0
max_log_records=0
>>> import numpy as np
>>> import pandas as pd
>>> from numba import njit
>>> import vectorbtpro as vbt

>>> import vectorbtpro as vbt
>>> import numpy as np
>>> import pandas as pd

>>> data = vbt.BinanceData.pull('BTCUSDT')
>>> data
<vectorbtpro.data.custom.binance.BinanceData at 0x7f9c40c59550>

>>> data.plot().show()  To plot a specific symbol and/or feature, pass it as symbol and feature respectively

symbol
feature
>>> data.data['BTCUSDT'].info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 1616 entries, 2017-08-17 00:00:00+00:00 to 2022-01-18 00:00:00+00:00
Freq: D
Data columns (total 10 columns):
 #   Column              Non-Null Count  Dtype              
---  ------              --------------  -----              
 0   Open                1616 non-null   float64            
 1   High                1616 non-null   float64            
 2   Low                 1616 non-null   float64            
 3   Close               1616 non-null   float64            
 4   Volume              1616 non-null   float64            
 5   Close time          1616 non-null   datetime64[ns, UTC]
 6   Quote volume        1616 non-null   float64            
 7   Number of trades    1616 non-null   int64              
 8   Taker base volume   1616 non-null   float64            
 9   Taker quote volume  1616 non-null   float64            
dtypes: datetime64[ns, UTC

>>> open_price = data.get('Open')
>>> close_price = data.get('Close')  OHLCV can be also accessed directly, like data.close

data.close
>>> vbt.IF.list_indicators("RSI*")
['vbt:RSI', 'talib:RSI', 'pandas_ta:RSI', 'ta:RSIIndicator', 'technical:RSI']

>>> vbt.indicator("talib:RSI")
vectorbtpro.indicators.factory.talib.RSI

>>> vbt.RSI  To list all vectorbt indicators, check out the module custom
vectorbtpro.indicators.custom.RSI

>>> vbt.talib('RSI')  To list all supported TA-Lib indicators, call IndicatorFactory.list_talib_indicators
vectorbtpro.indicators.factory.talib.RSI

>>> vbt.ta('RSIIndicator')  To list all supported TA indicators, call IndicatorFactory.list_ta_indicators
vectorbtpro.indicators.factory.ta.RSIIndicator

>>> vbt.pandas_ta('RSI')  To list all supported Pandas TA indicators, call IndicatorFactory.list_pandas_ta_indicators
vectorbtpro.indicators.factory.pandas_ta.RSI

>>> vbt.technical('RSI')  To list all supported technical indicators, call IndicatorFactory.list_technical_indicators
vectorbtpro.indicators.factory.technical.RSI

run
>>> vbt.phelp(vbt.RSI.run)
RSI.run(
    close,
    window=Default(value=14),
    wtype=Default(value='wilder'),
    short_name='rsi',
    hide_params=None,
    hide_default=True,
    **kwargs
):
    Run `RSI` indicator.

    * Inputs: `close`
    * Parameters: `window`, `wtype`
    * Outputs: `rsi`

    Pass a list of parameter names as `hide_params` to hide their column levels.
    Set `hide_default` to False to show the column levels of the parameters with a default value.

    Other keyword arguments are passed to `RSI.run_pipeline`.

close
open_price
close
>>> rsi = vbt.RSI.run(open_price)
>>> rsi
<vectorbtpro.indicators.custom.RSI at 0x7f9c20921ac8>

rsi
phelp
>>> rsi.rsi
Open time
2017-08-17 00:00:00+00:00          NaN
2017-08-18 00:00:00+00:00          NaN
2017-08-19 00:00:00+00:00          NaN
2017-08-20 00:00:00+00:00          NaN
2017-08-21 00:00:00+00:00          NaN
...                                ...
2022-07-30 00:00:00+00:00    60.541637
2022-07-31 00:00:00+00:00    59.503179
2022-08-01 00:00:00+00:00    56.750576
2022-08-02 00:00:00+00:00    56.512434
2022-08-03 00:00:00+00:00    54.177385
Freq: D, Name: Open, Length: 1813, dtype: float64

>>> entries = rsi.rsi.vbt.crossed_below(30)  Using GenericAccessor.crossed_below
>>> entries
Open time
2017-08-17 00:00:00+00:00    False
2017-08-18 00:00:00+00:00    False
2017-08-19 00:00:00+00:00    False
2017-08-20 00:00:00+00:00    False
2017-08-21 00:00:00+00:00    False
...                            ...
2022-07-30 00:00:00+00:00    False
2022-07-31 00:00:00+00:00    False
2022-08-01 00:00:00+00:00    False
2022-08-02 00:00:00+00:00    False
2022-08-03 00:00:00+00:00    False
Freq: D, Name: Open, Length: 1813, dtype: bool

>>> exits = rsi.rsi.vbt.crossed_above(70)  Using GenericAccessor.crossed_above
>>> exits
Open time
2017-08-17 00:00:00+00:00    False
2017-08-18 00:00:00+00:00    False
2017-08-19 00:00:00+00:00    False
2017-08-20 00:00:00+00:00    False
2017-08-21 00:00:00+00:00    False
...                            ...
2022-07-30 00:00:00+00:00    False
2022-07-31 00:00:00+00:00    False
2022-08-01 00:00:00+00:00    False
2022-08-02 00:00:00+00:00    False
2022-08-03 00:00:00+00:00    False
Freq: D, Name: Open, Length: 1813, dtype: bool

rsi
>>> entries = rsi.rsi_crossed_below(30)
>>> exits = rsi.rsi_crossed_above(70)

dir(rsi)
>>> def plot_rsi(rsi, entries, exits):
...     fig = rsi.plot()  Using RSI.plot
...     entries.vbt.signals.plot_as_entries(rsi.rsi, fig=fig)  Using SignalsSRAccessor.plot_as_entries
...     exits.vbt.signals.plot_as_exits(rsi.rsi, fig=fig)  Using SignalsSRAccessor.plot_as_exits
...     return fig

>>> plot_rsi(rsi, entries, exits).show()

>>> clean_entries, clean_exits = entries.vbt.signals.clean(exits)  Using SignalsAccessor.clean

>>> plot_rsi(rsi, clean_entries, clean_exits).show()

clean_entries
clean_exits
>>> clean_entries.vbt.signals.total()  Get the total number of entry signals
8

>>> clean_exits.vbt.signals.total()  Get the total number of exit signals
7

>>> ranges = clean_entries.vbt.signals.between_ranges(other=clean_exits)  Get range records of type Ranges between each entry and exit
>>> ranges.duration.mean(wrap_kwargs=dict(to_timedelta=True))  Get the average duration between each entry and exit
Timedelta('86 days 10:17:08.571428572')

>>> pf = vbt.Portfolio.from_signals(
...     close=close_price, 
...     entries=clean_entries, 
...     exits=clean_exits,
...     size=100,
...     size_type='value',
...     init_cash='auto'
... )
>>> pf
<vectorbtpro.portfolio.base.Portfolio at 0x7f9c40eea438>

None
Portfolio
>>> pf.stats()
Start                         2017-08-17 00:00:00+00:00
End                           2022-08-03 00:00:00+00:00
Period                               1813 days 00:00:00
Start Value                                       100.0
Min Value                                     97.185676
Max Value                                    203.182943
End Value                                    171.335425
Total Return [%]                              71.335425
Benchmark Return [%]                         446.481746
Total Time Exposure [%]                       38.113624
Max Gross Exposure [%]                            100.0
Max Drawdown [%]                              46.385941
Max Drawdown Duration                1613 days 00:00:00
Total Orders                                         15
Total Fees Paid                                     0.0
Total Trades                                          8
Win Rate [%]                                  71.428571
Best Trade [%]                                54.519055
Worst Trade [%]                              -32.078597
Avg Winning Trade [%]                         26.905709
Avg Losing Trade [%]                         -19.345383
Avg Winning Trade Duration             87 days 09:36:00
Avg Losing Trade Duration              84 days 00:00:00
Profit Factor                                  3.477019
Expectancy                                    13.691111
Sharpe Ratio                                   0.505486
Calmar Ratio                                   0.246836
Omega Ratio                                    1.132505
Sortino Ratio                                  0.796701
dtype: object

pf.metrics
calc_func
>>> pf.plot(settings=dict(bm_returns=False)).show()

lower_th
upper_th
window
ewm
>>> def test_rsi(window=14, wtype="wilder", lower_th=30, upper_th=70):
...     rsi = vbt.RSI.run(open_price, window=window, wtype=wtype)
...     entries = rsi.rsi_crossed_below(lower_th)
...     exits = rsi.rsi_crossed_above(upper_th)
...     pf = vbt.Portfolio.from_signals(
...         close=close_price, 
...         entries=entries, 
...         exits=exits,
...         size=100,
...         size_type='value',
...         init_cash='auto')
...     return pf.stats([
...         'total_return', 
...         'total_trades', 
...         'win_rate', 
...         'expectancy'
...     ])

>>> test_rsi()
Total Return [%]    71.335425
Total Trades                8
Win Rate [%]        71.428571
Expectancy          13.691111
dtype: object

>>> test_rsi(lower_th=20, upper_th=80)
Total Return [%]    6.652287
Total Trades               2
Win Rate [%]            50.0
Expectancy          3.737274
dtype: object

>>> from itertools import product

>>> lower_ths = range(20, 31)  20, 21, ..., 30
>>> upper_ths = range(70, 81)  70, 71, ..., 80
>>> th_combs = list(product(lower_ths, upper_ths))  Generate all possible combinations between lower_ths and upper_ths, also called as a Cartesian product
>>> len(th_combs)
121

>>> comb_stats = [
...     test_rsi(lower_th=lower_th, upper_th=upper_th)
...     for lower_th, upper_th in th_combs
... ]  Iterate over each combination and compute its statistics using a list comprehension. This creates a list of Series.

lower_ths
upper_ths
>>> comb_stats_df = pd.DataFrame(comb_stats)
>>> comb_stats_df
     Total Return [%]  Total Trades  Win Rate [%]  Expectancy
0           24.369550             3     66.666667   10.606342
1           37.380341             3     66.666667   16.203667
2           34.560194             3     66.666667   14.981187
3           31.090080             3     66.666667   13.833710
4           31.090080             3     66.666667   13.833710
..                ...           ...           ...         ...
116         51.074571             6     80.000000   18.978193
117         62.853840             6     80.000000   21.334047
118         40.685579             5     75.000000   21.125494
119         -5.990835             4     66.666667   13.119897
120        -10.315159             4     66.666667   11.678455

[121 rows x 4 columns]

lower_th
upper_th
comb_stats_df
>>> comb_stats_df.index = pd.MultiIndex.from_tuples(
...     th_combs, 
...     names=['lower_th', 'upper_th'])
>>> comb_stats_df
                   Total Return [%]  Total Trades  Win Rate [%]  Expectancy
lower_th upper_th                                                          
20       70               24.369550             3     66.666667   10.606342
         71               37.380341             3     66.666667   16.203667
         72               34.560194             3     66.666667   14.981187
         73               31.090080             3     66.666667   13.833710
         74               31.090080             3     66.666667   13.833710
...                             ...           ...           ...         ...
30       76               51.074571             6     80.000000   18.978193
         77               62.853840             6     80.000000   21.334047
         78               40.685579             5     75.000000   21.125494
         79               -5.990835             4     66.666667   13.119897
         80              -10.315159             4     66.666667   11.678455

[121 rows x 4 columns]

>>> comb_stats_df['Expectancy'].vbt.heatmap().show()

>>> windows = list(range(8, 21))
>>> wtypes = ["simple", "exp", "wilder"]
>>> lower_ths = list(range(20, 31))
>>> upper_ths = list(range(70, 81))

itertools.product
param_product=True
windows
wtypes
open_price
>>> rsi = vbt.RSI.run(
...     open_price, 
...     window=windows, 
...     wtype=wtypes, 
...     param_product=True)
>>> rsi.rsi.columns
MultiIndex([( 8, 'simple'),
            ( 8,    'exp'),
            ( 8, 'wilder'),
            ...
            (20, 'simple'),
            (20,    'exp'),
            (20, 'wilder')],
           names=['rsi_window', 'rsi_wtype'])

rsi_window
rsi_wtype
len(open_price.columns)
len(windows)
len(wtypes)
rsi
lower_ths
upper_th_index
rsi
rsi_crossed_below
rsi_crossed_above
rsi
>>> lower_ths_prod, upper_ths_prod = zip(*product(lower_ths, upper_ths))
>>> len(lower_ths_prod)  The first value in lower_ths_prod builds a combination with the first value in upper_ths_prod, the second with the second, and so on - 121 combinations in total.
121
>>> len(upper_ths_prod)
121

>>> lower_th_index = vbt.Param(lower_ths_prod, name='lower_th')  Convert thresholds to vbt.Param to instruct broadcast that we want to build a product with the columns in rsi
>>> entries = rsi.rsi_crossed_below(lower_th_index)
>>> entries.columns
MultiIndex([(20,  8, 'simple'),
            (20,  8,    'exp'),
            (20,  8, 'wilder'),
            ...
            (30, 20, 'simple'),
            (30, 20,    'exp'),
            (30, 20, 'wilder')],
           names=['lower_th', 'rsi_window', 'rsi_wtype'], length=4719)

>>> upper_th_index = vbt.Param(upper_ths_prod, name='upper_th')
>>> exits = rsi.rsi_crossed_above(upper_th_index)
>>> exits.columns
MultiIndex([(70,  8, 'simple'),
            (70,  8,    'exp'),
            (70,  8, 'wilder'),
            ...
            (80, 20, 'simple'),
            (80, 20,    'exp'),
            (80, 20, 'wilder')],
           names=['upper_th', 'rsi_window', 'rsi_wtype'], length=4719)

lower_ths_prod
upper_ths_prod
vbt.Param
rsi
entries
exits
lower_th
upper_th
close_price
>>> pf = vbt.Portfolio.from_signals(
...     close=close_price, 
...     entries=entries, 
...     exits=exits,
...     size=100,
...     size_type='value',
...     init_cash='auto'
... )
>>> pf
<vectorbtpro.portfolio.base.Portfolio at 0x7f9c415ed5c0>

>>> stats_df = pf.stats([
...     'total_return', 
...     'total_trades', 
...     'win_rate', 
...     'expectancy'
... ], agg_func=None)  By default, StatsBuilderMixin.stats takes the mean out of all columns and returns a Series. We, on the other hand, want to disable the aggregation function and stack all Series into one big DataFrame.
>>> stats_df
                                        Total Return [%]  Total Trades  \\
lower_th upper_th rsi_window rsi_wtype                                   
20       70       8          simple           -25.285842            31   
                             exp               -7.939736            29   
                             wilder            61.979801            11   
...                                                  ...           ...   
                  20         simple           -59.159157             4   
                             exp               -3.331163             8   
                             wilder            31.479482             3   

                                        Win Rate [%]  Expectancy  
lower_th upper_th rsi_window rsi_wtype                            
20       70       8          simple        51.612903   -1.224523  
                             exp           58.620690   -0.307862  
                             wilder        72.727273    5.634527  
...                                              ...         ...  
                  20         simple        33.333333  -16.159733  
                             exp           57.142857    7.032204  
                             wilder        50.000000   38.861607  

[4719 rows x 4 columns]

>>> print(pf.getsize())
9.4 MB

>>> np.product(pf.wrapper.shape) * 8 / 1024 / 1024
65.27364349365234

rsi_window
>>> stats_df['Expectancy'].groupby('rsi_window').mean()
rsi_window
8      0.154425
9      0.064130
10    -0.915478
11    -0.523294
12     0.742266
13     3.898482
14     4.414367
15     6.916872
16     8.915225
17    12.204188
18    12.897135
19    14.508950
20    16.429515
Name: Expectancy, dtype: float64

>>> stats_df.sort_values(by='Expectancy', ascending=False).head()
                                        Total Return [%]  Total Trades  \\
lower_th upper_th rsi_window rsi_wtype                                   
22       80       20         wilder           187.478208             2   
21       80       20         wilder           187.478208             2   
26       80       20         wilder           152.087039             3   
23       80       20         wilder           187.478208             2   
25       80       20         wilder           201.297495             3   

                                        Win Rate [%]  Expectancy  
lower_th upper_th rsi_window rsi_wtype                            
22       80       20         wilder            100.0   93.739104  
21       80       20         wilder            100.0   93.739104  
26       80       20         wilder            100.0   93.739104  
23       80       20         wilder            100.0   93.739104  
25       80       20         wilder            100.0   93.739104  

>>> pf[(22, 80, 20, "wilder")].plot_value().show()

column
pf.plot_value(column=(22, 80, 20, "wilder"))
open_price
close_price
>>> data = vbt.BinanceData.pull(['BTCUSDT', 'ETHUSDT'])

MultiIndex([(20, 70,  8, 'simple', 'BTCUSDT'),
            (20, 70,  8, 'simple', 'ETHUSDT'),
            (20, 70,  8,    'exp', 'BTCUSDT'),
            ...
            (30, 80, 20,    'exp', 'ETHUSDT'),
            (30, 80, 20, 'wilder', 'BTCUSDT'),
            (30, 80, 20, 'wilder', 'ETHUSDT')],
           names=['lower_th', 'upper_th', 'rsi_window', 'rsi_wtype', 'symbol'], length=9438)

symbol
>>> eth_mask = stats_df.index.get_level_values('symbol') == 'ETHUSDT'
>>> btc_mask = stats_df.index.get_level_values('symbol') == 'BTCUSDT'
>>> pd.DataFrame({
...     'ETHUSDT': stats_df[eth_mask]['Expectancy'].values,
...     'BTCUSDT': stats_df[btc_mask]['Expectancy'].values
... }).vbt.histplot(xaxis=dict(title="Expectancy")).show()  Using GenericAccessor.histplot

>>> import numpy as np

>>> def rolling_window(a, window):
...     shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)
...     strides = a.strides + (a.strides[-1],)
...     return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)

>>> np.mean(rolling_window(np.arange(10), 3), axis=1)
array([1., 2., 3., 4., 5., 6., 7., 8.])

>>> import pandas as pd

>>> index = pd.date_range('2020-01-01', '2020-01-10')
>>> sr = pd.Series(range(len(index)), index=index)
>>> sr.rolling(3).mean()
2020-01-01    NaN
2020-01-02    NaN
2020-01-03    1.0
2020-01-04    2.0
2020-01-05    3.0
2020-01-06    4.0
2020-01-07    5.0
2020-01-08    6.0
2020-01-09    7.0
2020-01-10    8.0
Freq: D, dtype: float64

>>> from numba import njit

>>> @njit
... def moving_average_nb(a, window_len):
...     b = np.empty_like(a, dtype=np.float_)
...     for i in range(len(a)):
...         window_start = max(0, i + 1 - window_len)
...         window_end = i + 1
...         if window_end - window_start < window_len:
...             b[i] = np.nan
...         else:
...             b[i] = np.mean(a[window_start:window_end])
...     return b

>>> moving_average_nb(np.arange(10), 3)
array([nan, nan, 1., 2., 3., 4., 5., 6., 7., 8.])

>>> big_a = np.arange(1000000)
>>> %timeit moving_average_nb.py_func(big_a, 10)  Python
6.54 s ± 142 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %timeit np.mean(rolling_window(big_a, 10), axis=1)  NumPy
24.7 ms ± 173 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> %timeit pd.Series(big_a).rolling(10).mean()  Pandas
10.2 ms ± 309 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

>>> %timeit moving_average_nb(big_a, 10)  Numba!
5.12 ms ± 7.21 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

nb
>>> arr = sr.values
>>> result = moving_average_nb(arr, 3)
>>> new_sr = pd.Series(result, index=sr.index, name=sr.name)
>>> new_sr
2020-01-01    NaN
2020-01-02    NaN
2020-01-03    1.0
2020-01-04    2.0
2020-01-05    3.0
2020-01-06    4.0
2020-01-07    5.0
2020-01-08    6.0
2020-01-09    7.0
2020-01-10    8.0
Freq: D, dtype: float64

>>> import vectorbtpro as vbt

>>> sr.vbt.rolling_mean(3)
2020-01-01    NaN
2020-01-02    NaN
2020-01-03    1.0
2020-01-04    2.0
2020-01-05    3.0
2020-01-06    4.0
2020-01-07    5.0
2020-01-08    6.0
2020-01-09    7.0
2020-01-10    8.0
Freq: D, dtype: float64

vbt
>>> df = pd.DataFrame({'a': range(10), 'b': range(9, -1, -1)})
>>> df.vbt.rolling_mean(3)
     a    b
0  NaN  NaN
1  NaN  NaN
2  1.0  8.0
3  2.0  7.0
4  3.0  6.0
5  4.0  5.0
6  5.0  4.0
7  6.0  3.0
8  7.0  2.0
9  8.0  1.0

rolling_mean
vbt
GenericAccessor
vbt.returns
>>> ret = pd.Series([0.1, 0.2, -0.1])
>>> ret.vbt.returns.total()
0.18800000000000017

>>> p1 = pd.DataFrame({
...     'open': [1, 2, 3, 4, 5],
...     'high': [2.5, 3.5, 4.5, 5.5, 6.5],
...     'low': [0.5, 1.5, 2.5, 3.5, 4.5],
...     'close': [2, 3, 4, 5, 6]
... }, index=pd.date_range('2020-01-01', '2020-01-05'))
>>> p1
            open  high  low  close
2020-01-01     1   2.5  0.5      2
2020-01-02     2   3.5  1.5      3
2020-01-03     3   4.5  2.5      4
2020-01-04     4   5.5  3.5      5
2020-01-05     5   6.5  4.5      6

>>> single_pf = vbt.Portfolio.from_holding(
...     open=p1['open'], 
...     high=p1['high'], 
...     low=p1['low'], 
...     close=p1['close']
... )
>>> single_pf.value
2020-01-01    100.0
2020-01-02    150.0
2020-01-03    200.0
2020-01-04    250.0
2020-01-05    300.0
Freq: D, dtype: float64

>>> p2 = pd.DataFrame({
...     'open': [6, 5, 4, 3, 2],
...     'high': [6.5, 5.5, 4.5, 3.5, 2.5],
...     'low': [4.5, 3.5, 2.5, 1.5, 0.5],
...     'close': [5, 4, 3, 2, 1]
... }, index=pd.date_range('2020-01-01', '2020-01-05'))
>>> p2
            open  high  low  close
2020-01-01     6   6.5  4.5      5
2020-01-02     5   5.5  3.5      4
2020-01-03     4   4.5  2.5      3
2020-01-04     3   3.5  1.5      2
2020-01-05     2   2.5  0.5      1

>>> multi_open = pd.DataFrame({
...     'p1': p1['open'],
...     'p2': p2['open']
... })
>>> multi_high = pd.DataFrame({
...     'p1': p1['high'],
...     'p2': p2['high']
... })
>>> multi_low = pd.DataFrame({
...     'p1': p1['low'],
...     'p2': p2['low']
... })
>>> multi_close = pd.DataFrame({
...     'p1': p1['close'],
...     'p2': p2['close']
... })

>>> multi_pf = vbt.Portfolio.from_holding(
...     open=multi_open,
...     high=multi_high,
...     low=multi_low,
...     close=multi_close
... )
>>> multi_pf.value
               p1     p2
2020-01-01  100.0  100.0
2020-01-02  150.0   80.0
2020-01-03  200.0   60.0
2020-01-04  250.0   40.0
2020-01-05  300.0   20.0

>>> candle_green = multi_close > multi_open
>>> prev_candle_green = candle_green.vbt.signals.fshift(1)
>>> prev_candle_green
               p1     p2
2020-01-01  False  False
2020-01-02   True  False
2020-01-03   True  False
2020-01-04   True  False
2020-01-05   True  False

>>> candle_red = multi_close < multi_open
>>> prev_candle_red = candle_red.vbt.signals.fshift(1)
>>> prev_candle_red
               p1     p2
2020-01-01  False  False
2020-01-02  False   True
2020-01-03  False   True
2020-01-04  False   True
2020-01-05  False   True

multi_close
multi_open
p1
p2
>>> macd = vbt.MACD.run(
...     multi_close,
...     fast_window=2,
...     slow_window=(3, 4),
...     signal_window=2,
...     macd_wtype="simple",
...     signal_wtype="weighted"
... )
>>> macd.signal
macd_fast_window               2             2  << fast window for MACD line
macd_slow_window               3             4  << slow window for MACD line
macd_signal_window             2             2  << window for signal line
macd_macd_wtype           simple        simple  << window type for MACD line
macd_signal_wtype       weighted      weighted  << window type for signal line   
                         p1   p2       p1   p2  << price
2020-01-01              NaN  NaN      NaN  NaN
2020-01-02              NaN  NaN      NaN  NaN
2020-01-03              NaN  NaN      NaN  NaN
2020-01-04              0.5 -0.5      NaN  NaN
2020-01-05              0.5 -0.5      1.0 -1.0

macd_fast_window
>>> part_arrays = dict(
...     close=pd.DataFrame({
...         'a': [1, 2, 3, 4], 
...         'b': [4, 3, 2, 1]
...     }),  Per element
...     size=pd.Series([1, -1, 1, -1]),  Per row
...     direction=[['longonly', 'shortonly']],  Per column
...     fees=0.01  Per entire array
... )
>>> full_arrays = vbt.broadcast(part_arrays)

>>> full_arrays['close']
   a  b
0  1  4
1  2  3
2  3  2
3  4  1

>>> full_arrays['size']
   a  b
0  1  1
1 -1 -1
2  1  1
3 -1 -1

>>> full_arrays['direction']
          a          b
0  longonly  shortonly
1  longonly  shortonly
2  longonly  shortonly
3  longonly  shortonly

>>> full_arrays['fees']
      a     b
0  0.01  0.01
1  0.01  0.01
2  0.01  0.01
3  0.01  0.01

>>> fast_ma = vbt.MA.run(multi_close, window=[2, 3], short_name='fast')
>>> slow_ma = vbt.MA.run(multi_close, window=[3, 4], short_name='slow')

>>> fast_ma.ma
fast_window    2    2    3    3
              p1   p2   p1   p2
2020-01-01   NaN  NaN  NaN  NaN
2020-01-02   2.5  4.5  NaN  NaN
2020-01-03   3.5  3.5  3.0  4.0
2020-01-04   4.5  2.5  4.0  3.0
2020-01-05   5.5  1.5  5.0  2.0

>>> slow_ma.ma
slow_window    3    3    4    4
              p1   p2   p1   p2
2020-01-01   NaN  NaN  NaN  NaN
2020-01-02   NaN  NaN  NaN  NaN
2020-01-03   3.0  4.0  NaN  NaN
2020-01-04   4.0  3.0  3.5  3.5
2020-01-05   5.0  2.0  4.5  2.5

>>> fast_ma.ma > slow_ma.ma  Comparison with Pandas fails
ValueError: Can only compare identically-labeled DataFrame objects

>>> fast_ma.ma.values > slow_ma.ma.values  Comparison with NumPy succeeds
array([[False, False, False, False],
       [False, False, False, False],
       [ True, False, False, False],
       [ True, False,  True, False],
       [ True, False,  True, False]])

>>> fast_ma.ma.vbt > slow_ma.ma  Comparison with vectorbt succeeds
fast_window      2      2      3      3
slow_window      3      3      4      4
                p1     p2     p1     p2
2020-01-01   False  False  False  False
2020-01-02   False  False  False  False
2020-01-03    True  False  False  False
2020-01-04    True  False   True  False
2020-01-05    True  False   True  False

.vbt
>>> df1 = pd.DataFrame({'a': [0], 'b': [1]})
>>> df2 = pd.DataFrame({'b': [0], 'a': [1]})
>>> df1 + df2  Pandas connects column a in df1 and df2
   a  b
0  1  1

>>> df1.values + df2.values
array([[0, 2]])

>>> df1.vbt + df2  vectorbt connects the first column in df1 with the first column in df2, regardless of their labels
   a  b
   b  a
0  0  2

a
df1
df2
df1
df2
>>> fast_ma.ma > multi_close  Comparison with Pandas fails
ValueError: Can only compare identically-labeled DataFrame objects

>>> fast_ma.ma.values > multi_close.values  Comparison with NumPy fails
ValueError: operands could not be broadcast together with shapes (5,4) (5,2) 

>>> fast_ma.ma.vbt > multi_close  Comparison with vectorbt succeeds
fast_window      2      2      3      3
                p1     p2     p1     p2
2020-01-01   False  False  False  False
2020-01-02   False   True  False  False
2020-01-03   False   True  False   True
2020-01-04   False   True  False   True
2020-01-05   False   True  False   True

>>> above_lower = multi_close.vbt > vbt.Param([1, 2], name='lower')
>>> below_upper = multi_close.vbt < vbt.Param([3, 4], name='upper')
>>> above_lower.vbt & below_upper
lower           1      1      2      2
upper           3      3      4      4
               p1     p2     p1     p2
2020-01-01   True  False  False  False
2020-01-02  False  False   True  False
2020-01-03  False  False  False   True
2020-01-04  False   True  False  False
2020-01-05  False  False  False  False

>>> a = np.array([1])

>>> vbt.flex_select_1d_nb(a, 0)
1

>>> vbt.flex_select_1d_nb(a, 1)
1

>>> vbt.flex_select_1d_nb(a, 2)
1

>>> full_a = np.broadcast_to(a, (1000,))

>>> full_a[2]
1

>>> a = np.array([[0]])  Same for each element
>>> b = np.array([[1, 2, 3]])  Same for each row
>>> c = np.array([[4], [5], [6]])  Same for each column

>>> vbt.flex_select_nb(a, 2, 1)  Query the element under the third row and second column
0

>>> vbt.flex_select_nb(b, 2, 1)
2

>>> vbt.flex_select_nb(c, 2, 1)
6

BaseDataMixin()

BaseDataMixin.assert_has_feature(
    feature
)

BaseDataMixin.assert_has_symbol(
    symbol
)

BaseDataMixin.get(
    features=None,
    symbols=None,
    feature=None,
    symbol=None,
    **kwargs
)

BaseDataMixin.get_feature(
    feature,
    raise_error=False
)

BaseDataMixin.get_feature_idx(
    feature,
    raise_error=False
)

BaseDataMixin.get_symbol(
    symbol,
    raise_error=False
)

BaseDataMixin.get_symbol_idx(
    symbol,
    raise_error=False
)

BaseDataMixin.has_feature(
    feature
)

BaseDataMixin.has_multiple_keys(
    keys
)

BaseDataMixin.has_symbol(
    symbol
)

BaseDataMixin.select_feature_idxs(
    idxs,
    **kwargs
)

BaseDataMixin.select_features(
    features,
    **kwargs
)

BaseDataMixin.select_symbol_idxs(
    idxs,
    **kwargs
)

BaseDataMixin.select_symbols(
    symbols,
    **kwargs
)

Data(
    wrapper,
    data,
    single_key=True,
    classes=None,
    level_name=None,
    fetch_kwargs=None,
    returned_kwargs=None,
    last_index=None,
    delisted=None,
    tz_localize=None,
    tz_convert=None,
    missing_index=None,
    missing_columns=None,
    **kwargs
)

Data.align_columns(
    data,
    missing=None,
    silence_warnings=None
)

missing
Data.align_data(
    data,
    last_index=None,
    delisted=None,
    tz_localize=None,
    tz_convert=None,
    missing_index=None,
    missing_columns=None,
    silence_warnings=None
)

Data.align_index(
    data,
    missing=None,
    silence_warnings=None
)

missing
Data.build_feature_config_doc(
    source_cls=None
)

Data.check_dict_type(
    arg,
    arg_name=None,
    dict_type=None
)

Data.column_stack(
    *objs,
    wrapper_kwargs=None,
    **kwargs
)

Data.concat(
    keys=None,
    attach_classes=True,
    index_stack_kwargs=None,
    **kwargs
)

HybridConfig()

Data._feature_config
Data._feature_config
Data.fetch(
    *args,
    **kwargs
)

Data.fetch_feature(
    feature,
    **kwargs
)

tz_localize
tz_convert
freq
Data.fetch_symbol(
    symbol,
    **kwargs
)

tz_localize
tz_convert
freq
Data.fix_data_dict_type(
    data
)

Data.fix_dict_types_in_kwargs(
    data_type,
    **kwargs
)

Data.from_csv(
    *args,
    fetch_kwargs=None,
    **kwargs
)

fetch_kwargs
Data.from_data(
    data,
    columns_are_symbols=False,
    invert_data=False,
    single_key=True,
    classes=None,
    level_name=None,
    tz_localize=None,
    tz_convert=None,
    missing_index=None,
    missing_columns=None,
    wrapper_kwargs=None,
    fetch_kwargs=None,
    returned_kwargs=None,
    last_index=None,
    delisted=None,
    silence_warnings=None,
    **kwargs
)

data
dict
columns_are_symbols
bool
invert_data
bool
single_key
bool
classes
level_name
bool
hashable
iterable
hashable
tz_localize
timezone_like
tz_convert
timezone_like
missing_index
str
missing_columns
str
wrapper_kwargs
dict
fetch_kwargs
returned_kwargs
last_index
delisted
silence_warnings
bool
**kwargs
__init__
Data.from_data_str(
    data_str
)

YFData:BTC-USD
BTC-USD
Data.from_duckdb(
    *args,
    fetch_kwargs=None,
    **kwargs
)

fetch_kwargs
Data.from_feather(
    *args,
    fetch_kwargs=None,
    **kwargs
)

fetch_kwargs
Data.from_hdf(
    *args,
    fetch_kwargs=None,
    **kwargs
)

fetch_kwargs
Data.from_parquet(
    *args,
    fetch_kwargs=None,
    **kwargs
)

fetch_kwargs
Data.from_sql(
    *args,
    fetch_kwargs=None,
    **kwargs
)

fetch_kwargs
Data.get_base_setting(
    *args,
    **kwargs
)

CustomData.get_setting
path_id="base"
Data.get_base_settings(
    *args,
    **kwargs
)

CustomData.get_settings
path_id="base"
Data.get_feature_wrapper(
    features=None,
    **kwargs
)

Data.get_key_wrapper(
    keys=None,
    attach_classes=True,
    index_stack_kwargs=None,
    **kwargs
)

attach_classes
Data.get_keys(
    dict_type
)

Data.get_symbol_wrapper(
    symbols=None,
    **kwargs
)

Data.has_base_setting(
    *args,
    **kwargs
)

CustomData.has_setting
path_id="base"
Data.has_base_settings(
    *args,
    **kwargs
)

CustomData.has_settings
path_id="base"
Data.has_key_dict(
    arg,
    dict_type=None
)

Data.indexing_func(
    *args,
    **kwargs
)

Data.invert(
    key_wrapper_kwargs=None,
    **kwargs
)

Data.invert_data(
    dct
)

Data.merge(
    *datas,
    rename=None,
    **kwargs
)

datas
HybridConfig(
    start=dict(
        title='Start',
        calc_func=<function Data.<lambda> at 0x7fa2099a1cf0>,
        agg_func=None,
        tags='wrapper'
    ),
    end=dict(
        title='End',
        calc_func=<function Data.<lambda> at 0x7fa2099a1d80>,
        agg_func=None,
        tags='wrapper'
    ),
    period=dict(
        title='Period',
        calc_func=<function Data.<lambda> at 0x7fa2099a1e10>,
        apply_to_timedelta=True,
        agg_func=None,
        tags='wrapper'
    ),
    total_features=dict(
        title='Total Features',
        check_is_feature_oriented=True,
        calc_func=<function Data.<lambda> at 0x7fa2099a1ea0>,
        agg_func=None,
        tags='data'
    ),
    total_symbols=dict(
        title='Total Symbols',
        check_is_symbol_oriented=True,
        calc_func=<function Data.<lambda> at 0x7fa2099a1f30>,
        tags='data'
    ),
    null_counts=dict(
        title='Null Counts',
        calc_func=<function Data.<lambda> at 0x7fa2099a1fc0>,
        agg_func=<function Data.<lambda> at 0x7fa2099a2050>,
        tags='data'
    )
)

Data._metrics
Data._metrics
missing
missing
Data.override_feature_config_doc(
    __pdoc__,
    source_cls=None
)

Data.plot(
    column=None,
    feature=None,
    symbol=None,
    feature_map=None,
    plot_volume=None,
    base=None,
    **kwargs
)

column
hashable
feature
hashable
symbol
hashable
feature_map
sequence
str
plot_volume
bool
base
float
kwargs
dict
>>> import vectorbtpro as vbt

>>> start = '2021-01-01 UTC'  # crypto is in UTC
>>> end = '2021-06-01 UTC'
>>> data = vbt.YFData.pull(['BTC-USD', 'ETH-USD', 'ADA-USD'], start=start, end=end)

>>> data.plot(feature='Close', base=1).show()

>>> data.plot(symbol='BTC-USD').show()

plots
Data.prepare_dt(
    obj,
    parse_dates=True,
    to_utc=True,
    remove_utc_tz=False
)

parse_dates
parse_dates
parse_dates
to_utc
to_utc
to_utc
to_utc
to_utc
to_utc
Data.prepare_dt_column(
    sr,
    parse_dates=False,
    tz_localize=None,
    tz_convert=None,
    force_tz_convert=False,
    remove_tz=False
)

Data.prepare_dt_index(
    index,
    parse_dates=False,
    tz_localize=None,
    tz_convert=None,
    force_tz_convert=False,
    remove_tz=False
)

parse_dates
tz_localize
tz_convert
force_tz_convert
Data.prepare_tzaware_index(
    obj,
    tz_localize=None,
    tz_convert=None
)

parse_dates=True
force_tz_convert=True
Data.pull(
    keys=None,
    *,
    keys_are_features=None,
    features=None,
    symbols=None,
    classes=None,
    level_name=None,
    tz_localize=None,
    tz_convert=None,
    missing_index=None,
    missing_columns=None,
    wrapper_kwargs=None,
    skip_on_error=None,
    silence_warnings=None,
    execute_kwargs=None,
    return_raw=False,
    **kwargs
)

keys
hashable
sequence
hashable
or dict
keys_are_features
features
symbols
keys_are_features
bool
keys
features
hashable
sequence
hashable
or dict
symbols
hashable
sequence
hashable
or dict
classes
level_name
bool
hashable
iterable
hashable
tz_localize
any
tz_convert
any
missing_index
str
missing_columns
str
wrapper_kwargs
dict
skip_on_error
bool
silence_warnings
bool
execute_kwargs
dict
return_raw
bool
**kwargs
Data.realign(
    rule=None,
    *args,
    wrapper_meta=None,
    ffill=True,
    **kwargs
)

realign_func
Data.rename(
    rename,
    **kwargs
)

rename
Data.rename_in_dict(
    dct,
    rename
)

Data.replace(
    **kwargs
)

Data.resample(
    *args,
    wrapper_meta=None,
    **kwargs
)

resample_func
Data.resolve_base_setting(
    *args,
    **kwargs
)

CustomData.resolve_setting
path_id="base"
Data.resolve_key_arg(
    arg,
    k,
    arg_name,
    check_dict_type=True,
    template_context=None,
    is_kwargs=False
)

Data.resolve_keys_meta(
    keys=None,
    keys_are_features=None,
    features=None,
    symbols=None
)

Data.row_stack(
    *objs,
    wrapper_kwargs=None,
    **kwargs
)

Data.run(
    func,
    *args,
    on_features=None,
    on_symbols=None,
    pass_as_first=False,
    ignore_args=None,
    rename_args=None,
    location=None,
    prepend_location=None,
    unpack=False,
    concat=True,
    silence_warnings=False,
    raise_errors=False,
    execute_kwargs=None,
    merge_func=None,
    merge_kwargs=None,
    template_context=None,
    return_keys=False,
    **kwargs
)

data
open
func
rename_args
close
unpack
*args
**kwargs
func
Data.select(
    keys,
    **kwargs
)

Data.select_classes(
    key,
    **kwargs
)

Data.select_delisted(
    key,
    **kwargs
)

Data.select_feature_from_dict(
    feature,
    dct,
    **kwargs
)

Data.select_feature_kwargs(
    feature,
    kwargs,
    **kwargs_
)

Data.select_fetch_kwargs(
    key,
    **kwargs
)

Data.select_from_dict(
    dct,
    keys,
    raise_error=False
)

Data.select_key_from_dict(
    key,
    dct,
    dct_name='dct',
    dict_type=None,
    check_dict_type=True
)

Data.select_key_kwargs(
    key,
    kwargs,
    kwargs_name='kwargs',
    dict_type=None,
    check_dict_type=True
)

Data.select_last_index(
    key,
    **kwargs
)

Data.select_returned_kwargs(
    key,
    **kwargs
)

Data.select_symbol_from_dict(
    symbol,
    dct,
    **kwargs
)

Data.select_symbol_kwargs(
    symbol,
    kwargs,
    **kwargs_
)

Data.set_base_settings(
    *args,
    **kwargs
)

CustomData.set_settings
path_id="base"
Data.sql(
    query,
    dbcon=None,
    database=':memory:',
    db_config=None,
    alias='',
    params=None,
    other_objs=None,
    date_as_object=False,
    align_dtypes=True,
    squeeze=True,
    **kwargs
)

**kwargs
as_dict=True
duckdb.sql
squeeze
stats
HybridConfig(
    plot=RepEval(
        template='\n                if symbols is None:\n                    symbols = self.symbols\n                if not self.has_multiple_keys(symbols):\n                    symbols = [symbols]\n                [\n                    dict(\n                        check_is_not_grouped=True,\n                        plot_func="plot",\n                        plot_volume=False,\n                        symbol=s,\n                        title=s,\n                        pass_add_trace_kwargs=True,\n                        xaxis_kwargs=dict(rangeslider_visible=False, showgrid=True),\n                        yaxis_kwargs=dict(showgrid=True),\n                        tags="data",\n                    )\n                    for s in symbols\n                ]',
        context=dict(
            symbols=None
        ),
        strict=None,
        sub_id=None,
        context_merge_kwargs=None
    )
)

Data._subplots
Data._subplots
Data.switch_class(
    new_cls,
    clear_fetch_kwargs=False,
    clear_returned_kwargs=False,
    **kwargs
)

Data.to_csv(
    path_or_buf='.',
    ext='csv',
    mkdir_kwargs=None,
    check_dict_type=True,
    template_context=None,
    return_meta=False,
    **kwargs
)

path_or_buf
path_or_buf
Data.to_duckdb(
    connection=None,
    table=None,
    schema=None,
    catalog=None,
    write_format=None,
    write_path='.',
    write_options=None,
    mkdir_kwargs=None,
    to_utc=None,
    remove_utc_tz=True,
    if_exists='fail',
    connection_config=None,
    check_dict_type=True,
    template_context=None,
    return_meta=False,
    return_connection=False
)

connection
return_connection
write_format
write_path
catalog
schema
if_exists
if_exists
if_exists
write_format
write_path
write_format
COPY
dict(compression="gzip")
to_utc
remove_utc_tz
to_utc
Data.to_feather(
    path_or_buf='.',
    mkdir_kwargs=None,
    check_dict_type=True,
    template_context=None,
    return_meta=False,
    **kwargs
)

path_or_buf
path_or_buf
Data.to_feature_oriented(
    **kwargs
)

Data.to_hdf(
    path_or_buf='.',
    key=None,
    mkdir_kwargs=None,
    format='table',
    check_dict_type=True,
    template_context=None,
    return_meta=False,
    **kwargs
)

path_or_buf
Data.to_parquet(
    path_or_buf='.',
    mkdir_kwargs=None,
    partition_cols=None,
    partition_by=None,
    period_index_to='str',
    groupby_kwargs=None,
    keep_groupby_names=False,
    engine=None,
    check_dict_type=True,
    template_context=None,
    return_meta=False,
    **kwargs
)

path_or_buf
path_or_buf
partition_cols
partition_by
path_or_buf
partition_by
**groupby_kwargs
partition_cols
partition_cols
Data.to_sql(
    engine=None,
    table=None,
    schema=None,
    to_utc=None,
    remove_utc_tz=True,
    attach_row_number=False,
    from_row_number=None,
    row_number_column=None,
    engine_config=None,
    dispose_engine=None,
    check_dict_type=True,
    template_context=None,
    return_meta=False,
    return_engine=False,
    **kwargs
)

engine
dispose_engine
return_engine
schema
to_utc
remove_utc_tz
to_utc
Data.to_symbol_oriented(
    **kwargs
)

Data.transform(
    transform_func,
    *args,
    per_feature=False,
    per_symbol=False,
    pass_frame=False,
    key_wrapper_kwargs=None,
    template_context=None,
    **kwargs
)

per_feature
per_symbol
pass_frame
pass_frame
transform_func
key_wrapper_kwargs
Data.try_fetch_feature(
    feature,
    skip_on_error=False,
    silence_warnings=False,
    fetch_kwargs=None
)

Data.try_fetch_symbol(
    symbol,
    skip_on_error=False,
    silence_warnings=False,
    fetch_kwargs=None
)

Data.try_run(
    data,
    func_name,
    *args,
    raise_errors=False,
    silence_warnings=False,
    **kwargs
)

Data.try_update_feature(
    feature,
    skip_on_error=False,
    silence_warnings=False,
    update_kwargs=None
)

Data.try_update_symbol(
    symbol,
    skip_on_error=False,
    silence_warnings=False,
    update_kwargs=None
)

Data.update(
    *,
    concat=True,
    skip_on_error=None,
    silence_warnings=None,
    execute_kwargs=None,
    return_raw=False,
    **kwargs
)

concat
bool
skip_on_error
bool
silence_warnings
bool
execute_kwargs
dict
return_raw
bool
**kwargs
Data.update_feature(
    feature,
    **kwargs
)

Data.update_fetch_kwargs(
    check_dict_type=True,
    **kwargs
)

Data.update_returned_kwargs(
    check_dict_type=True,
    **kwargs
)

Data.update_symbol(
    symbol,
    **kwargs
)

Data.use_feature_config_of(
    cls
)

DataWithFeatures()

DataWithFeatures.field_config
${cls_name}
${feature_config}

MetaData(
    *args,
    **kwargs
)

StatsBuilderMixin.metrics
builtins.type
MetaFeatures(
    *args,
    **kwargs
)

builtins.type
OHLCDataMixin()

OHLCDataMixin.get_daily_log_returns(
    **kwargs
)

OHLCDataMixin.get_daily_returns(
    **kwargs
)

OHLCDataMixin.get_drawdowns(
    **kwargs
)

OHLCDataMixin.get_log_returns(
    **kwargs
)

OHLCDataMixin.get_returns(
    **kwargs
)

OHLCDataMixin.get_returns_acc(
    **kwargs
)

feature_dict(
    *args,
    **kwargs
)

builtins.dict
key_dict(
    *args,
    **kwargs
)

builtins.dict
run_arg_dict(
    *args,
    **kwargs
)

builtins.dict
run_func_dict(
    *args,
    **kwargs
)

builtins.dict
symbol_dict(
    *args,
    **kwargs
)

builtins.dict
>>> import vectorbtpro as vbt

>>> def get_yf_symbol(symbol, period="max", start=None, end=None, **kwargs):
...     import yfinance as yf
...     if start is not None:
...         start = vbt.dt.to_tzaware_datetime(start, tz=vbt.dt.get_local_tz())  Convert to datetime using to_tzaware_datetime
...     if end is not None:
...         end = vbt.dt.to_tzaware_datetime(end, tz=vbt.dt.get_local_tz())
...     return yf.Ticker(symbol).history(
...         period=period, 
...         start=start, 
...         end=end, 
...         **kwargs
...     )

>>> get_yf_symbol("BTC-USD", start="2020-01-01", end="2020-01-05")
                                  Open         High          Low        Close  \\
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2019-12-31 00:00:00+00:00  21167946112        0.0           0.0  
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  

2019-12-31
2020-01-01
2020-01-01
2019-12-31 22:00:00
2020-01-01 UTC
get_yf_symbol
>>> class YFData(vbt.Data):
...     @classmethod
...     def fetch_symbol(cls, symbol, **kwargs):
...         return get_yf_symbol(symbol, **kwargs)

get_yf_symbol
YFData
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start="2020-01-01", 
...     end="2020-01-05"
... )

>>> yf_data.data["ETH-USD"]
                                 Open        High         Low       Close  \\
Date                                                                        
2019-12-31 00:00:00+00:00  132.612274  133.732681  128.798157  129.610855   
2020-01-01 00:00:00+00:00  129.630661  132.835358  129.198288  130.802002   
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179   
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707   
2020-01-04 00:00:00+00:00  134.168518  136.052719  133.040558  135.069366   

                                Volume  Dividends  Stock Splits  
Date                                                             
2019-12-31 00:00:00+00:00   8936866397        0.0           0.0  
2020-01-01 00:00:00+00:00   7935230330        0.0           0.0  
2020-01-02 00:00:00+00:00   8032709256        0.0           0.0  
2020-01-03 00:00:00+00:00  10476845358        0.0           0.0  
2020-01-04 00:00:00+00:00   7430904515        0.0           0.0 

None
skip_on_error
>>> import pandas as pd

>>> class YFData(vbt.Data):
...     @classmethod
...     def fetch_symbol(cls, symbol, **kwargs):
...         returned_kwargs = dict(timestamp=pd.Timestamp.now())
...         return get_yf_symbol(symbol, **kwargs), returned_kwargs

>>> yf_data = YFData.pull("BTC-USD", start="2020-01-01", end="2020-01-05")
>>> yf_data.returned_kwargs
symbol_dict({'BTC-USD': {'timestamp': Timestamp('2023-08-28 20:08:50.893763')}})

>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start=vbt.symbol_dict({  Use symbol_dict to specify any argument per symbol
...         "BTC-USD": "2020-01-01", 
...         "ETH-USD": "2020-01-03"
...     }),
...     end=vbt.symbol_dict({
...         "BTC-USD": "2020-01-03", 
...         "ETH-USD": "2020-01-05"
...     })
... )
UserWarning: Symbols have mismatching index. Setting missing data points to NaN.

>>> yf_data.data["BTC-USD"]
                                  Open         High          Low        Close  \\
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00          NaN          NaN          NaN          NaN   
2020-01-04 00:00:00+00:00          NaN          NaN          NaN          NaN   

                                 Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00  2.116795e+10        0.0           0.0  
2020-01-01 00:00:00+00:00  1.856566e+10        0.0           0.0  
2020-01-02 00:00:00+00:00  2.080208e+10        0.0           0.0  
2020-01-03 00:00:00+00:00           NaN        NaN           NaN  
2020-01-04 00:00:00+00:00           NaN        NaN           NaN  

>>> yf_data.data["ETH-USD"]
                                 Open        High         Low       Close  \\
Date                                                                        
2019-12-31 00:00:00+00:00         NaN         NaN         NaN         NaN   
2020-01-01 00:00:00+00:00         NaN         NaN         NaN         NaN   
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179   
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707   
2020-01-04 00:00:00+00:00  134.168518  136.052719  133.040558  135.069366   

                                 Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00           NaN        NaN           NaN  
2020-01-01 00:00:00+00:00           NaN        NaN           NaN  
2020-01-02 00:00:00+00:00  8.032709e+09        0.0           0.0  
2020-01-03 00:00:00+00:00  1.047685e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  7.430905e+09        0.0           0.0 

missing_index
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start=vbt.symbol_dict({  # (1)!
...         "BTC-USD": "2020-01-01", 
...         "ETH-USD": "2020-01-03"
...     }),
...     end=vbt.symbol_dict({
...         "BTC-USD": "2020-01-03", 
...         "ETH-USD": "2020-01-05"
...     }),
...     missing_index="drop"
... )
UserWarning: Symbols have mismatching index. Dropping missing data points.

>>> yf_data.data["BTC-USD"]
                                 Open         High         Low        Close  \\
Date                                                                          
2020-01-02 00:00:00+00:00  7202.55127  7212.155273  6935.27002  6985.470215   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  

>>> yf_data.data["ETH-USD"]
                                 Open        High        Low       Close  \\
Date                                                                       
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.95491  127.410179   

                               Volume  Dividends  Stock Splits  
Date                                                            
2020-01-02 00:00:00+00:00  8032709256        0.0           0.0  

YFData
>>> class YFData(vbt.Data):
...     @classmethod
...     def fetch_symbol(cls, symbol, **kwargs):
...         return get_yf_symbol(symbol, **kwargs)
...
...     def update_symbol(self, symbol, **kwargs):
...         defaults = self.select_fetch_kwargs(symbol)  
...         defaults["start"] = self.select_last_index(symbol)  
...         kwargs = vbt.merge_dicts(defaults, kwargs)  
...         return self.fetch_symbol(symbol, **kwargs)  Pass the final arguments to Data.fetch_symbol

>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start=vbt.symbol_dict({
...         "BTC-USD": "2020-01-01", 
...         "ETH-USD": "2020-01-03"
...     }),
...     end=vbt.symbol_dict({
...         "BTC-USD": "2020-01-03", 
...         "ETH-USD": "2020-01-05"
...     })
... )
UserWarning: Symbols have mismatching index. Setting missing data points to NaN.

YFData
BTC-USD
ETH-USD
>>> yf_data.last_index
symbol_dict({
    'BTC-USD': Timestamp('2020-01-02 00:00:00+0000', tz='UTC'), 
    'ETH-USD': Timestamp('2020-01-04 00:00:00+0000', tz='UTC')
})

>>> yf_data.fetch_kwargs
symbol_dict({
    'BTC-USD': {'start': '2020-01-01', 'end': '2020-01-03'}, 
    'ETH-USD': {'start': '2020-01-03', 'end': '2020-01-05'}
})

start
end
>>> yf_data_updated = yf_data.update(end="2020-01-06")  Same date for both symbols

>>> yf_data_updated.data["BTC-USD"]
                                  Open         High          Low        Close  \\
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   
2020-01-05 00:00:00+00:00  7410.451660  7544.497070  7400.535645  7411.317383   

                                 Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00  2.116795e+10        0.0           0.0  
2020-01-01 00:00:00+00:00  1.856566e+10        0.0           0.0  
2020-01-02 00:00:00+00:00  2.080208e+10        0.0           0.0  
2020-01-03 00:00:00+00:00  2.811148e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  1.844427e+10        0.0           0.0  
2020-01-05 00:00:00+00:00  1.972507e+10        0.0           0.0  

>>> yf_data_updated.data["ETH-USD"]
                                 Open        High         Low       Close  \\
Date                                                                        
2019-12-31 00:00:00+00:00         NaN         NaN         NaN         NaN   
2020-01-01 00:00:00+00:00         NaN         NaN         NaN         NaN   
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179   
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707   
2020-01-04 00:00:00+00:00  134.168518  136.052719  133.040558  135.069366   
2020-01-05 00:00:00+00:00  135.072098  139.410202  135.045624  136.276779   

                                 Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00           NaN        NaN           NaN  
2020-01-01 00:00:00+00:00           NaN        NaN           NaN  
2020-01-02 00:00:00+00:00  8.032709e+09        0.0           0.0  
2020-01-03 00:00:00+00:00  1.047685e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  7.430905e+09        0.0           0.0  
2020-01-05 00:00:00+00:00  7.526675e+09        0.0           0.0 

last_index
BTC-USD
2020-01-02
2020-01-05
ETH-USD
2020-01-04
2020-01-05
>>> yf_data_updated.last_index
symbol_dict({
    'BTC-USD': Timestamp('2020-01-05 00:00:00+0000', tz='UTC'), 
    'ETH-USD': Timestamp('2020-01-05 00:00:00+0000', tz='UTC')
})

last_index
>>> yf_data_updated = yf_data_updated.update(start="2020-01-01", end="2020-01-02")

>>> yf_data_updated.data["BTC-USD"]
                                  Open         High          Low        Close  \\
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   

                                 Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00  2.116795e+10        0.0           0.0  
2020-01-01 00:00:00+00:00  1.856566e+10        0.0           0.0 

>>> yf_data_updated.data["ETH-USD"]
                                 Open        High         Low       Close  \\
Date                                                                        
2019-12-31 00:00:00+00:00  132.612274  133.732681  128.798157  129.610855   
2020-01-01 00:00:00+00:00  129.630661  132.835358  129.198288  130.802002   

                                 Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00  8.936866e+09        0.0           0.0  
2020-01-01 00:00:00+00:00  7.935230e+09        0.0           0.0  

concat
>>> yf_data_new = yf_data.update(end="2020-01-06", concat=False)

>>> yf_data_new.data["BTC-USD"]
                                  Open         High          Low        Close  \\
Date                                                                            
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   
2020-01-05 00:00:00+00:00  7410.451660  7544.497070  7400.535645  7411.317383   

                                 Volume  Dividends  Stock Splits  
Date                                                              
2020-01-02 00:00:00+00:00  2.080208e+10        0.0           0.0  
2020-01-03 00:00:00+00:00  2.811148e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  1.844427e+10        0.0           0.0  
2020-01-05 00:00:00+00:00  1.972507e+10        0.0           0.0  

>>> yf_data_new.data["ETH-USD"]
                                 Open        High         Low       Close  \\
Date                                                                        
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179   
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707   
2020-01-04 00:00:00+00:00  134.168518  136.052719  133.040558  135.069366   
2020-01-05 00:00:00+00:00  135.072098  139.410202  135.045624  136.276779   

                                 Volume  Dividends  Stock Splits  
Date                                                              
2020-01-02 00:00:00+00:00  8.032709e+09        0.0           0.0  
2020-01-03 00:00:00+00:00  1.047685e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  7.430905e+09        0.0           0.0  
2020-01-05 00:00:00+00:00  7.526675e+09        0.0           0.0  

2019-12-31
2020-01-01
ETH-USD
2020-01-04
2020-01-05
2020-01-02
2020-01-03
BTC-USD
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start="2020-01-01", 
...     end="2020-01-05"
... )

>>> yf_data.get(symbols="BTC-USD")
                                  Open         High          Low        Close  \\
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2019-12-31 00:00:00+00:00  21167946112        0.0           0.0  
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  

>>> yf_data.get(features=["High", "Low"], symbols="BTC-USD")
                                  High          Low
Date                                               
2019-12-31 00:00:00+00:00  7335.290039  7169.777832
2020-01-01 00:00:00+00:00  7254.330566  7174.944336
2020-01-02 00:00:00+00:00  7212.155273  6935.270020
2020-01-03 00:00:00+00:00  7413.715332  6914.996094
2020-01-04 00:00:00+00:00  7427.385742  7309.514160

>>> yf_data.get(features="Close")
symbol                         BTC-USD     ETH-USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121  129.610855
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366

>>> open_price, close_price = yf_data.get(features=["Open", "Close"])  Tuple with DataFrames, one per feature

features
symbols
yf_data.get(features="Close")
features=["Close"]
>>> yf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121  129.610855
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366

>>> yf_data.returns
symbol                      BTC-USD   ETH-USD
Date                                         
2019-12-31 00:00:00+00:00  0.000000  0.000000
2020-01-01 00:00:00+00:00  0.000914  0.009190
2020-01-02 00:00:00+00:00 -0.029819 -0.025931
2020-01-03 00:00:00+00:00  0.051452  0.053069
2020-01-04 00:00:00+00:00  0.008955  0.006690

data
close
>>> yf_data.run("sma", 3)
<vectorbtpro.indicators.factory.talib.SMA at 0x7ff75218b5b0>

>>> yf_data.run("talib_sma", 3)
<vectorbtpro.indicators.factory.talib.SMA at 0x7ff752111070>

>>> yf_data.run("from_holding")
<vectorbtpro.portfolio.base.Portfolio at 0x7ff767e18970>

>>> yf_data.features
['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']

>>> yf_data.symbols
['BTC-USD', 'ETH-USD']

>>> yf_data.single_key
False

>>> yf_data.last_index["BTC-USD"]
Timestamp('2020-01-04 00:00:00+0000', tz='UTC')

dict(data.fetch_kwargs[symbol])
>>> yf_data.select("BTC-USD")
<__main__.YFData at 0x7ff6a97f4b38>

>>> yf_data.select("BTC-USD").get()
                                  Open         High          Low        Close  \\
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2019-12-31 00:00:00+00:00  21167946112        0.0           0.0  
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  

YFData
BTC-USD
copy_mode_="deep"
>>> yf_data.rename({
...     "BTC-USD": "BTC/USD",
...     "ETH-USD": "ETH/USD"
... }).get("Close")
symbol                         BTC/USD     ETH/USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121  129.610855
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366

classes
symbol_class
>>> cls_yfdata = vbt.YFData.pull(
...     ["META", "GOOGL", "NFLX", "BAC", "WFC", "TLT", "SHV"],
...     classes=[
...         dict(class1="Equity", class2="Technology"),
...         dict(class1="Equity", class2="Technology"),
...         dict(class1="Equity", class2="Technology"),
...         dict(class1="Equity", class2="Financial"),
...         dict(class1="Equity", class2="Financial"),
...         dict(class1="Fixed Income", class2="Treasury"),
...         dict(class1="Fixed Income", class2="Treasury"),
...     ],
...     start="2010-01-01",
...     missing_columns="nan"
... )
>>> cls_yfdata.close
class1                         Equity                                     \\
class2                     Technology                          Financial   
symbol                           META       GOOGL        NFLX        BAC   
Date                                                                       
2010-01-04 00:00:00-05:00         NaN   15.684434    7.640000  12.977036   
2010-01-05 00:00:00-05:00         NaN   15.615365    7.358571  13.398854   
2010-01-06 00:00:00-05:00         NaN   15.221722    7.617143  13.555999   
...                               ...         ...         ...        ...   
2023-08-24 00:00:00-04:00  286.750000  129.779999  406.929993  28.620001   
2023-08-25 00:00:00-04:00  285.500000  129.880005  416.029999  28.500000   
2023-08-28 00:00:00-04:00  290.260010  131.009995  418.059998  28.764999   

class1                               Fixed Income              
class2                                   Treasury              
symbol                           WFC          TLT         SHV  
Date                                                           
2010-01-04 00:00:00-05:00  19.073046    62.717960   99.920975  
2010-01-05 00:00:00-05:00  19.596645    63.123013   99.884712  
2010-01-06 00:00:00-05:00  19.624567    62.278038   99.893806  
...                              ...          ...         ...  
2023-08-24 00:00:00-04:00  41.430000    94.910004  110.389999  
2023-08-25 00:00:00-04:00  41.230000    95.220001  110.389999  
2023-08-28 00:00:00-04:00  41.880001    95.320000  110.400002  

[3436 rows x 7 columns]

>>> new_cls_yfdata = cls_yfdata.replace(
...     classes=vbt.symbol_dict({
...         "META": dict(class1="Equity", class2="Technology"),
...         "GOOGL": dict(class1="Equity", class2="Technology"),
...         "NFLX": dict(class1="Equity", class2="Technology"),
...         "BAC": dict(class1="Equity", class2="Financial"),
...         "WFC": dict(class1="Equity", class2="Financial"),
...         "TLT": dict(class1="Fixed Income", class2="Treasury"),
...         "SHV": dict(class1="Fixed Income", class2="Treasury"),
...     })
... )

>>> new_cls_yfdata = cls_yfdata.update_classes(
...     class1=vbt.symbol_dict({
...         "META": "Equity",
...         "GOOGL": "Equity",
...         "NFLX": "Equity",
...         "BAC": "Equity",
...         "WFC": "Equity",
...         "TLT": "Fixed Income",
...         "SHV": "Fixed Income",
...     }),
...     class2=vbt.symbol_dict({
...         "META": "Technology",
...         "GOOGL": "Technology",
...         "NFLX": "Technology",
...         "BAC": "Financial",
...         "WFC": "Financial",
...         "TLT": "Treasury",
...         "SHV": "Treasury",
...     })
... )

symbol
BTCUSDT
>>> btc_df = pd.DataFrame({
...     "Open": [7194.89, 7202.55, 6984.42],
...     "High": [7254.33, 7212.15, 7413.71],
...     "Low": [7174.94, 6935.27, 6985.47],
...     "Close": [7200.17, 6985.47, 7344.88]
... }, index=pd.date_range("2020-01-01", periods=3))
>>> btc_df
               Open     High      Low    Close
2020-01-01  7194.89  7254.33  7174.94  7200.17
2020-01-02  7202.55  7212.15  6935.27  6985.47
2020-01-03  6984.42  7413.71  6985.47  7344.88

>>> my_data = vbt.Data.from_data(btc_df)
>>> my_data.hlc3
2020-01-01 00:00:00+00:00    7209.813333
2020-01-02 00:00:00+00:00    7044.296667
2020-01-03 00:00:00+00:00    7248.020000
Freq: D, dtype: float64

>>> eth_df = pd.DataFrame({
...     "Open": [127.41, 134.16, 135.07],
...     "High": [134.55, 136.05, 139.41],
...     "Low": [126.49, 133.04, 135.04],
...     "Close": [134.17, 135.06, 136.27]
... }, index=pd.date_range("2020-01-03", periods=3))  Use different dates to demonstrate alignment
>>> eth_df
              Open    High     Low   Close
2020-01-03  127.41  134.55  126.49  134.17
2020-01-04  134.16  136.05  133.04  135.06
2020-01-05  135.07  139.41  135.04  136.27

>>> my_data = vbt.Data.from_data({"BTCUSDT": btc_df, "ETHUSDT": eth_df})
>>> my_data.hlc3
symbol                         BTCUSDT     ETHUSDT
2020-01-01 00:00:00+00:00  7209.813333         NaN
2020-01-02 00:00:00+00:00  7044.296667         NaN
2020-01-03 00:00:00+00:00  7248.020000  131.736667
2020-01-04 00:00:00+00:00          NaN  134.716667
2020-01-05 00:00:00+00:00          NaN  136.906667

columns_are_symbols
>>> hlc3_data = vbt.Data.from_data(my_data.hlc3, columns_are_symbols=True)
>>> hlc3_data.get()
symbol                         BTCUSDT     ETHUSDT
2020-01-01 00:00:00+00:00  7209.813333         NaN
2020-01-02 00:00:00+00:00  7044.296667         NaN
2020-01-03 00:00:00+00:00  7248.020000  131.736667
2020-01-04 00:00:00+00:00          NaN  134.716667
2020-01-05 00:00:00+00:00          NaN  136.906667

invert_data=True
>>> yf_data_btc = YFData.pull(
...     "BTC-USD", 
...     start="2020-01-01", 
...     end="2020-01-03"
... )
>>> yf_data_eth = YFData.pull(
...     "ETH-USD", 
...     start="2020-01-03", 
...     end="2020-01-05"
... )
>>> merged_yf_data = YFData.merge(yf_data_btc, yf_data_eth)
>>> merged_yf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121         NaN
2020-01-01 00:00:00+00:00  7200.174316         NaN
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00          NaN  134.171707
2020-01-04 00:00:00+00:00          NaN  135.069366
UserWarning: Symbols have mismatching index. Setting missing data points to NaN.

>>> yf_data_btc1 = YFData.pull(
...     "BTC-USD", 
...     start="2020-01-01", 
...     end="2020-01-03"
... )
>>> yf_data_btc2 = YFData.pull(
...     "BTC-USD", 
...     start="2020-01-05", 
...     end="2020-01-07"
... )
>>> yf_data_eth = YFData.pull(
...     "ETH-USD", 
...     start="2020-01-06", 
...     end="2020-01-08"
... )
>>> merged_yf_data = YFData.merge(yf_data_btc1, yf_data_btc2, yf_data_eth)
>>> merged_yf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121         NaN
2020-01-01 00:00:00+00:00  7200.174316         NaN
2020-01-02 00:00:00+00:00  6985.470215         NaN
2020-01-04 00:00:00+00:00  7410.656738         NaN
2020-01-05 00:00:00+00:00  7411.317383  136.276779
2020-01-06 00:00:00+00:00  7769.219238  144.304153
2020-01-07 00:00:00+00:00          NaN  143.543991

YFData
YFData
YFData
YFData.update_symbol
fetch_symbol
update_symbol
>>> bn_data_btc = vbt.BinanceData.pull(
...     "BTCUSDT", 
...     start="2020-01-01", 
...     end="2020-01-04")
>>> bn_data_btc.close
Open time
2020-01-01 00:00:00+00:00    7200.85
2020-01-02 00:00:00+00:00    6965.71
2020-01-03 00:00:00+00:00    7344.96
Freq: D, Name: Close, dtype: float64

>>> ccxt_data_eth = vbt.CCXTData.pull(
...     "ETH/USDT", 
...     start="2020-01-03", 
...     end="2020-01-06")
>>> ccxt_data_eth.close
Open time
2020-01-03 00:00:00+00:00    134.35
2020-01-04 00:00:00+00:00    134.20
2020-01-05 00:00:00+00:00    135.37
Freq: D, Name: Close, dtype: float64

>>> class MergedData(vbt.Data):
...     @classmethod
...     def fetch_symbol(cls, symbol, **kwargs):
...         if symbol.startswith("BN_"):
...             return vbt.BinanceData.fetch_symbol(symbol[3:], **kwargs)
...         if symbol.startswith("CCXT_"):
...             return vbt.CCXTData.fetch_symbol(symbol[5:], **kwargs)
...         raise ValueError(f"Unknown symbol '{symbol}'")
...
...     def update_symbol(self, symbol, **kwargs):
...         fetch_kwargs = self.select_fetch_kwargs(symbol)
...         fetch_kwargs["start"] = self.select_last_index(symbol)
...         kwargs = vbt.merge_dicts(fetch_kwargs, kwargs)
...         return self.fetch_symbol(symbol, **kwargs)

>>> merged_data = MergedData.merge(
...     bn_data_btc, 
...     ccxt_data_eth,
...     rename={
...         "BTCUSDT": "BN_BTCUSDT", 
...         "ETH/USDT": "CCXT_ETH/USDT"
...     },
...     missing_columns="drop"
... )
UserWarning: Symbols have mismatching index. Setting missing data points to NaN.
UserWarning: Symbols have mismatching columns. Dropping missing data points.

>>> merged_data = merged_data.update(end="2020-01-07")
>>> merged_data.close
symbol                     BN_BTCUSDT  CCXT_ETH/USDT
Open time                                           
2020-01-01 00:00:00+00:00     7200.85            NaN
2020-01-02 00:00:00+00:00     6965.71            NaN
2020-01-03 00:00:00+00:00     7344.96         134.35
2020-01-04 00:00:00+00:00     7354.11         134.20
2020-01-05 00:00:00+00:00     7358.75         135.37
2020-01-06 00:00:00+00:00     7758.00         144.15

>>> vbt.pprint(vbt.BinanceData.feature_config)
HybridConfig({
    'Quote volume': dict(
        resample_func=<function BinanceData.<lambda> at 0x7ff7648d7280>
    ),
    'Taker base volume': dict(
        resample_func=<function BinanceData.<lambda> at 0x7ff7648d7310>
    ),
    'Taker quote volume': dict(
        resample_func=<function BinanceData.<lambda> at 0x7ff7648d73a0>
    )
})

>>> full_yf_data = vbt.YFData.pull("BTC-USD")  Use the built-in Yahoo Finance class - it already knows how to resample features such as dividends
>>> ms_yf_data = full_yf_data.resample("MS")
>>> ms_yf_data.close
Date
2014-09-01 00:00:00+00:00      386.944000
2014-10-01 00:00:00+00:00      338.321014
2014-11-01 00:00:00+00:00      378.046997
                                      ...     
2023-06-01 00:00:00+00:00    30477.251953
2023-07-01 00:00:00+00:00    29230.111328
2023-08-01 00:00:00+00:00    25995.177734
Freq: MS, Name: Close, Length: 108, dtype: float64

>>> resampler = vbt.Resampler.from_pd_date_range(
...     full_yf_data.wrapper.index,
...     start=full_yf_data.wrapper.index[0],
...     end=full_yf_data.wrapper.index[-1],
...     freq="Y",
...     silence_warnings=True
... )
>>> y_yf_data = full_yf_data.resample(resampler)
>>> y_yf_data.close
2014-12-31 00:00:00+00:00      426.619995
2015-12-31 00:00:00+00:00      961.237976
2016-12-31 00:00:00+00:00    12952.200195
2017-12-31 00:00:00+00:00     3865.952637
2018-12-31 00:00:00+00:00     7292.995117
2019-12-31 00:00:00+00:00    28840.953125
2020-12-31 00:00:00+00:00    47178.125000
2021-12-31 00:00:00+00:00    39194.972656
Freq: A-DEC, Name: Close, dtype: float64

bn_data_btc
>>> data_btc = vbt.Data.from_data(bn_data_btc.data, single_key=True)
>>> data_btc.resample("MS")
ValueError: Cannot resample feature 'Quote volume'. Specify resample_func in feature_config.

>>> for k, v in vbt.BinanceData.feature_config.items():
...     data_btc.feature_config[k] = v
>>> data_btc.resample("MS")
<vectorbtpro.data.base.Data at 0x7fc0dfce1630>

>>> data_btc = vbt.Data.from_data(bn_data_btc.data, single_key=True)
>>> data_btc.use_feature_config_of(vbt.BinanceData)
>>> data_btc.resample("MS").close
Open time
2020-01-01 00:00:00+00:00    7344.96
Freq: MS, Name: Close, dtype: float64

>>> data = vbt.YFData.pull(
...     ["BTC-USD", "AAPL"], 
...     start="2020-01-01", 
...     end="2020-01-04"
... )
>>> data.close
symbol                         BTC-USD       AAPL
Date                                             
2020-01-01 00:00:00+00:00  7200.174316        NaN
2020-01-02 00:00:00+00:00  6985.470215        NaN
2020-01-02 05:00:00+00:00          NaN  73.249016
2020-01-03 00:00:00+00:00  7344.884277        NaN
2020-01-03 05:00:00+00:00          NaN  72.536888

>>> new_index = data.index.ceil("D").drop_duplicates()
>>> new_data = data.realign(new_index, ffill=False)
>>> new_data.close
symbol                         BTC-USD       AAPL
Date                                             
2020-01-01 00:00:00+00:00  7200.174316        NaN
2020-01-02 00:00:00+00:00  6985.470215        NaN
2020-01-03 00:00:00+00:00  7344.884277  73.249016
2020-01-04 00:00:00+00:00          NaN  72.536888

>>> full_yf_data = YFData.pull(["BTC-USD", "ETH-USD"])
>>> full_yf_data.close.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 3268 entries, 2014-09-17 00:00:00+00:00 to 2023-08-28 00:00:00+00:00
Freq: D
Data columns (total 2 columns):
 #   Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   BTC-USD  3268 non-null   float64
 1   ETH-USD  2119 non-null   float64
dtypes: float64(2)
memory usage: 76.6 KB

>>> new_full_yf_data = full_yf_data.transform(lambda df: df.dropna())
>>> new_full_yf_data.close.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 2119 entries, 2017-11-09 00:00:00+00:00 to 2023-08-28 00:00:00+00:00
Freq: D
Data columns (total 2 columns):
 #   Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   BTC-USD  2119 non-null   float64
 1   ETH-USD  2119 non-null   float64
dtypes: float64(2)
memory usage: 49.7 KB

per_feature=True
per_symbol=True
iloc
loc
xs
[]
>>> sub_yf_data = yf_data.loc["2020-01-01":"2020-01-03"]  Select rows (in loc the second date is inclusive!). Returns a new data instance.
>>> sub_yf_data
<__main__.YFData at 0x7fa9a0012396>

>>> sub_yf_data = sub_yf_data[["Open", "High", "Low", "Close"]]  Select columns. Returns a new data instance.
>>> sub_yf_data
<__main__.YFData at 0x7fa9a0032358>

>>> sub_yf_data.data["BTC-USD"]
                                  Open         High          Low        Close
Date                                                                         
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277

>>> sub_yf_data.data["ETH-USD"]
                                 Open        High         Low       Close
Date                                                                     
2020-01-01 00:00:00+00:00  129.630661  132.835358  129.198288  130.802002
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707

loc
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start=vbt.symbol_dict({
...         "BTC-USD": "2020-01-01", 
...         "ETH-USD": "2020-01-03"
...     }),
...     end=vbt.symbol_dict({
...         "BTC-USD": "2020-01-03", 
...         "ETH-USD": "2020-01-05"
...     })
... )

>>> yf_data.data["BTC-USD"].info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 5 entries, 2019-12-31 00:00:00+00:00 to 2020-01-04 00:00:00+00:00
Freq: D
Data columns (total 7 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   Open          3 non-null      float64
 1   High          3 non-null      float64
 2   Low           3 non-null      float64
 3   Close         3 non-null      float64
 4   Volume        3 non-null      float64
 5   Dividends     3 non-null      float64
 6   Stock Splits  3 non-null      float64
dtypes: float64(7)
memory usage: 320.0 bytes

>>> yf_data.data["BTC-USD"].describe()
              Open         High          Low        Close        Volume  \\
count     3.000000     3.000000     3.000000     3.000000  3.000000e+00   
mean   7230.627441  7267.258626  7093.330729  7126.414551  2.017856e+10   
std      55.394933    62.577102   136.908963   122.105641  1.408740e+09   
min    7194.892090  7212.155273  6935.270020  6985.470215  1.856566e+10   
25%    7198.721680  7233.242920  7052.523926  7089.534668  1.968387e+10   
50%    7202.551270  7254.330566  7169.777832  7193.599121  2.080208e+10   
75%    7248.495117  7294.810303  7172.361084  7196.886719  2.098501e+10   
max    7294.438965  7335.290039  7174.944336  7200.174316  2.116795e+10   

       Dividends  Stock Splits  
count        3.0           3.0  
mean         0.0           0.0  
std          0.0           0.0  
min          0.0           0.0  
25%          0.0           0.0  
50%          0.0           0.0  
75%          0.0           0.0  
max          0.0           0.0  

>>> yf_data.stats()
Start                   2019-12-31 00:00:00+00:00
End                     2020-01-04 00:00:00+00:00
Period                            5 days 00:00:00
Total Symbols                                   2
Null Counts: BTC-USD                           14
Null Counts: ETH-USD                           14
Name: agg_stats, dtype: object

>>> yf_data.stats(column="Volume")  Print the stats for the column Volume only
Start                   2019-12-31 00:00:00+00:00
End                     2020-01-04 00:00:00+00:00
Period                            5 days 00:00:00
Total Symbols                                   2
Null Counts: BTC-USD                            2
Null Counts: ETH-USD                            2
Name: Volume, dtype: object

Volume
symbol
feature
base
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start="2020-01-01", 
...     end="2020-06-01"
... )

>>> yf_data.plot(column="Close", base=100).show()  Since our data is symbol-oriented, column here is an alias for feature

column
feature
>>> yf_data["Close"].get().vbt.rebase(100).vbt.plot()  Using GenericAccessor.rebase and GenericAccessor.plot

>>> yf_data.plots().show()

>>> yf_data.plots(column="Close").show()

template_context
>>> yf_data.plots(
...     column="Close", 
...     template_context=dict(symbols=["BTC-USD"])
... ).show()

plot
>>> from vectorbtpro.utils.colors import adjust_opacity

>>> fig = yf_data.plots(
...     column="Close",
...     subplot_settings=dict(
...         plot_0=dict(trace_kwargs=dict(line_color="mediumslateblue")),
...         plot_1=dict(trace_kwargs=dict(line_color="limegreen"))
...     )
... )
>>> sma = vbt.talib("SMA").run(yf_data.close, vbt.Default(10))  Hide the timeperiod parameter from the column hierarchy by wrapping it with Default
>>> sma["BTC-USD"].real.rename("SMA(10)").vbt.plot(
...     trace_kwargs=dict(line_color=adjust_opacity("mediumslateblue", 0.7)),
...     add_trace_kwargs=dict(row=1, col=1),  Specify the subplot to plot this SMA over
...     fig=fig
... )
>>> sma["ETH-USD"].real.rename("SMA(10)").vbt.plot(
...     trace_kwargs=dict(line_color=adjust_opacity("limegreen", 0.7)),
...     add_trace_kwargs=dict(row=2, col=1),
...     fig=fig
... )
>>> fig.show()

timeperiod
YFData
repo
conda create --name vectorbtpro python=3.10

conda activate vectorbtpro

vectorbtpro
*
conda info --envs

pip
pip uninstall vectorbt

git+https
pip install -U "vectorbtpro[base] @ git+https://github.com/polakowo/vectorbt.pro.git"

GH_TOKEN
pip install -U "vectorbtpro[base] @ git+https://${GH_TOKEN}@github.com/polakowo/vectorbt.pro.git"

git+ssh
pip install -U "vectorbtpro[base] @ git+ssh://git@github.com/polakowo/vectorbt.pro.git"

pip install -U git+https://github.com/polakowo/vectorbt.pro.git

develop
pip install -U git+https://github.com/polakowo/vectorbt.pro.git@develop

%env
%env GH_USER=abcdef...
%env GH_TOKEN=abcdef...

!conda install -c conda-forge ta-lib --yes

!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
!tar -xzvf ta-lib-0.4.0-src.tar.gz
%cd ta-lib
!./configure --prefix=/usr
!make
!make install

!pip install -U "vectorbtpro[base] @ git+https://${GH_USER}:${GH_TOKEN}@github.com/polakowo/vectorbt.pro.git"

# setup.py
setup(
    # ...
    install_requires=[
        "vectorbtpro @ git+https://github.com/polakowo/vectorbt.pro.git"
    ]
    # ...
)

git
git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro

pip install -e vectorbtpro

git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro --depth=1

git pull --unshallow

git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro --depth=1

cd vectorbtpro

docker build . -t vectorbtpro

mkdir work

docker run -it --rm -p 8888:8888 -v "$PWD/work":/home/jovyan/work vectorbtpro

-v
{PWD/work}
/home/jovyan/work
~/work
docker run -it --rm -p 10000:8888 -v "$PWD/work":/home/jovyan/work vectorbtpro

http://127.0.0.1:8888/lab?token=9e85949d9901633d1de9dad7a963b43257e29fb232883908
work
y
git pull

docker build . -t vectorbtpro

Code
pip install ".[base]"

.whl
pip install wheel
pip install "filename.whl[base]"

filename
(1)
(1)
pybind11
pip install pybind11

llvmlite
pip install --ignore-installed 'llvmlite'

show_svg()
pip install kaleido==0.1.0post1

>>> import numpy as np

>>> def rolling_window(a, window):
...     shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)
...     strides = a.strides + (a.strides[-1],)
...     return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)

>>> np.mean(rolling_window(np.arange(10), 3), axis=1)
array([1., 2., 3., 4., 5., 6., 7., 8.])

>>> import pandas as pd

>>> index = pd.date_range('2020-01-01', '2020-01-10')
>>> sr = pd.Series(range(len(index)), index=index)
>>> sr.rolling(3).mean()
2020-01-01    NaN
2020-01-02    NaN
2020-01-03    1.0
2020-01-04    2.0
2020-01-05    3.0
2020-01-06    4.0
2020-01-07    5.0
2020-01-08    6.0
2020-01-09    7.0
2020-01-10    8.0
Freq: D, dtype: float64

>>> from numba import njit

>>> @njit
... def moving_average_nb(a, window_len):
...     b = np.empty_like(a, dtype=np.float_)
...     for i in range(len(a)):
...         window_start = max(0, i + 1 - window_len)
...         window_end = i + 1
...         if window_end - window_start < window_len:
...             b[i] = np.nan
...         else:
...             b[i] = np.mean(a[window_start:window_end])
...     return b

>>> moving_average_nb(np.arange(10), 3)
array([nan, nan, 1., 2., 3., 4., 5., 6., 7., 8.])

>>> big_a = np.arange(1000000)
>>> %timeit moving_average_nb.py_func(big_a, 10)  
6.54 s ± 142 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %timeit np.mean(rolling_window(big_a, 10), axis=1)  
24.7 ms ± 173 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> %timeit pd.Series(big_a).rolling(10).mean()  
10.2 ms ± 309 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

>>> %timeit moving_average_nb(big_a, 10)  
5.12 ms ± 7.21 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

nb
>>> arr = sr.values
>>> result = moving_average_nb(arr, 3)
>>> new_sr = pd.Series(result, index=sr.index, name=sr.name)
>>> new_sr
2020-01-01    NaN
2020-01-02    NaN
2020-01-03    1.0
2020-01-04    2.0
2020-01-05    3.0
2020-01-06    4.0
2020-01-07    5.0
2020-01-08    6.0
2020-01-09    7.0
2020-01-10    8.0
Freq: D, dtype: float64

>>> import vectorbtpro as vbt

>>> sr.vbt.rolling_mean(3)
2020-01-01    NaN
2020-01-02    NaN
2020-01-03    1.0
2020-01-04    2.0
2020-01-05    3.0
2020-01-06    4.0
2020-01-07    5.0
2020-01-08    6.0
2020-01-09    7.0
2020-01-10    8.0
Freq: D, dtype: float64

vbt
>>> df = pd.DataFrame({'a': range(10), 'b': range(9, -1, -1)})
>>> df.vbt.rolling_mean(3)
     a    b
0  NaN  NaN
1  NaN  NaN
2  1.0  8.0
3  2.0  7.0
4  3.0  6.0
5  4.0  5.0
6  5.0  4.0
7  6.0  3.0
8  7.0  2.0
9  8.0  1.0

rolling_mean
vbt
GenericAccessor
vbt.returns
>>> ret = pd.Series([0.1, 0.2, -0.1])
>>> ret.vbt.returns.total()
0.18800000000000017

>>> p1 = pd.DataFrame({
...     'open': [1, 2, 3, 4, 5],
...     'high': [2.5, 3.5, 4.5, 5.5, 6.5],
...     'low': [0.5, 1.5, 2.5, 3.5, 4.5],
...     'close': [2, 3, 4, 5, 6]
... }, index=pd.date_range('2020-01-01', '2020-01-05'))
>>> p1
            open  high  low  close
2020-01-01     1   2.5  0.5      2
2020-01-02     2   3.5  1.5      3
2020-01-03     3   4.5  2.5      4
2020-01-04     4   5.5  3.5      5
2020-01-05     5   6.5  4.5      6

>>> single_pf = vbt.Portfolio.from_holding(
...     open=p1['open'], 
...     high=p1['high'], 
...     low=p1['low'], 
...     close=p1['close']
... )
>>> single_pf.value
2020-01-01    100.0
2020-01-02    150.0
2020-01-03    200.0
2020-01-04    250.0
2020-01-05    300.0
Freq: D, dtype: float64

>>> p2 = pd.DataFrame({
...     'open': [6, 5, 4, 3, 2],
...     'high': [6.5, 5.5, 4.5, 3.5, 2.5],
...     'low': [4.5, 3.5, 2.5, 1.5, 0.5],
...     'close': [5, 4, 3, 2, 1]
... }, index=pd.date_range('2020-01-01', '2020-01-05'))
>>> p2
            open  high  low  close
2020-01-01     6   6.5  4.5      5
2020-01-02     5   5.5  3.5      4
2020-01-03     4   4.5  2.5      3
2020-01-04     3   3.5  1.5      2
2020-01-05     2   2.5  0.5      1

>>> multi_open = pd.DataFrame({
...     'p1': p1['open'],
...     'p2': p2['open']
... })
>>> multi_high = pd.DataFrame({
...     'p1': p1['high'],
...     'p2': p2['high']
... })
>>> multi_low = pd.DataFrame({
...     'p1': p1['low'],
...     'p2': p2['low']
... })
>>> multi_close = pd.DataFrame({
...     'p1': p1['close'],
...     'p2': p2['close']
... })

>>> multi_pf = vbt.Portfolio.from_holding(
...     open=multi_open,
...     high=multi_high,
...     low=multi_low,
...     close=multi_close
... )
>>> multi_pf.value
               p1     p2
2020-01-01  100.0  100.0
2020-01-02  150.0   80.0
2020-01-03  200.0   60.0
2020-01-04  250.0   40.0
2020-01-05  300.0   20.0

>>> candle_green = multi_close > multi_open
>>> prev_candle_green = candle_green.vbt.signals.fshift(1)
>>> prev_candle_green
               p1     p2
2020-01-01  False  False
2020-01-02   True  False
2020-01-03   True  False
2020-01-04   True  False
2020-01-05   True  False

>>> candle_red = multi_close < multi_open
>>> prev_candle_red = candle_red.vbt.signals.fshift(1)
>>> prev_candle_red
               p1     p2
2020-01-01  False  False
2020-01-02  False   True
2020-01-03  False   True
2020-01-04  False   True
2020-01-05  False   True

multi_close
multi_open
p1
p2
>>> macd = vbt.MACD.run(
...     multi_close,
...     fast_window=2,
...     slow_window=(3, 4),
...     signal_window=2,
...     macd_wtype="simple",
...     signal_wtype="weighted"
... )
>>> macd.signal
macd_fast_window               2             2  << fast window for MACD line
macd_slow_window               3             4  << slow window for MACD line
macd_signal_window             2             2  << window for signal line
macd_macd_wtype           simple        simple  << window type for MACD line
macd_signal_wtype       weighted      weighted  << window type for signal line   
                         p1   p2       p1   p2  << price
2020-01-01              NaN  NaN      NaN  NaN
2020-01-02              NaN  NaN      NaN  NaN
2020-01-03              NaN  NaN      NaN  NaN
2020-01-04              0.5 -0.5      NaN  NaN
2020-01-05              0.5 -0.5      1.0 -1.0

macd_fast_window
>>> part_arrays = dict(
...     close=pd.DataFrame({
...         'a': [1, 2, 3, 4], 
...         'b': [4, 3, 2, 1]
...     }),  Per element
...     size=pd.Series([1, -1, 1, -1]),  Per row
...     direction=[['longonly', 'shortonly']],  Per column
...     fees=0.01  Per entire array
... )
>>> full_arrays = vbt.broadcast(part_arrays)

>>> full_arrays['close']
   a  b
0  1  4
1  2  3
2  3  2
3  4  1

>>> full_arrays['size']
   a  b
0  1  1
1 -1 -1
2  1  1
3 -1 -1

>>> full_arrays['direction']
          a          b
0  longonly  shortonly
1  longonly  shortonly
2  longonly  shortonly
3  longonly  shortonly

>>> full_arrays['fees']
      a     b
0  0.01  0.01
1  0.01  0.01
2  0.01  0.01
3  0.01  0.01

>>> fast_ma = vbt.MA.run(multi_close, window=[2, 3], short_name='fast')
>>> slow_ma = vbt.MA.run(multi_close, window=[3, 4], short_name='slow')

>>> fast_ma.ma
fast_window    2    2    3    3
              p1   p2   p1   p2
2020-01-01   NaN  NaN  NaN  NaN
2020-01-02   2.5  4.5  NaN  NaN
2020-01-03   3.5  3.5  3.0  4.0
2020-01-04   4.5  2.5  4.0  3.0
2020-01-05   5.5  1.5  5.0  2.0

>>> slow_ma.ma
slow_window    3    3    4    4
              p1   p2   p1   p2
2020-01-01   NaN  NaN  NaN  NaN
2020-01-02   NaN  NaN  NaN  NaN
2020-01-03   3.0  4.0  NaN  NaN
2020-01-04   4.0  3.0  3.5  3.5
2020-01-05   5.0  2.0  4.5  2.5

>>> fast_ma.ma > slow_ma.ma  Comparison with Pandas fails
ValueError: Can only compare identically-labeled DataFrame objects

>>> fast_ma.ma.values > slow_ma.ma.values  Comparison with NumPy succeeds
array([[False, False, False, False],
       [False, False, False, False],
       [ True, False, False, False],
       [ True, False,  True, False],
       [ True, False,  True, False]])

>>> fast_ma.ma.vbt > slow_ma.ma  Comparison with vectorbt succeeds
fast_window      2      2      3      3
slow_window      3      3      4      4
                p1     p2     p1     p2
2020-01-01   False  False  False  False
2020-01-02   False  False  False  False
2020-01-03    True  False  False  False
2020-01-04    True  False   True  False
2020-01-05    True  False   True  False

.vbt
>>> df1 = pd.DataFrame({'a': [0], 'b': [1]})
>>> df2 = pd.DataFrame({'b': [0], 'a': [1]})
>>> df1 + df2  Pandas connects column a in df1 and df2
   a  b
0  1  1

>>> df1.values + df2.values
array([[0, 2]])

>>> df1.vbt + df2  vectorbt connects the first column in df1 with the first column in df2, regardless of their labels
   a  b
   b  a
0  0  2

a
df1
df2
df1
df2
>>> fast_ma.ma > multi_close  Comparison with Pandas fails
ValueError: Can only compare identically-labeled DataFrame objects

>>> fast_ma.ma.values > multi_close.values  Comparison with NumPy fails
ValueError: operands could not be broadcast together with shapes (5,4) (5,2) 

>>> fast_ma.ma.vbt > multi_close  Comparison with vectorbt succeeds
fast_window      2      2      3      3
                p1     p2     p1     p2
2020-01-01   False  False  False  False
2020-01-02   False   True  False  False
2020-01-03   False   True  False   True
2020-01-04   False   True  False   True
2020-01-05   False   True  False   True

>>> above_lower = multi_close.vbt > vbt.Param([1, 2], name='lower')
>>> below_upper = multi_close.vbt < vbt.Param([3, 4], name='upper')
>>> above_lower.vbt & below_upper
lower           1      1      2      2
upper           3      3      4      4
               p1     p2     p1     p2
2020-01-01   True  False  False  False
2020-01-02  False  False   True  False
2020-01-03  False  False  False   True
2020-01-04  False   True  False  False
2020-01-05  False  False  False  False

>>> a = np.array([1])

>>> vbt.flex_select_1d_nb(a, 0)
1

>>> vbt.flex_select_1d_nb(a, 1)
1

>>> vbt.flex_select_1d_nb(a, 2)
1

>>> full_a = np.broadcast_to(a, (1000,))

>>> full_a[2]
1

>>> a = np.array([[0]])  Same for each element
>>> b = np.array([[1, 2, 3]])  Same for each row
>>> c = np.array([[4], [5], [6]])  Same for each column

>>> vbt.flex_select_nb(a, 2, 1)  Query the element under the third row and second column
0

>>> vbt.flex_select_nb(b, 2, 1)
2

>>> vbt.flex_select_nb(c, 2, 1)
6

CorrStats
np.dtype
namedtuple
eval
>>> import vectorbtpro as vbt
>>> import numpy as np
>>> import pandas as pd
>>> from datetime import datetime, timedelta

>>> dct = {'planet' : {'has': {'plants': 'yes', 'animals': 'yes', 'cryptonite': 'no'}, 'name': 'Earth'}}
>>> print(vbt.prettify(dct))
{
    'planet': {
        'has': {
            'plants': 'yes',
            'animals': 'yes',
            'cryptonite': 'no'
        },
        'name': 'Earth'
    }
}

>>> eval(vbt.prettify(dct)) == dct
True

vbt.prettify
vbt.utils.formatting.prettify
vbt
__all__
dict
>>> print(vbt.Records.field_config)
Config(
    dtype=None,
    settings={
        'id': {
            'name': 'id',
            'title': 'Id'
        },
        'col': {
            'name': 'col',
            'title': 'Column',
            'mapping': 'columns'
        },
        'idx': {
            'name': 'idx',
            'title': 'Timestamp',
            'mapping': 'index'
        }
    }
)

dict
vbt.settings.portfolio.log
vbt.settings['portfolio']['log']
dict
readonly_
>>> cfg = vbt.Config(
...     readonly=False,  Regular argument that comes into dict
...     readonly_=True  Reserved argument that makes our config read-only
... )
>>> print(cfg)
Config(
    readonly=False
)

>>> cfg['change'] = 'something'
TypeError: Config is read-only

__init__
new_instance = ConfiguredClass(**old_instance.config)
_writeable_attrs
>>> class CorrStats(vbt.Configured):
...     def __init__(self, obj1, obj2):
...         vbt.Configured.__init__(self, obj1=obj1, obj2=obj2)
...
...         self._obj1 = obj1
...         self._obj2 = obj2
...
...     @property
...     def obj1(self):
...         return self._obj1
...
...     @property
...     def obj2(self):
...         return self._obj2
...
...     def corr(self):
...         if isinstance(self.obj1, pd.Series):
...             return self.obj1.corr(self.obj2)
...         return self.obj1.corrwith(self.obj2)
...
...     def rolling_corr(self, window):
...         return self.obj1.rolling(window).corr(self.obj2)

CorrStats
>>> index = [datetime(2020, 1, 1) + timedelta(days=i) for i in range(5)]
>>> df1 = pd.DataFrame({
...     'a': [1, 5, 2, 4, 3],
...     'b': [3, 2, 4, 1, 5]
... }, index=index)
>>> df2 = pd.DataFrame({
...     'a': [1, 2, 3, 4, 5],
...     'b': [5, 4, 3, 2, 1]
... }, index=index)

>>> corrstats = CorrStats(df1, df2)
>>> print(corrstats.config)
Config(
    obj1=<DataFrame object at 0x7fd490461400 of shape (5, 2)>,
    obj2=<DataFrame object at 0x7fd490461240 of shape (5, 2)>
)

>>> df3 = pd.DataFrame({
...     'a': [3, 2, 1, 5, 4],
...     'b': [4, 5, 1, 2, 3]
... }, index=index)
>>> corrstats.obj1 = df3
AttributeError: can't set attribute

>>> corrstats.config['obj1'] = df3
TypeError: Config is read-only

>>> corrstats._obj1 = df3  This would work, but at what cost?
>>> corrstats.obj1.iloc[:] = df3  This would work too, unfortunately

>>> new_corrstats = corrstats.replace(obj1=df3)
>>> new_corrstats.obj1  df1 has been replaced with df3
            a  b
2020-01-01  3  4
2020-01-02  2  5
2020-01-03  1  1
2020-01-04  5  2
2020-01-05  4  3

>>> new_corrstats.obj2  df2 remains untouched
            a  b
2020-01-01  1  5
2020-01-02  2  4
2020-01-03  3  3
2020-01-04  4  2
2020-01-05  5  1

df1
df3
df2
>>> corrstats.save('corrstats')  Saves the instance along with both objects

>>> corrstats = CorrStats.load('corrstats')

getattr
>>> sr = pd.Series([1, 2, 3, 4, 5])
>>> attr_chain = [('rolling', (3,)), 'mean', 'min']
>>> vbt.deep_getattr(sr, attr_chain)
2.0

$
sub_id
>>> def some_function(*args, **kwargs):
...     context = {}
...     args = vbt.substitute_templates(args, context=context, strict=False)
...     kwargs = vbt.substitute_templates(kwargs, context=context, strict=False)
...     print(args)
...     print(kwargs)
...     
...     context['result'] = 100  New context
...     args = vbt.substitute_templates(args, context=context)
...     kwargs = vbt.substitute_templates(kwargs, context=context)
...     print(args)
...     print(kwargs)

>>> some_function(vbt.Rep('result'), double_result=vbt.RepEval('result * 2'))
(Rep(template='result', context=None, strict=None, sub_id=None),)
{'double_result': RepEval(template='result * 2', context=None, strict=None, sub_id=None)}
(100,)
{'double_result': 200}

>>> columns = pd.MultiIndex.from_tuples([
...     ('BTC-USD', 'group1'), 
...     ('ETH-USD', 'group1'), 
...     ('ADA-USD', 'group2'),
...     ('SOL-USD', 'group2')
... ], names=['symbol', 'group'])
>>> vbt.Grouper(columns, 'group').get_groups()
array([0, 0, 1, 1])

pd_indexing_func
CorrStats
>>> class CorrStats(vbt.Configured, vbt.PandasIndexer):
...     def __init__(self, obj1, obj2):
...         vbt.Configured.__init__(self, obj1=obj1, obj2=obj2)
...         vbt.PandasIndexer.__init__(self)
...
...         self._obj1 = obj1
...         self._obj2 = obj2
...
...     def indexing_func(self, pd_indexing_func):  Here, pd_indexing_func is just lambda x: x.loc[key]
...         return self.replace(
...             obj1=pd_indexing_func(self.obj1),
...             obj2=pd_indexing_func(self.obj2)
...         )
...
...     @property
...     def obj1(self):
...         return self._obj1
...
...     @property
...     def obj2(self):
...         return self._obj2
...
...     def corr(self):
...         if isinstance(self.obj1, pd.Series):
...             return self.obj1.corr(self.obj2)
...         return self.obj1.corrwith(self.obj2)
...
...     def rolling_corr(self, window):
...         return self.obj1.rolling(window).corr(self.obj2)

>>> corrstats = CorrStats(df1, df2)
>>> corrstats.corr()
a    0.3
b   -0.3
dtype: float64

>>> corrstats.loc['2020-01-01':'2020-01-03', 'a'].corr()  Running the correlation coefficient on a subset of data
0.24019223070763066

pd_indexing_func
lambda x: x.loc[key]
>>> df = pd.DataFrame({
...     'a': range(0, 5),
...     'b': range(5, 10),
...     'c': range(10, 15),
...     'd': range(15, 20)
... }, index=index)
>>> wrapper = vbt.ArrayWrapper.from_obj(df)
>>> print(wrapper)
ArrayWrapper(
    index=<DatetimeIndex object at 0x7ff6d8528d68 of shape (5,)>,
    columns=<Index object at 0x7ff6d857fcc0 of shape (4,)>,
    ndim=2,
    freq=None,
    column_only_select=None,
    group_select=None,
    grouped_ndim=None,
    grouper=Grouper(
        index=<Index object at 0x7ff6d857fcc0 of shape (4,)>,
        group_by=None,
        allow_enable=True,
        allow_disable=True,
        allow_modify=True
    )
)

>>> def sum_per_column(df):
...     wrapper = vbt.ArrayWrapper.from_obj(df)
...     result = np.sum(df.values, axis=0)
...     return wrapper.wrap_reduced(result)

>>> sum_per_column(df)
a    10
b    35
c    60
d    85
dtype: int64

>>> big_df = pd.DataFrame(np.random.uniform(size=(1000, 1000)))

>>> %timeit big_df.sum()
2.52 ms ± 3.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

>>> %timeit sum_per_column(big_df)
428 µs ± 1.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

>>> def sum_per_group(df, group_by):
...     wrapper = vbt.ArrayWrapper.from_obj(df, group_by=group_by)
...     results = []
...     for group_idxs in wrapper.grouper.yield_group_idxs():
...         group_result = np.sum(df.values[:, group_idxs])
...         results.append(group_result)
...     return wrapper.wrap_reduced(results)

>>> sum_per_group(df, False)  No grouping (one group per column)
a    10
b    35
c    60
d    85
dtype: int64

>>> sum_per_group(df, True)  One group with all columns
190

>>> group_by = pd.Index(['group1', 'group1', 'group2', 'group2'])
>>> sum_per_group(df, group_by)  Multiple groups with multiple columns
group1     45
group2    145
dtype: int64

CorrStats
CorrStats
>>> class CorrStats(vbt.Wrapping):
...     _expected_keys = vbt.Wrapping._expected_keys | {"obj1", "obj2"}  We need to specify the argument names that we are about to pass to Wrapping (or set _expected_keys=None to disable)
...
...     @classmethod
...     def from_objs(cls, obj1, obj2):  Convenient class method to broadcast objects, create a wrapper, and pass everything to the constructor
...         (obj1, obj2), wrapper = vbt.broadcast(
...             obj1, obj2, 
...             to_pd=False,
...             return_wrapper=True
...         )
...         return cls(wrapper, obj1, obj2)
...
...     def __init__(self, wrapper, obj1, obj2):
...         vbt.Wrapping.__init__(self, wrapper, obj1=obj1, obj2=obj2)
...
...         self._obj1 = vbt.to_2d_array(obj1)
...         self._obj2 = vbt.to_2d_array(obj2)
...
...     def indexing_func(self, pd_indexing_func, **kwargs):  Indexing method that uses the wrapper to translate pd_indexing_func to an array of selected rows and columns, applies them on both NumPy arrays, and creates a new CorrStats instance
...         wrapper_meta = self.wrapper.indexing_func_meta(pd_indexing_func, **kwargs)
...         new_wrapper = wrapper_meta["new_wrapper"]
...         row_idxs = wrapper_meta["row_idxs"]
...         col_idxs = wrapper_meta["col_idxs"]
...         return self.replace(
...             wrapper=new_wrapper,
...             obj1=self.obj1[row_idxs, :][:, col_idxs],
...             obj2=self.obj2[row_idxs, :][:, col_idxs]
...         )
...
...     @property
...     def obj1(self):
...         return self._obj1
...
...     @property
...     def obj2(self):
...         return self._obj2
...
...     def corr(self):  Computation is done on NumPy objects and the result is converted into Pandas
...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)
...         return self.wrapper.wrap_reduced(out)
...
...     def rolling_corr(self, window):
...         out = vbt.nb.rolling_corr_nb(
...             self.obj1, self.obj2, 
...             window, minp=window)
...         return self.wrapper.wrap(out)

_expected_keys=None
pd_indexing_func
CorrStats
CorrStats
wrapper
obj1
obj2
CorrStats.corr
CorrStats.rolling_corr
CorrStats.from_objs
CorrStats
CorrStats
df1
df2
df1
df2
>>> df1.corrwith(df2)  Using Pandas
a    0.3
b   -0.3
dtype: float64

>>> corrstats = CorrStats.from_objs(df1, df2)
>>> corrstats.corr()  Using our class
a    0.3
b   -0.3
dtype: float64

>>> df2_sh = vbt.pd_acc.concat(
...     df2, df2.vbt.shuffle(seed=42), 
...     keys=['plain', 'shuffled'])
>>> df2_sh 
              plain    shuffled   
               a  b        a  b
2020-01-01     1  5        2  2
2020-01-02     2  4        5  4
2020-01-03     3  3        3  3
2020-01-04     4  2        1  5
2020-01-05     5  1        4  1

>>> df1.corrwith(df2_sh)  Pandas doesn't know how to join this information
ValueError: cannot join with no overlapping index names

>>> corrstats = CorrStats.from_objs(df1, df2_sh)
>>> corrstats.corr()  vectorbt knows exactly what to do thanks to smart broadcasting
plain     a    0.3
          b   -0.3
shuffled  a    0.4
          b   -0.9
dtype: float64

>>> big_df1 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))
>>> big_df2 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))

>>> %timeit big_df1.rolling(10).corr(big_df2)  Using Pandas
271 ms ± 13.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> corrstats = CorrStats.from_objs(big_df1, big_df2)
>>> %timeit corrstats.rolling_corr(10)  Using our class
12 ms ± 50.5 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

obj1
obj2
pd_indexing_func
>>> corrstats = CorrStats.from_objs(df1, df2_sh)
>>> corrstats.loc['2020-01-02':'2020-01-05'].rolling_corr(3)  Compute the rolling correlation coefficient only between 2020-01-02 and 2020-01-05
                         plain            shuffled
                   a         b         a         b
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03       NaN       NaN       NaN       NaN
2020-01-04 -0.327327  0.327327  0.327327 -0.981981
2020-01-05  0.500000 -0.240192 -0.654654 -0.960769

column_only_select
wrapper
wrapper.ndim
from_
__init__
vbt.MA.run()
vbt.MA()
CorrStats
>>> df.vbt
<vectorbtpro.accessors.Vbt_DFAccessor at 0x7fe3c19f7d68>

>>> df.vbt.to_2d_array()
array([[ 0,  5, 10, 15],
       [ 1,  6, 11, 16],
       [ 2,  7, 12, 17],
       [ 3,  8, 13, 18],
       [ 4,  9, 14, 19]])

combine_func
>>> pd.Series([1, 2, 3]).vbt.combine(np.array([[4, 5, 6]]), np.add)
   0  1  2
0  5  6  7
1  6  7  8
2  7  8  9

BaseAccessor.__add__
>>> pd.Series([1, 2, 3]) + np.array([[4, 5, 6]])  Without .vbt, the addition is done by Pandas
ValueError: Length of values (1) does not match length of index (3)

>>> pd.Series([1, 2, 3]).vbt + np.array([[4, 5, 6]])  With .vbt, the addition is done by vectorbt
   0  1  2
0  5  6  7
1  6  7  8
2  7  8  9

.vbt
.vbt
CorrStats
CorrStats
>>> class CorrStats(vbt.Analyzable):  Replace Wrapping with Analyzable
...     _expected_keys = vbt.Analyzable._expected_keys | {"obj1", "obj2"}
...
...     @classmethod
...     def from_objs(cls, obj1, obj2):
...         (obj1, obj2), wrapper = vbt.broadcast(
...             obj1, obj2, 
...             to_pd=False,
...             return_wrapper=True
...         )
...         return cls(wrapper, obj1, obj2)
...
...     def __init__(self, wrapper, obj1, obj2):
...         vbt.Analyzable.__init__(self, wrapper, obj1=obj1, obj2=obj2)  Replace Wrapping with Analyzable
...
...         self._obj1 = vbt.to_2d_array(obj1)
...         self._obj2 = vbt.to_2d_array(obj2)
...
...     def indexing_func(self, pd_indexing_func, **kwargs):
...         wrapper_meta = self.wrapper.indexing_func_meta(pd_indexing_func, **kwargs)
...         new_wrapper = wrapper_meta["new_wrapper"]
...         row_idxs = wrapper_meta["row_idxs"]
...         col_idxs = wrapper_meta["col_idxs"]
...         return self.replace(
...             wrapper=new_wrapper,
...             obj1=self.obj1[row_idxs, :][:, col_idxs],
...             obj2=self.obj2[row_idxs, :][:, col_idxs]
...         )
...
...     @property
...     def obj1(self):
...         return self._obj1
...
...     @property
...     def obj2(self):
...         return self._obj2
...
...     def corr(self):
...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)
...         return self.wrapper.wrap_reduced(out)
...
...     def rolling_corr(self, window):
...         out = vbt.nb.rolling_corr_nb(
...             self.obj1, self.obj2, 
...             window, minp=window)
...         return self.wrapper.wrap(out)
...
...     _metrics = vbt.HybridConfig(  Define default metrics for StatsBuilderMixin.stats
...         corr=dict(
...             title='Corr. Coefficient',
...             calc_func='corr'
...         )
...     )
...
...     _subplots = vbt.HybridConfig(  Define default subplots for PlotsBuilderMixin.plots
...          rolling_corr=dict(
...              title=vbt.Sub("Rolling Corr. Coefficient (window=$window)"),
...              plot_func=vbt.Sub('rolling_corr($window).vbt.plot'),  Recall Templating and Attribute resolution?
...              pass_trace_names=False
...          )
...     )

Wrapping
Analyzable
Wrapping
Analyzable
Wrapping
Analyzable
CorrStats.corr
CorrStats.rolling_corr
CorrStats.from_objs
CorrStats.stats
CorrStats.plots
>>> corrstats = CorrStats.from_objs(df1, df2)
>>> corrstats.stats(column='a')  Compute metrics for all columns and display only a
Corr. Coefficient    0.3
Name: a, dtype: object

>>> corrstats['a'].stats()  Compute and display metrics only for a
Corr. Coefficient    0.3
Name: a, dtype: object

>>> corrstats.plots(template_context=dict(window=3)).show()  Set the rolling window to 3 and display subplots for all columns

a
a
CorrStats
>>> df.vbt.stats(column='a')
Start        2020-01-01 00:00:00
End          2020-01-05 00:00:00
Period           5 days 00:00:00
Count                          5
Mean                         2.0
Std                     1.581139
Min                            0
Median                       2.0
Max                            4
Min Index    2020-01-01 00:00:00
Max Index    2020-01-05 00:00:00
Name: a, dtype: object

>>> dd_df = pd.DataFrame({
...     'a': [10, 11, 12, 11, 12, 13, 12],
...     'b': [14, 13, 12, 11, 12, 13, 14]
... }, index=[datetime(2020, 1, 1) + timedelta(days=i) for i in range(7)])
>>> drawdowns = dd_df.vbt.drawdowns
>>> drawdowns.records_readable
   Drawdown Id Column Peak Timestamp Start Timestamp Valley Timestamp  \\
0            0      a     2020-01-03      2020-01-04       2020-01-04   
1            1      a     2020-01-06      2020-01-07       2020-01-07   
2            0      b     2020-01-01      2020-01-02       2020-01-04   

  End Timestamp  Peak Value  Valley Value  End Value     Status  
0    2020-01-05        12.0          11.0       12.0  Recovered  
1    2020-01-07        13.0          12.0       12.0     Active  
2    2020-01-07        14.0          11.0       14.0  Recovered 

>>> drawdowns['b'].records_readable  Select all records of the column b and display them in a human-readable format
   Drawdown Id Column Peak Timestamp Start Timestamp Valley Timestamp  \\
0            0      b     2020-01-01      2020-01-02       2020-01-04   

  End Timestamp  Peak Value  Valley Value  End Value     Status  
0    2020-01-07        14.0          11.0       14.0  Recovered

b
>>> drawdowns.status.values  Raw data
array([1, 0, 1])

>>> drawdowns.get_apply_mapping_arr('status')  Data enhanced using the field config
array(['Recovered', 'Active', 'Recovered'], dtype=object)

>>> drawdowns.col_mapper.col_map  An array with the indices of records ordered by column, and an array with the number of records per column
(array([0, 1, 2]), array([2, 1]))

a
b
drawdowns
>>> dd_ma = drawdowns.drawdown
>>> dd_ma
<vectorbtpro.records.mapped_array.MappedArray at 0x7ff6d8514f98>

>>> dd_ma.values  Holds three values: two from a and one from b
array([-0.08333333, -0.07692308, -0.21428571])

>>> dd_ma.stats(column='a')
Start        2020-01-01 00:00:00
End          2020-01-07 00:00:00
Period           7 days 00:00:00
Count                          2
Mean                   -0.080128
Std                     0.004533
Min                    -0.083333
Median                 -0.080128
Max                    -0.076923
Min Index    2020-01-05 00:00:00
Max Index    2020-01-07 00:00:00
Name: a, dtype: object

>>> dd_ma.to_pd()  Can be converted into Pandas, but at what cost?
                   a         b
2020-01-01       NaN       NaN
2020-01-02       NaN       NaN
2020-01-03       NaN       NaN
2020-01-04       NaN       NaN
2020-01-05 -0.083333       NaN
2020-01-06       NaN       NaN
2020-01-07 -0.076923 -0.214286

a
b
>>> dd_ma['b'].values
array([-0.21428571])

CorrStats
>>> import vectorbtpro as vbt
>>> import numpy as np
>>> import pandas as pd

>>> data = vbt.BinanceData.pull('BTCUSDT')
>>> data
<vectorbtpro.data.custom.binance.BinanceData at 0x7f9c40c59550>

>>> data.plot().show()  To plot a specific symbol and/or feature, pass it as symbol and feature respectively

symbol
feature
>>> data.data['BTCUSDT'].info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 1616 entries, 2017-08-17 00:00:00+00:00 to 2022-01-18 00:00:00+00:00
Freq: D
Data columns (total 10 columns):
 #   Column              Non-Null Count  Dtype              
---  ------              --------------  -----              
 0   Open                1616 non-null   float64            
 1   High                1616 non-null   float64            
 2   Low                 1616 non-null   float64            
 3   Close               1616 non-null   float64            
 4   Volume              1616 non-null   float64            
 5   Close time          1616 non-null   datetime64[ns, UTC]
 6   Quote volume        1616 non-null   float64            
 7   Number of trades    1616 non-null   int64              
 8   Taker base volume   1616 non-null   float64            
 9   Taker quote volume  1616 non-null   float64            
dtypes: datetime64[ns, UTC

>>> open_price = data.get('Open')
>>> close_price = data.get('Close')  OHLCV can be also accessed directly, like data.close

data.close
>>> vbt.IF.list_indicators("RSI*")
['vbt:RSI', 'talib:RSI', 'pandas_ta:RSI', 'ta:RSIIndicator', 'technical:RSI']

>>> vbt.indicator("talib:RSI")
vectorbtpro.indicators.factory.talib.RSI

>>> vbt.RSI  To list all vectorbt indicators, check out the module custom
vectorbtpro.indicators.custom.RSI

>>> vbt.talib('RSI')  To list all supported TA-Lib indicators, call IndicatorFactory.list_talib_indicators
vectorbtpro.indicators.factory.talib.RSI

>>> vbt.ta('RSIIndicator')  To list all supported TA indicators, call IndicatorFactory.list_ta_indicators
vectorbtpro.indicators.factory.ta.RSIIndicator

>>> vbt.pandas_ta('RSI')  To list all supported Pandas TA indicators, call IndicatorFactory.list_pandas_ta_indicators
vectorbtpro.indicators.factory.pandas_ta.RSI

>>> vbt.technical('RSI')  To list all supported technical indicators, call IndicatorFactory.list_technical_indicators
vectorbtpro.indicators.factory.technical.RSI

run
>>> vbt.phelp(vbt.RSI.run)
RSI.run(
    close,
    window=Default(value=14),
    wtype=Default(value='wilder'),
    short_name='rsi',
    hide_params=None,
    hide_default=True,
    **kwargs
):
    Run `RSI` indicator.

    * Inputs: `close`
    * Parameters: `window`, `wtype`
    * Outputs: `rsi`

    Pass a list of parameter names as `hide_params` to hide their column levels.
    Set `hide_default` to False to show the column levels of the parameters with a default value.

    Other keyword arguments are passed to `RSI.run_pipeline`.

close
open_price
close
>>> rsi = vbt.RSI.run(open_price)
>>> rsi
<vectorbtpro.indicators.custom.RSI at 0x7f9c20921ac8>

rsi
phelp
>>> rsi.rsi
Open time
2017-08-17 00:00:00+00:00          NaN
2017-08-18 00:00:00+00:00          NaN
2017-08-19 00:00:00+00:00          NaN
2017-08-20 00:00:00+00:00          NaN
2017-08-21 00:00:00+00:00          NaN
...                                ...
2022-07-30 00:00:00+00:00    60.541637
2022-07-31 00:00:00+00:00    59.503179
2022-08-01 00:00:00+00:00    56.750576
2022-08-02 00:00:00+00:00    56.512434
2022-08-03 00:00:00+00:00    54.177385
Freq: D, Name: Open, Length: 1813, dtype: float64

>>> entries = rsi.rsi.vbt.crossed_below(30)  Using GenericAccessor.crossed_below
>>> entries
Open time
2017-08-17 00:00:00+00:00    False
2017-08-18 00:00:00+00:00    False
2017-08-19 00:00:00+00:00    False
2017-08-20 00:00:00+00:00    False
2017-08-21 00:00:00+00:00    False
...                            ...
2022-07-30 00:00:00+00:00    False
2022-07-31 00:00:00+00:00    False
2022-08-01 00:00:00+00:00    False
2022-08-02 00:00:00+00:00    False
2022-08-03 00:00:00+00:00    False
Freq: D, Name: Open, Length: 1813, dtype: bool

>>> exits = rsi.rsi.vbt.crossed_above(70)  Using GenericAccessor.crossed_above
>>> exits
Open time
2017-08-17 00:00:00+00:00    False
2017-08-18 00:00:00+00:00    False
2017-08-19 00:00:00+00:00    False
2017-08-20 00:00:00+00:00    False
2017-08-21 00:00:00+00:00    False
...                            ...
2022-07-30 00:00:00+00:00    False
2022-07-31 00:00:00+00:00    False
2022-08-01 00:00:00+00:00    False
2022-08-02 00:00:00+00:00    False
2022-08-03 00:00:00+00:00    False
Freq: D, Name: Open, Length: 1813, dtype: bool

rsi
>>> entries = rsi.rsi_crossed_below(30)
>>> exits = rsi.rsi_crossed_above(70)

dir(rsi)
>>> def plot_rsi(rsi, entries, exits):
...     fig = rsi.plot()  Using RSI.plot
...     entries.vbt.signals.plot_as_entries(rsi.rsi, fig=fig)  Using SignalsSRAccessor.plot_as_entries
...     exits.vbt.signals.plot_as_exits(rsi.rsi, fig=fig)  Using SignalsSRAccessor.plot_as_exits
...     return fig

>>> plot_rsi(rsi, entries, exits).show()

>>> clean_entries, clean_exits = entries.vbt.signals.clean(exits)  Using SignalsAccessor.clean

>>> plot_rsi(rsi, clean_entries, clean_exits).show()

clean_entries
clean_exits
>>> clean_entries.vbt.signals.total()  Get the total number of entry signals
8

>>> clean_exits.vbt.signals.total()  Get the total number of exit signals
7

>>> ranges = clean_entries.vbt.signals.between_ranges(other=clean_exits)  Get range records of type Ranges between each entry and exit
>>> ranges.duration.mean(wrap_kwargs=dict(to_timedelta=True))  Get the average duration between each entry and exit
Timedelta('86 days 10:17:08.571428572')

>>> pf = vbt.Portfolio.from_signals(
...     close=close_price, 
...     entries=clean_entries, 
...     exits=clean_exits,
...     size=100,
...     size_type='value',
...     init_cash='auto'
... )
>>> pf
<vectorbtpro.portfolio.base.Portfolio at 0x7f9c40eea438>

None
Portfolio
>>> pf.stats()
Start                         2017-08-17 00:00:00+00:00
End                           2022-08-03 00:00:00+00:00
Period                               1813 days 00:00:00
Start Value                                       100.0
Min Value                                     97.185676
Max Value                                    203.182943
End Value                                    171.335425
Total Return [%]                              71.335425
Benchmark Return [%]                         446.481746
Total Time Exposure [%]                       38.113624
Max Gross Exposure [%]                            100.0
Max Drawdown [%]                              46.385941
Max Drawdown Duration                1613 days 00:00:00
Total Orders                                         15
Total Fees Paid                                     0.0
Total Trades                                          8
Win Rate [%]                                  71.428571
Best Trade [%]                                54.519055
Worst Trade [%]                              -32.078597
Avg Winning Trade [%]                         26.905709
Avg Losing Trade [%]                         -19.345383
Avg Winning Trade Duration             87 days 09:36:00
Avg Losing Trade Duration              84 days 00:00:00
Profit Factor                                  3.477019
Expectancy                                    13.691111
Sharpe Ratio                                   0.505486
Calmar Ratio                                   0.246836
Omega Ratio                                    1.132505
Sortino Ratio                                  0.796701
dtype: object

pf.metrics
calc_func
>>> pf.plot(settings=dict(bm_returns=False)).show()

lower_th
upper_th
window
ewm
>>> def test_rsi(window=14, wtype="wilder", lower_th=30, upper_th=70):
...     rsi = vbt.RSI.run(open_price, window=window, wtype=wtype)
...     entries = rsi.rsi_crossed_below(lower_th)
...     exits = rsi.rsi_crossed_above(upper_th)
...     pf = vbt.Portfolio.from_signals(
...         close=close_price, 
...         entries=entries, 
...         exits=exits,
...         size=100,
...         size_type='value',
...         init_cash='auto')
...     return pf.stats([
...         'total_return', 
...         'total_trades', 
...         'win_rate', 
...         'expectancy'
...     ])

>>> test_rsi()
Total Return [%]    71.335425
Total Trades                8
Win Rate [%]        71.428571
Expectancy          13.691111
dtype: object

>>> test_rsi(lower_th=20, upper_th=80)
Total Return [%]    6.652287
Total Trades               2
Win Rate [%]            50.0
Expectancy          3.737274
dtype: object

>>> from itertools import product

>>> lower_ths = range(20, 31)  20, 21, ..., 30
>>> upper_ths = range(70, 81)  70, 71, ..., 80
>>> th_combs = list(product(lower_ths, upper_ths))  Generate all possible combinations between lower_ths and upper_ths, also called as a Cartesian product
>>> len(th_combs)
121

>>> comb_stats = [
...     test_rsi(lower_th=lower_th, upper_th=upper_th)
...     for lower_th, upper_th in th_combs
... ]  Iterate over each combination and compute its statistics using a list comprehension. This creates a list of Series.

lower_ths
upper_ths
>>> comb_stats_df = pd.DataFrame(comb_stats)
>>> comb_stats_df
     Total Return [%]  Total Trades  Win Rate [%]  Expectancy
0           24.369550             3     66.666667   10.606342
1           37.380341             3     66.666667   16.203667
2           34.560194             3     66.666667   14.981187
3           31.090080             3     66.666667   13.833710
4           31.090080             3     66.666667   13.833710
..                ...           ...           ...         ...
116         51.074571             6     80.000000   18.978193
117         62.853840             6     80.000000   21.334047
118         40.685579             5     75.000000   21.125494
119         -5.990835             4     66.666667   13.119897
120        -10.315159             4     66.666667   11.678455

[121 rows x 4 columns]

lower_th
upper_th
comb_stats_df
>>> comb_stats_df.index = pd.MultiIndex.from_tuples(
...     th_combs, 
...     names=['lower_th', 'upper_th'])
>>> comb_stats_df
                   Total Return [%]  Total Trades  Win Rate [%]  Expectancy
lower_th upper_th                                                          
20       70               24.369550             3     66.666667   10.606342
         71               37.380341             3     66.666667   16.203667
         72               34.560194             3     66.666667   14.981187
         73               31.090080             3     66.666667   13.833710
         74               31.090080             3     66.666667   13.833710
...                             ...           ...           ...         ...
30       76               51.074571             6     80.000000   18.978193
         77               62.853840             6     80.000000   21.334047
         78               40.685579             5     75.000000   21.125494
         79               -5.990835             4     66.666667   13.119897
         80              -10.315159             4     66.666667   11.678455

[121 rows x 4 columns]

>>> comb_stats_df['Expectancy'].vbt.heatmap().show()

>>> windows = list(range(8, 21))
>>> wtypes = ["simple", "exp", "wilder"]
>>> lower_ths = list(range(20, 31))
>>> upper_ths = list(range(70, 81))

itertools.product
param_product=True
windows
wtypes
open_price
>>> rsi = vbt.RSI.run(
...     open_price, 
...     window=windows, 
...     wtype=wtypes, 
...     param_product=True)
>>> rsi.rsi.columns
MultiIndex([( 8, 'simple'),
            ( 8,    'exp'),
            ( 8, 'wilder'),
            ...
            (20, 'simple'),
            (20,    'exp'),
            (20, 'wilder')],
           names=['rsi_window', 'rsi_wtype'])

rsi_window
rsi_wtype
len(open_price.columns)
len(windows)
len(wtypes)
rsi
lower_ths
upper_th_index
rsi
rsi_crossed_below
rsi_crossed_above
rsi
>>> lower_ths_prod, upper_ths_prod = zip(*product(lower_ths, upper_ths))
>>> len(lower_ths_prod)  The first value in lower_ths_prod builds a combination with the first value in upper_ths_prod, the second with the second, and so on - 121 combinations in total.
121
>>> len(upper_ths_prod)
121

>>> lower_th_index = vbt.Param(lower_ths_prod, name='lower_th')  Convert thresholds to vbt.Param to instruct broadcast that we want to build a product with the columns in rsi
>>> entries = rsi.rsi_crossed_below(lower_th_index)
>>> entries.columns
MultiIndex([(20,  8, 'simple'),
            (20,  8,    'exp'),
            (20,  8, 'wilder'),
            ...
            (30, 20, 'simple'),
            (30, 20,    'exp'),
            (30, 20, 'wilder')],
           names=['lower_th', 'rsi_window', 'rsi_wtype'], length=4719)

>>> upper_th_index = vbt.Param(upper_ths_prod, name='upper_th')
>>> exits = rsi.rsi_crossed_above(upper_th_index)
>>> exits.columns
MultiIndex([(70,  8, 'simple'),
            (70,  8,    'exp'),
            (70,  8, 'wilder'),
            ...
            (80, 20, 'simple'),
            (80, 20,    'exp'),
            (80, 20, 'wilder')],
           names=['upper_th', 'rsi_window', 'rsi_wtype'], length=4719)

lower_ths_prod
upper_ths_prod
vbt.Param
rsi
entries
exits
lower_th
upper_th
close_price
>>> pf = vbt.Portfolio.from_signals(
...     close=close_price, 
...     entries=entries, 
...     exits=exits,
...     size=100,
...     size_type='value',
...     init_cash='auto'
... )
>>> pf
<vectorbtpro.portfolio.base.Portfolio at 0x7f9c415ed5c0>

>>> stats_df = pf.stats([
...     'total_return', 
...     'total_trades', 
...     'win_rate', 
...     'expectancy'
... ], agg_func=None)  By default, StatsBuilderMixin.stats takes the mean out of all columns and returns a Series. We, on the other hand, want to disable the aggregation function and stack all Series into one big DataFrame.
>>> stats_df
                                        Total Return [%]  Total Trades  \\
lower_th upper_th rsi_window rsi_wtype                                   
20       70       8          simple           -25.285842            31   
                             exp               -7.939736            29   
                             wilder            61.979801            11   
...                                                  ...           ...   
                  20         simple           -59.159157             4   
                             exp               -3.331163             8   
                             wilder            31.479482             3   

                                        Win Rate [%]  Expectancy  
lower_th upper_th rsi_window rsi_wtype                            
20       70       8          simple        51.612903   -1.224523  
                             exp           58.620690   -0.307862  
                             wilder        72.727273    5.634527  
...                                              ...         ...  
                  20         simple        33.333333  -16.159733  
                             exp           57.142857    7.032204  
                             wilder        50.000000   38.861607  

[4719 rows x 4 columns]

>>> print(pf.getsize())
9.4 MB

>>> np.product(pf.wrapper.shape) * 8 / 1024 / 1024
65.27364349365234

rsi_window
>>> stats_df['Expectancy'].groupby('rsi_window').mean()
rsi_window
8      0.154425
9      0.064130
10    -0.915478
11    -0.523294
12     0.742266
13     3.898482
14     4.414367
15     6.916872
16     8.915225
17    12.204188
18    12.897135
19    14.508950
20    16.429515
Name: Expectancy, dtype: float64

>>> stats_df.sort_values(by='Expectancy', ascending=False).head()
                                        Total Return [%]  Total Trades  \\
lower_th upper_th rsi_window rsi_wtype                                   
22       80       20         wilder           187.478208             2   
21       80       20         wilder           187.478208             2   
26       80       20         wilder           152.087039             3   
23       80       20         wilder           187.478208             2   
25       80       20         wilder           201.297495             3   

                                        Win Rate [%]  Expectancy  
lower_th upper_th rsi_window rsi_wtype                            
22       80       20         wilder            100.0   93.739104  
21       80       20         wilder            100.0   93.739104  
26       80       20         wilder            100.0   93.739104  
23       80       20         wilder            100.0   93.739104  
25       80       20         wilder            100.0   93.739104  

>>> pf[(22, 80, 20, "wilder")].plot_value().show()

column
pf.plot_value(column=(22, 80, 20, "wilder"))
open_price
close_price
>>> data = vbt.BinanceData.pull(['BTCUSDT', 'ETHUSDT'])

MultiIndex([(20, 70,  8, 'simple', 'BTCUSDT'),
            (20, 70,  8, 'simple', 'ETHUSDT'),
            (20, 70,  8,    'exp', 'BTCUSDT'),
            ...
            (30, 80, 20,    'exp', 'ETHUSDT'),
            (30, 80, 20, 'wilder', 'BTCUSDT'),
            (30, 80, 20, 'wilder', 'ETHUSDT')],
           names=['lower_th', 'upper_th', 'rsi_window', 'rsi_wtype', 'symbol'], length=9438)

symbol
>>> eth_mask = stats_df.index.get_level_values('symbol') == 'ETHUSDT'
>>> btc_mask = stats_df.index.get_level_values('symbol') == 'BTCUSDT'
>>> pd.DataFrame({
...     'ETHUSDT': stats_df[eth_mask]['Expectancy'].values,
...     'BTCUSDT': stats_df[btc_mask]['Expectancy'].values
... }).vbt.histplot(xaxis=dict(title="Expectancy")).show()  Using GenericAccessor.histplot

>>> import vectorbtpro as vbt
>>> import numpy as np
>>> import pandas as pd

>>> def mov_avg_crossover(ts1, ts2, w1, w2):
...     ts1, ts2 = vbt.broadcast(ts1, ts2)  Both time series become Pandas objects with the same shape - (7, 2)
...
...     w1, w2 = vbt.broadcast(  Window w1 becomes [2, 2] and window w2 becomes [3, 4]. This builds two parameter combinations: (2, 3) and (2, 4).
...         vbt.to_1d_array(w1), 
...         vbt.to_1d_array(w2))
...
...     ts1_mas = []
...     for w in w1:
...         ts1_mas.append(ts1.vbt.rolling_mean(w) / ts1)  Calculate the normalized moving average based on each of the windows
...     ts2_mas = []
...     for w in w2:
...         ts2_mas.append(ts2.vbt.rolling_mean(w) / ts2)
...
...     ts1_ma = pd.concat(ts1_mas, axis=1)  Concatenate the DataFrames along the column axis
...     ts2_ma = pd.concat(ts2_mas, axis=1)
...
...     ts1_ma.columns = vbt.combine_indexes((  Create a new column hierarchy with the window values on top
...         pd.Index(w1, name="ts1_window"), 
...         ts1.columns))
...     ts2_ma.columns = vbt.combine_indexes((
...         pd.Index(w2, name="ts2_window"), 
...         ts2.columns))
...
...     return ts1_ma.vbt - ts2_ma  Calculate the difference of both concatenated DataFrames

>>> def generate_index(n):  Convenient function to generate a datetime-like index
...     return pd.date_range("2020-01-01", periods=n)

>>> ts1 = pd.Series([1, 2, 3, 4, 5, 6, 7], index=generate_index(7))
>>> ts2 = pd.DataFrame({
...     'a': [5, 4, 3, 2, 3, 4, 5],
...     'b': [2, 3, 4, 5, 4, 3, 2]
... }, index=generate_index(7))
>>> w1 = 2
>>> w2 = [3, 4]

>>> mov_avg_crossover(ts1, ts2, w1, w2)
ts1_window                                       2
ts2_window                   3                   4
                   a         b         a         b
2020-01-01       NaN       NaN       NaN       NaN
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03 -0.500000  0.083333       NaN       NaN
2020-01-04 -0.625000  0.075000 -0.875000  0.175000
2020-01-05  0.011111 -0.183333 -0.100000 -0.100000
2020-01-06  0.166667 -0.416667  0.166667 -0.416667
2020-01-07  0.128571 -0.571429  0.228571 -0.821429

(7, 2)
w1
[2, 2]
w2
[3, 4]
(2, 3)
(2, 4)
ts1
ts2
custom_func
>>> def custom_func(ts1, ts2, w1, w2):
...     ts1_mas = []
...     for w in w1:
...         ts1_mas.append(vbt.nb.rolling_mean_nb(ts1, w) / ts1)  Using rolling_mean_nb
...     ts2_mas = []
...     for w in w2:
...         ts2_mas.append(vbt.nb.rolling_mean_nb(ts2, w) / ts2)
...
...     ts1_ma = np.column_stack(ts1_mas)  Concatenate the NumPy arrays along the column axis
...     ts2_ma = np.column_stack(ts2_mas)
...
...     return ts1_ma - ts2_ma  Calculate the difference of both concatenated NumPy arrays

>>> outputs = vbt.IndicatorBase.run_pipeline(
...     num_ret_outputs=1,
...     custom_func=custom_func,
...     inputs=dict(ts1=ts1, ts2=ts2),
...     params=dict(w1=w1, w2=w2)
... )
>>> outputs
(<vectorbtpro.base.wrapping.ArrayWrapper at 0x7fb188993160>,
 [array([[1, 1],
         [2, 2],
         [3, 3],
         [4, 4],
         [5, 5],
         [6, 6],
         [7, 7]]),
  array([[5, 2],
         [4, 3],
         [3, 4],
         [2, 5],
         [3, 4],
         [4, 3],
         [5, 2]])],
 array([0, 1, 0, 1]),
 [],
 [array([[        nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan],
         [-0.5       ,  0.08333333,         nan,         nan],
         [-0.625     ,  0.075     , -0.875     ,  0.175     ],
         [ 0.01111111, -0.18333333, -0.1       , -0.1       ],
         [ 0.16666667, -0.41666667,  0.16666667, -0.41666667],
         [ 0.12857143, -0.57142857,  0.22857143, -0.82142857]])],
 [[2, 2], [3, 4]],
 [Int64Index([2, 2, 2, 2], dtype='int64'),
  Int64Index([3, 3, 4, 4], dtype='int64')],
 [])

custom_func
>>> MADiff = vbt.IF(
...     class_name='MADiff',
...     input_names=['ts1', 'ts2'],
...     param_names=['w1', 'w2'],
...     output_names=['diff'],
... ).with_custom_func(custom_func)

>>> madiff = MADiff.run(ts1, ts2, w1, w2)
>>> madiff.diff
madiff_w1                                        2
madiff_w2                    3                   4
                   a         b         a         b
2020-01-01       NaN       NaN       NaN       NaN
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03 -0.500000  0.083333       NaN       NaN
2020-01-04 -0.625000  0.075000 -0.875000  0.175000
2020-01-05  0.011111 -0.183333 -0.100000 -0.100000
2020-01-06  0.166667 -0.416667  0.166667 -0.416667
2020-01-07  0.128571 -0.571429  0.228571 -0.821429

vbt.IF
MADiff.run
custom_func
run
MADiff
crossed_above
cross_below
stats
>>> madiff.diff_stats(column=(2, 3, 'a'))
Start        2020-01-01 00:00:00
End          2020-01-07 00:00:00
Period           7 days 00:00:00
Count                          5
Mean                    -0.16373
Std                     0.371153
Min                       -0.625
Median                  0.011111
Max                     0.166667
Min Index    2020-01-04 00:00:00
Max Index    2020-01-06 00:00:00
Name: (2, 3, a), dtype: object

run
input_names
param_names
>>> MADiff_factory = vbt.IF(
...     class_name='MADiff',
...     input_names=['ts1', 'ts2'],
...     param_names=['w1', 'w2'],
...     output_names=['diff'],
... )
>>> MADiff_factory.Indicator
vectorbtpro.indicators.factory.MADiff

>>> MADiff_factory.Indicator.run()
NotImplementedError: 

with_
run
>>> MADiff = MADiff_factory.with_custom_func(custom_func)
>>> MADiff
vectorbtpro.indicators.factory.MADiff

with_
vbt.IF(...)
MADiff
from_
pass_wrapper=True
with_custom_func
custom_func
>>> def apply_func(ts1, ts2, w1, w2):
...     ts1_ma = vbt.nb.rolling_mean_nb(ts1, w1) / ts1
...     ts2_ma = vbt.nb.rolling_mean_nb(ts2, w2) / ts2
...     return ts1_ma - ts2_ma

>>> MADiff = vbt.IF(
...     class_name='MADiff',
...     input_names=['ts1', 'ts2'],
...     param_names=['w1', 'w2'],
...     output_names=['diff'],
... ).with_apply_func(apply_func)

>>> madiff = MADiff.run(ts1, ts2, w1, w2)
>>> madiff.diff
madiff_w1                                        2
madiff_w2                    3                   4
                   a         b         a         b
2020-01-01       NaN       NaN       NaN       NaN
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03 -0.500000  0.083333       NaN       NaN
2020-01-04 -0.625000  0.075000 -0.875000  0.175000
2020-01-05  0.011111 -0.183333 -0.100000 -0.100000
2020-01-06  0.166667 -0.416667  0.166667 -0.416667
2020-01-07  0.128571 -0.571429  0.228571 -0.821429

apply_func
ts1
ts2
w1
w2
ts1
ts2
w1
w2
>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_apply_func(vbt.nb.rolling_cov_nb)

>>> rollcov = RollCov.run(ts1, ts2, [2, 3])
>>> rollcov.rollcov
rollcov_w            2                   3
               a     b         a         b
2020-01-01   NaN   NaN       NaN       NaN
2020-01-02 -0.25  0.25       NaN       NaN
2020-01-03 -0.25  0.25 -0.666667  0.666667
2020-01-04 -0.25  0.25 -0.666667  0.666667
2020-01-05  0.25 -0.25  0.000000  0.000000
2020-01-06  0.25 -0.25  0.666667 -0.666667
2020-01-07  0.25 -0.25  0.666667 -0.666667

apply_func
custom_func
>>> from vectorbtpro.base.combining import apply_and_concat

>>> def apply_func(i, ts1, ts2, w):  In contrast to our previous apply_func, an apply function used in apply_and_concat must take the index of the iteration and select the perameters manually using this index
...     return vbt.nb.rolling_cov_nb(ts1, ts2, w[i])

>>> def custom_func(ts1, ts2, w):
...     return apply_and_concat(len(w), apply_func, ts1, ts2, w)  apply_and_concat requires the number of iterations, which is simply the length of any parameter array

>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_custom_func(custom_func)

apply_func
select_params=False
>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_apply_func(apply_func, select_params=False)

execute_kwargs
>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_apply_func(vbt.nb.rolling_cov_nb)

>>> RollCov.run(
...     ts1, ts2, np.full(100, 2),
...     execute_kwargs=dict(show_progress=True)
... )

jit_select_params
jit_kwargs
jit_select_params
remove_kwargs
kwargs_as_args
jitted_loop
execute_kwargs
apply_func
>>> def apply_func(*args, **kwargs):
...     for i, arg in enumerate(args):
...         print("arg {}: {}".format(i, type(arg)))
...     for k, v in kwargs.items():
...         print("kwarg {}: {}".format(k, type(v)))
...     raise NotImplementedError

>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_apply_func(apply_func, select_params=False)

>>> try:
...     RollCov.run(ts1, ts2, [2, 3], some_arg="some_value")
... except:
...     pass
arg 0: <class 'int'>
arg 1: <class 'numpy.ndarray'>
arg 2: <class 'numpy.ndarray'>
arg 3: <class 'list'>
kwarg some_arg: <class 'str'>

>>> MADiff = vbt.IF.from_expr(
...     "rolling_mean(@in_ts1, @p_w1) / @in_ts1 - rolling_mean(@in_ts2, @p_w2) / @in_ts2",
...     factory_kwargs=dict(class_name="MADiff")  We can still override any information passed to the factory class
... )
>>> madiff = MADiff.run(ts1, ts2, w1, w2)
>>> madiff.out
madiff_w1                                        2
madiff_w2                    3                   4
                   a         b         a         b
2020-01-01       NaN       NaN       NaN       NaN
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03 -0.500000  0.083333       NaN       NaN
2020-01-04 -0.625000  0.075000 -0.875000  0.175000
2020-01-05  0.011111 -0.183333 -0.100000 -0.100000
2020-01-06  0.166667 -0.416667  0.166667 -0.416667
2020-01-07  0.128571 -0.571429  0.228571 -0.821429

vbt.IF(...)
input_names
mov_avg_crossover
run
>>> vbt.phelp(MADiff.run)
MADiff.run(
    ts1,
    ts2,
    w1,
    w2,
    short_name='madiff',
    hide_params=None,
    hide_default=True,
    **kwargs
):
    Run `MADiff` indicator.

    * Inputs: `ts1`, `ts2`
    * Parameters: `w1`, `w2`
    * Outputs: `out`

    Pass a list of parameter names as `hide_params` to hide their column levels.
    Set `hide_default` to False to show the column levels of the parameters with a default value.

    Other keyword arguments are passed to `MADiff.run_pipeline`.

MADiff.run
ts1
ts2
w1
w2
diff
MADiff
MADiff.diff
>>> ts = pd.Series([3, 2, 1, 2, 3])
>>> fast_ma, slow_ma = vbt.MA.run_combs(
...     ts, [2, 3, 4], 
...     short_names=['fast_ma', 'slow_ma'])
>>> fast_ma.ma_crossed_above(slow_ma)
fast_ma_window             2      3
slow_ma_window      3      4      4
0               False  False  False
1               False  False  False
2               False  False  False
3               False  False  False
4                True   True  False

window
r=2
run
>>> import itertools

>>> windows = [2, 3, 4]
>>> fast_windows, slow_windows = zip(*itertools.combinations(windows, 2))
>>> fast_ma = vbt.MA.run(ts, fast_windows, short_name='fast_ma')
>>> slow_ma = vbt.MA.run(ts, slow_windows, short_name='slow_ma')
>>> fast_ma.ma_crossed_above(slow_ma)
fast_ma_window             2      3
slow_ma_window      3      4      4
0               False  False  False
1               False  False  False
2               False  False  False
3               False  False  False
4                True   True  False

run_combs
run
run_combs
run
>>> import vectorbtpro as vbt

>>> data = vbt.BinanceData.pull(
...     ['BTCUSDT', 'ETHUSDT'], 
...     start='2020-01-01 UTC',
...     end='2022-01-01 UTC',
...     timeframe='1h'
... )

>>> data.to_hdf('my_data.h5')

>>> data = vbt.HDFData.pull('my_data.h5')

vbt.HDFData.pull(['my_data.h5/BTCUSDT', 'my_data.h5/ETHUSDT'])
>>> data.data['BTCUSDT'].info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 17514 entries, 2019-12-31 23:00:00+00:00 to 2021-12-31 22:00:00+00:00
Data columns (total 10 columns):
 #   Column              Non-Null Count  Dtype              
---  ------              --------------  -----              
 0   Open                17514 non-null  float64            
 1   High                17514 non-null  float64            
 2   Low                 17514 non-null  float64            
 3   Close               17514 non-null  float64            
 4   Volume              17514 non-null  float64            
 5   Close time          17514 non-null  datetime64[ns, UTC]
 6   Quote volume        17514 non-null  float64            
 7   Number of trades    17514 non-null  int64              
 8   Taker base volume   17514 non-null  float64            
 9   Taker quote volume  17514 non-null  float64            
dtypes: datetime64[ns, UTC](1), float64(8), int64(1)
memory usage: 2.0 MB

>>> data.stats()
Start                   2020-01-01 00:00:00+00:00
End                     2021-12-31 23:00:00+00:00
Period                                      17513
Total Symbols                                   2
Last Index: BTCUSDT     2021-12-31 23:00:00+00:00
Last Index: ETHUSDT     2021-12-31 23:00:00+00:00
Null Counts: BTCUSDT                            0
Null Counts: ETHUSDT                            0
Name: agg_stats, dtype: object

>>> high = data.get('High')
>>> low = data.get('Low')
>>> close = data.get('Close')

>>> close
symbol                      BTCUSDT  ETHUSDT
Open time                                   
2020-01-01 00:00:00+00:00   7177.02   128.87
2020-01-01 01:00:00+00:00   7216.27   130.64
2020-01-01 02:00:00+00:00   7242.85   130.85
2020-01-01 03:00:00+00:00   7225.01   130.20
2020-01-01 04:00:00+00:00   7217.27   130.20
...                             ...      ...
2021-12-31 19:00:00+00:00  45728.28  3626.27
2021-12-31 20:00:00+00:00  45879.24  3645.04
2021-12-31 21:00:00+00:00  46333.86  3688.41
2021-12-31 22:00:00+00:00  46303.99  3681.80
2021-12-31 23:00:00+00:00  46216.93  3676.23

[17513 rows x 2 columns]

data.get('Close', 'BTCUSDT')
BASIC UPPERBAND = (HIGH + LOW) / 2 + Multiplier * ATR
BASIC LOWERBAND = (HIGH + LOW) / 2 - Multiplier * ATR

FINAL UPPERBAND = IF (Current BASICUPPERBAND < Previous FINAL UPPERBAND) or (Previous Close > Previous FINAL UPPERBAND)
                  THEN Current BASIC UPPERBAND
                  ELSE Previous FINAL UPPERBAND
FINAL LOWERBAND = IF (Current BASIC LOWERBAND > Previous FINAL LOWERBAND) or (Previous Close < Previous FINAL LOWERBAND)
                  THEN Current BASIC LOWERBAND 
                  ELSE Previous FINAL LOWERBAND

SUPERTREND      = IF (Previous SUPERTREND == Previous FINAL UPPERBAND) and (Current Close <= Current FINAL UPPERBAND)) 
                  THEN Current FINAL UPPERBAND
                  ELIF (Previous SUPERTREND == Previous FINAL UPPERBAND) and (Current Close > Current FINAL UPPERBAND) 
                  THEN Current FINAL LOWERBAND
                  ELIF (Previous SUPERTREND == Previous FINAL LOWERBAND) and (Current Close >= Current FINAL LOWERBAND) 
                  THEN Current FINAL LOWERBAND
                  ELIF (Previous SUPERTREND == Previous FINAL LOWERBAND) and (Current Close < Current FINAL LOWERBAND) 
                  THEN Current FINAL UPPERBAND

trend
dir_
long
short
get_med_price
get_atr
get_basic_bands
get_final_bands
supertrend
>>> import pandas as pd
>>> import numpy as np

>>> def get_med_price(high, low):
...     return (high + low) / 2

>>> def get_atr(high, low, close, period):
...     tr0 = abs(high - low)
...     tr1 = abs(high - close.shift())
...     tr2 = abs(low - close.shift())
...     tr = pd.concat((tr0, tr1, tr2), axis=1).max(axis=1)  Take the maximum across three arrays at each time step
...     atr = tr.ewm(
...         alpha=1 / period, 
...         adjust=False, 
...         min_periods=period).mean()  Wilder's moving average as opposed to the standard exponential moving average
...     return atr

>>> def get_basic_bands(med_price, atr, multiplier):
...     matr = multiplier * atr
...     upper = med_price + matr
...     lower = med_price - matr
...     return upper, lower

>>> def get_final_bands(close, upper, lower):  This function was heavily inspired by the implementation found in Pandas TA
...     trend = pd.Series(np.full(close.shape, np.nan), index=close.index)
...     dir_ = pd.Series(np.full(close.shape, 1), index=close.index)
...     long = pd.Series(np.full(close.shape, np.nan), index=close.index)
...     short = pd.Series(np.full(close.shape, np.nan), index=close.index)
... 
...     for i in range(1, close.shape[0]):  Like in the real world, iterate over the entire data, one step at a time
...         if close.iloc[i] > upper.iloc[i - 1]:
...             dir_.iloc[i] = 1
...         elif close.iloc[i] < lower.iloc[i - 1]:
...             dir_.iloc[i] = -1
...         else:
...             dir_.iloc[i] = dir_.iloc[i - 1]
...             if dir_.iloc[i] > 0 and lower.iloc[i] < lower.iloc[i - 1]:
...                 lower.iloc[i] = lower.iloc[i - 1]
...             if dir_.iloc[i] < 0 and upper.iloc[i] > upper.iloc[i - 1]:
...                 upper.iloc[i] = upper.iloc[i - 1]
... 
...         if dir_.iloc[i] > 0:
...             trend.iloc[i] = long.iloc[i] = lower.iloc[i]
...         else:
...             trend.iloc[i] = short.iloc[i] = upper.iloc[i]
...             
...     return trend, dir_, long, short

>>> def supertrend(high, low, close, period=7, multiplier=3):
...     med_price = get_med_price(high, low)
...     atr = get_atr(high, low, close, period)
...     upper, lower = get_basic_bands(med_price, atr, multiplier)
...     return get_final_bands(close, upper, lower)

supertrend
BTCUSDT
>>> supert, superd, superl, supers = supertrend(
...     high['BTCUSDT'], 
...     low['BTCUSDT'], 
...     close['BTCUSDT']
... )

>>> supert
Open time
2020-01-01 00:00:00+00:00             NaN
2020-01-01 01:00:00+00:00             NaN
2020-01-01 02:00:00+00:00             NaN
                                      ...
2021-12-31 21:00:00+00:00    47608.346563
2021-12-31 22:00:00+00:00    47608.346563
2021-12-31 23:00:00+00:00    47608.346563
Length: 17513, dtype: float64

>>> superd  1 = uptrend, -1 = downtrend
Open time
2020-01-01 00:00:00+00:00    1
2020-01-01 01:00:00+00:00    1
2020-01-01 02:00:00+00:00    1
                           ...
2021-12-31 21:00:00+00:00   -1
2021-12-31 22:00:00+00:00   -1
2021-12-31 23:00:00+00:00   -1
Length: 17513, dtype: int64

>>> superl  Any positive value = uptrend, NaN = downtrend or unknown
Open time
2020-01-01 00:00:00+00:00   NaN
2020-01-01 01:00:00+00:00   NaN
2020-01-01 02:00:00+00:00   NaN
                            ...
2021-12-31 21:00:00+00:00   NaN
2021-12-31 22:00:00+00:00   NaN
2021-12-31 23:00:00+00:00   NaN
Length: 17513, dtype: float64

>>> supers  Any positive value = downtrend, NaN = uptrend or unknown
Open time
2020-01-01 00:00:00+00:00             NaN
2020-01-01 01:00:00+00:00             NaN
2020-01-01 02:00:00+00:00             NaN
                                      ...
2021-12-31 21:00:00+00:00    47608.346563
2021-12-31 22:00:00+00:00    47608.346563
2021-12-31 23:00:00+00:00    47608.346563
Length: 17513, dtype: float64

supert
supert.head(10)
>>> date_range = slice('2020-01-01', '2020-02-01')
>>> fig = close.loc[date_range, 'BTCUSDT'].rename('Close').vbt.plot()  Using GenericAccessor.plot
>>> supers.loc[date_range].rename('Short').vbt.plot(fig=fig)
>>> superl.loc[date_range].rename('Long').vbt.plot(fig=fig)
>>> fig.show()

supertrend
>>> %%timeit
>>> supertrend(high['BTCUSDT'], low['BTCUSDT'], close['BTCUSDT'])
2.15 s ± 19.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> SUPERTREND = vbt.pandas_ta('SUPERTREND')  IndicatorFactory.from_pandas_ta, or via the vbt.pandas_ta shortcut, is the vectorbt's function to wrap any Pandas TA indicator such that it's capable to take multiple columns and parameter combinations

>>> %%timeit
>>> SUPERTREND.run(high['BTCUSDT'], low['BTCUSDT'], close['BTCUSDT'])
784 ms ± 14.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

vbt.pandas_ta
get_med_price
get_basic_bands
get_atr
get_final_bands
>>> def get_atr_np(high, low, close, period):
...     shifted_close = vbt.nb.fshift_1d_nb(close)  Using fshift_1d_nb, which shifts one-dimensional data by one to n elements forward
...     tr0 = np.abs(high - low)
...     tr1 = np.abs(high - shifted_close)
...     tr2 = np.abs(low - shifted_close)
...     tr = np.column_stack((tr0, tr1, tr2)).max(axis=1)  Similarly to Pandas, this one also concatenates three arrays as columns and finds the maximum at each time step
...     atr = vbt.nb.wwm_mean_1d_nb(tr, period)  Using wwm_mean_1d_nb, which calculates the Wilder's exponential weighted moving average on one-dimensional data
...     return atr

n
>>> from numba import njit

>>> @njit
... def get_final_bands_nb(close, upper, lower):  It's become a convention in vectorbt to use the nb suffix for Numba-compiled functions
...     trend = np.full(close.shape, np.nan)  Removed pd.Series everywhere
...     dir_ = np.full(close.shape, 1)
...     long = np.full(close.shape, np.nan)
...     short = np.full(close.shape, np.nan)
... 
...     for i in range(1, close.shape[0]):
...         if close[i] > upper[i - 1]:  Removed iloc everywhere
...             dir_[i] = 1
...         elif close[i] < lower[i - 1]:
...             dir_[i] = -1
...         else:
...             dir_[i] = dir_[i - 1]
...             if dir_[i] > 0 and lower[i] < lower[i - 1]:
...                 lower[i] = lower[i - 1]
...             if dir_[i] < 0 and upper[i] > upper[i - 1]:
...                 upper[i] = upper[i - 1]
... 
...         if dir_[i] > 0:
...             trend[i] = long[i] = lower[i]
...         else:
...             trend[i] = short[i] = upper[i]
...             
...     return trend, dir_, long, short

nb
pd.Series
iloc
@njit
iloc[...]
[...]
>>> def faster_supertrend(high, low, close, period=7, multiplier=3):
...     med_price = get_med_price(high, low)
...     atr = get_atr_np(high, low, close, period)
...     upper, lower = get_basic_bands(med_price, atr, multiplier)
...     return get_final_bands_nb(close, upper, lower)

>>> supert, superd, superl, supers = faster_supertrend(
...     high['BTCUSDT'].values,  Access the NumPy array of any Pandas object using values
...     low['BTCUSDT'].values, 
...     close['BTCUSDT'].values
... )

>>> supert
array([          nan,           nan,           nan, ..., 47608.3465635,
       47608.3465635, 47608.3465635])
>>> superd
array([ 1,  1,  1, ..., -1, -1, -1])

>>> superl
array([nan, nan, nan, ..., nan, nan, nan])

>>> supers
array([          nan,           nan,           nan, ..., 47608.3465635,
       47608.3465635, 47608.3465635])

values
supertrend
>>> pd.Series(supert, index=close.index)
Open time
2020-01-01 00:00:00+00:00             NaN
2020-01-01 01:00:00+00:00             NaN
2020-01-01 02:00:00+00:00             NaN
                                      ...
2021-12-31 21:00:00+00:00    47608.346563
2021-12-31 22:00:00+00:00    47608.346563
2021-12-31 23:00:00+00:00    47608.346563
Length: 17513, dtype: float64

%%timeit
>>> faster_supertrend(
...     high['BTCUSDT'].values, 
...     low['BTCUSDT'].values,
...     close['BTCUSDT'].values
... )
1.11 ms ± 7.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

MEDPRICE
ATR
>>> import talib

>>> def faster_supertrend_talib(high, low, close, period=7, multiplier=3):
...     avg_price = talib.MEDPRICE(high, low)  See TA-Lib Functions
...     atr = talib.ATR(high, low, close, period)
...     upper, lower = get_basic_bands(avg_price, atr, multiplier)
...     return get_final_bands_nb(close, upper, lower)

>>> faster_supertrend_talib(
...     high['BTCUSDT'].values, 
...     low['BTCUSDT'].values, 
...     close['BTCUSDT'].values
... )
(array([          nan,           nan,           nan, ..., 47608.3465635,
        47608.3465635, 47608.3465635]),
 array([ 1,  1,  1, ..., -1, -1, -1]),
 array([nan, nan, nan, ..., nan, nan, nan]),
 array([          nan,           nan,           nan, ..., 47608.3465635,
        47608.3465635, 47608.3465635]))

>>> %%timeit
>>> faster_supertrend_talib(
...     high['BTCUSDT'].values, 
...     low['BTCUSDT'].values, 
...     close['BTCUSDT'].values
... )
253 µs ± 815 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

period
multiplier
faster_supertrend_talib
>>> SuperTrend = vbt.IF(
...     class_name='SuperTrend',
...     short_name='st',
...     input_names=['high', 'low', 'close'],
...     param_names=['period', 'multiplier'],
...     output_names=['supert', 'superd', 'superl', 'supers']
... ).with_apply_func(
...     faster_supertrend_talib, 
...     takes_1d=True,  Our function accepts one-dimensional arrays only
...     period=7,  Default parameter values
...     multiplier=3
... )

vbt.IF(...)
SuperTrend
faster_supertrend_talib
SuperTrend.run
>>> vbt.phelp(SuperTrend.run)  Use phelp to see what arguments are accepted by SuperTrend.run
SuperTrend.run(
    high,
    low,
    close,
    period=Default(value=7),
    multiplier=Default(value=3),
    short_name='st',
    hide_params=None,
    hide_default=True,
    **kwargs
):
    Run `SuperTrend` indicator.

    * Inputs: `high`, `low`, `close`
    * Parameters: `period`, `multiplier`
    * Outputs: `supert`, `superd`, `superl`, `supers`

    Pass a list of parameter names as `hide_params` to hide their column levels.
    Set `hide_default` to False to show the column levels of the parameters with a default value.

    Other keyword arguments are passed to `SuperTrend.run_pipeline`.

>>> st = SuperTrend.run(high, low, close)
>>> st.supert
symbol                          BTCUSDT      ETHUSDT
Open time                                           
2020-01-01 00:00:00+00:00           NaN          NaN
2020-01-01 01:00:00+00:00           NaN          NaN
2020-01-01 02:00:00+00:00           NaN          NaN
...                                 ...          ...
2021-12-31 21:00:00+00:00  47608.346563  3770.258246
2021-12-31 22:00:00+00:00  47608.346563  3770.258246
2021-12-31 23:00:00+00:00  47608.346563  3770.258246

[17513 rows x 2 columns]

SuperTrend.run
>>> %%timeit
>>> SuperTrend.run(high, low, close)
2 ms ± 130 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

vbt.IF(...)
input_names
param_names
faster_supertrend_talib
faster_supertrend_talib
with
with_apply_func
from
from_expr
faster_supertrend_talib
>>> expr = """
... SuperTrend[st]:
... medprice = @talib_medprice(high, low)
... atr = @talib_atr(high, low, close, @p_period)
... upper, lower = get_basic_bands(medprice, atr, @p_multiplier)
... supert, superd, superl, supers = get_final_bands(close, upper, lower)
... supert, superd, superl, supers
... """

@
@talib
@p
high
>>> SuperTrend = vbt.IF.from_expr(
...     expr, 
...     takes_1d=True,
...     get_basic_bands=get_basic_bands,  Expressions don't have access to the current global variables to avoid side effects, thus we need specify the objects that are unknown to the expression
...     get_final_bands=get_final_bands_nb,
...     period=7, 
...     multiplier=3
... )

>>> st = SuperTrend.run(high, low, close)
>>> st.supert
symbol                          BTCUSDT      ETHUSDT
Open time                                           
2020-01-01 00:00:00+00:00           NaN          NaN
2020-01-01 01:00:00+00:00           NaN          NaN
2020-01-01 02:00:00+00:00           NaN          NaN
...                                 ...          ...
2021-12-31 21:00:00+00:00  47608.346563  3770.258246
2021-12-31 22:00:00+00:00  47608.346563  3770.258246
2021-12-31 23:00:00+00:00  47608.346563  3770.258246

[17513 rows x 2 columns]

>>> %%timeit
>>> SuperTrend.run(high, low, close)
2.35 ms ± 81.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

SuperTrend
plot
>>> class SuperTrend(SuperTrend):
...     def plot(self, 
...              column=None,  We can plot only one column of data at a time
...              close_kwargs=None,  Keyword arguments with the suffix kwargs can be used to set up the trace, for example, to change the color of the line
...              superl_kwargs=None,
...              supers_kwargs=None,
...              fig=None,  We can pass our own figure
...              **layout_kwargs):  Any additional keyword argument passed to this method can be used to set up the layout of the figure
...         close_kwargs = close_kwargs if close_kwargs else {}
...         superl_kwargs = superl_kwargs if superl_kwargs else {}
...         supers_kwargs = supers_kwargs if supers_kwargs else {}
...         
...         close = self.select_col_from_obj(self.close, column).rename('Close')
...         supers = self.select_col_from_obj(self.supers, column).rename('Short')
...         superl = self.select_col_from_obj(self.superl, column).rename('Long')
...         
...         fig = close.vbt.plot(fig=fig, **close_kwargs, **layout_kwargs)  Update the layout only once
...         supers.vbt.plot(fig=fig, **supers_kwargs)
...         superl.vbt.plot(fig=fig, **superl_kwargs)
...         
...         return fig

kwargs
SuperTrend
>>> st = SuperTrend.run(high, low, close)
>>> st.loc[date_range, 'BTCUSDT'].plot(
...     superl_kwargs=dict(trace_kwargs=dict(line_color='limegreen')),
...     supers_kwargs=dict(trace_kwargs=dict(line_color='red'))
... ).show()

entries
exits
>>> entries = (~st.superl.isnull()).vbt.signals.fshift()  Using SignalsAccessor.fshift
>>> exits = (~st.supers.isnull()).vbt.signals.fshift()

>>> pf = vbt.Portfolio.from_signals(
...     close=close, 
...     entries=entries, 
...     exits=exits, 
...     fees=0.001, 
...     freq='1h'
... )

ETHUSDT
>>> pf['ETHUSDT'].stats()
Start                         2020-01-01 00:00:00+00:00
End                           2021-12-31 23:00:00+00:00
Period                                729 days 17:00:00
Start Value                                       100.0
Min Value                                     98.469385
Max Value                                   1805.987865
End Value                                   1135.272383
Total Return [%]                            1035.272383
Benchmark Return [%]                        2752.665477
Total Time Exposure [%]                       51.750128
Max Gross Exposure [%]                            100.0
Max Drawdown [%]                               37.39953
Max Drawdown Duration                  85 days 09:00:00
Total Orders                                        348
Total Fees Paid                              272.755758
Total Trades                                        174
Win Rate [%]                                  43.103448
Best Trade [%]                                33.286985
Worst Trade [%]                              -13.783496
Avg Winning Trade [%]                          7.815551
Avg Losing Trade [%]                          -3.021041
Avg Winning Trade Duration              3 days 06:43:12
Avg Losing Trade Duration     1 days 07:54:32.727272727
Profit Factor                                  1.390947
Expectancy                                     5.949841
Sharpe Ratio                                   2.258501
Calmar Ratio                                   6.320363
Omega Ratio                                    1.103525
Sortino Ratio                                   3.27869
Name: ETHUSDT, dtype: object

4, 5, ..., 20
2, 2.1, 2.2, ..., 4
SuperTrend.run
param_product=True
>>> periods = np.arange(4, 20)
>>> multipliers = np.arange(20, 41) / 10  Doing np.arange(2, 4.1, 0.1) produces 3.000000000000001 instead of 3.0, which would make it harder to do indexing later

>>> st = SuperTrend.run(
...     high, low, close, 
...     period=periods, 
...     multiplier=multipliers,
...     param_product=True,
...     execute_kwargs=dict(show_progress=True)  Show progress bar
... )

np.arange(2, 4.1, 0.1)
3.000000000000001
3.0
>>> st.wrapper.columns
MultiIndex([( 4, 2.0, 'BTCUSDT'),
            ( 4, 2.0, 'ETHUSDT'),
            ( 4, 2.1, 'BTCUSDT'),
            ( 4, 2.1, 'ETHUSDT'),
            ( 4, 2.2, 'BTCUSDT'),
            ...
            (19, 3.8, 'ETHUSDT'),
            (19, 3.9, 'BTCUSDT'),
            (19, 3.9, 'ETHUSDT'),
            (19, 4.0, 'BTCUSDT'),
            (19, 4.0, 'ETHUSDT')],
           names=['st_period', 'st_multiplier', 'symbol'], length=672)

>>> st.loc[date_range, (19, 4, 'ETHUSDT')].plot().show()

>>> print(st.getsize())
377.6 MB

>>> output_size = st.wrapper.shape[0] * st.wrapper.shape[1]
>>> n_outputs = 4
>>> data_type_size = 8
>>> input_size * n_outputs * data_type_size / 1024 / 1024
359.173828125

get_final_bands_nb
np.float32
np.float16
>>> entries = (~st.superl.isnull()).vbt.signals.fshift()
>>> exits = (~st.supers.isnull()).vbt.signals.fshift()

>>> pf = vbt.Portfolio.from_signals(
...     close=close, 
...     entries=entries, 
...     exits=exits, 
...     fees=0.001, 
...     freq='1h'
... )

>>> pf.sharpe_ratio.vbt.heatmap(
...     x_level='st_period', 
...     y_level='st_multiplier',
...     slider_level='symbol'
... )

>>> vbt.Portfolio.from_holding(close, freq='1h').sharpe_ratio
symbol
BTCUSDT    1.561447
ETHUSDT    2.170813
Name: sharpe_ratio, dtype: float64

True
False
|
>>> import vectorbtpro as vbt
>>> import numpy as np
>>> import pandas as pd

>>> data = vbt.BinanceData.pull(
...     ["BTCUSDT", "ETHUSDT"], 
...     start="2021-01-01",
...     end="2022-01-01"
... )
>>> data.get("Low")
symbol                      BTCUSDT  ETHUSDT
Open time                                   
2021-01-01 00:00:00+00:00  28624.57   714.29
2021-01-02 00:00:00+00:00  28946.53   714.91
2021-01-03 00:00:00+00:00  31962.99   768.71
...                             ...      ...
2021-12-29 00:00:00+00:00  46096.99  3604.20
2021-12-30 00:00:00+00:00  45900.00  3585.00
2021-12-31 00:00:00+00:00  45678.00  3622.29

[365 rows x 2 columns]

>>> bb = vbt.talib("BBANDS").run(
...     data.get("Close"),
...     timeperiod=vbt.Default(14),  Wrap each parameter with Default to hide its column level. Alternatively, we can pass a list of the parameters to hide with hide_params.
...     nbdevup=vbt.Default(2),
...     nbdevdn=vbt.Default(2)
... )
>>> bb.lowerband  Get the lower band as a Pandas object. We can list all the output names of an indicator using bb.output_names.
symbol                          BTC-USD      ETH-USD
Date                                                
2021-04-23 00:00:00+00:00           NaN          NaN
2021-04-24 00:00:00+00:00           NaN          NaN
2021-04-25 00:00:00+00:00           NaN          NaN
...                                 ...          ...
2022-04-21 00:00:00+00:00  38987.326323  2912.894415
2022-04-22 00:00:00+00:00  38874.059308  2898.681307
2022-04-23 00:00:00+00:00  38915.417003  2903.756905

[366 rows x 2 columns]

>>> mask = data.get("Low") < bb.lowerband  Compare two numeric arrays element-wise
>>> mask
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-01-01 00:00:00+00:00    False    False
2021-01-02 00:00:00+00:00    False    False
2021-01-03 00:00:00+00:00    False    False
...                            ...      ...
2021-12-29 00:00:00+00:00    False     True
2021-12-30 00:00:00+00:00    False     True
2021-12-31 00:00:00+00:00    False    False

[365 rows x 2 columns]

>>> mask.sum()  Get the number of signals in each column for a better overview
symbol
BTCUSDT    36
ETHUSDT    28
dtype: int64

hide_params
bb.output_names
>>> bb_mult = vbt.talib("BBANDS").run(
...     data.get("Close"),
...     timeperiod=vbt.Default(14),
...     nbdevup=[2, 3],
...     nbdevdn=[2, 3]  Two parameter combinations: (14, 2, 2) and (14, 3, 3)
... )
>>> mask = data.get("Low") < bb_mult.lowerband
ValueError: Can only compare identically-labeled DataFrame objects

BTCUSDT
ETHUSDT
(2, 2, BTCUSDT)
(2, 2, ETHUSDT)
(3, 3, BTCUSDT)
(3, 3, ETHUSDT)
vbt
__lt__
combine_func
(2, 2, BTCUSDT)
(3, 3, BTCUSDT)
BTCUSDT
(2, 2, ETHSDT)
(3, 3, ETHSDT)
ETHSDT
>>> mask = data.get("Low").vbt < bb_mult.lowerband  vbt must be always called on the left operand
>>> mask
bbands_nbdevup                          2               3
bbands_nbdevdn                          2               3
symbol                    BTCUSDT ETHUSDT BTCUSDT ETHUSDT
Open time                                                
2021-01-01 00:00:00+00:00   False   False   False   False
2021-01-02 00:00:00+00:00   False   False   False   False
2021-01-03 00:00:00+00:00   False   False   False   False
...                           ...     ...     ...     ...
2021-12-29 00:00:00+00:00   False    True   False   False
2021-12-30 00:00:00+00:00   False    True   False   False
2021-12-31 00:00:00+00:00   False   False   False   False

[365 rows x 4 columns]

>>> mask.sum()
bbands_nbdevup  bbands_nbdevdn  symbol 
2               2               BTCUSDT    53
                                ETHUSDT    48
3               3               BTCUSDT    10
                                ETHUSDT     9
dtype: int64

vbt
symbol
{name}_above
{name}_equal
{name}_below
>>> mask = bb_mult.lowerband_above(data.get("Low"))  Our indicator doesn't have an input or output for the low price, thus we need to reverse the comparison order and return whether the lower band is above the low price
>>> mask.sum()
bbands_nbdevup  bbands_nbdevdn  symbol 
2               2               BTCUSDT    53
                                ETHUSDT    48
3               3               BTCUSDT    10
                                ETHUSDT     9
dtype: int64

vbt
>>> bandwidth = (bb.upperband - bb.lowerband) / bb.middleband

>>> mask = bandwidth.vbt > vbt.Param([0.15, 0.3], name="threshold")  Passes both objects to BaseAccessor.combine, broadcasts them using broadcast, and combines them using numpy.greater. Thanks to broadcasting, each value in vbt.Param is combined with each column in the array. This works only for scalars!
>>> mask.sum()
threshold  symbol 
0.15       BTCUSDT    253
           ETHUSDT    316
0.30       BTCUSDT     65
           ETHUSDT    136
dtype: int64

>>> mask = bandwidth.vbt.combine(
...     [0.15, 0.3],  When passing a list, the DataFrame gets compared to each item in the list
...     combine_func=np.greater, 
...     keys=pd.Index([0.15, 0.3], name="threshold")  Argument keys is used to append a column level describing the items in the list
... )
>>> mask.sum()
threshold  symbol 
0.15       BTCUSDT    253
           ETHUSDT    316
0.30       BTCUSDT     65
           ETHUSDT    136
dtype: int64

vbt.Param
keys
>>> mask = pd.concat(
...     (bandwidth > 0.15, bandwidth > 0.3), 
...     keys=pd.Index([0.15, 0.3], name="threshold"), 
...     axis=1
... )
>>> mask.sum()
threshold  symbol 
0.15       BTCUSDT    253
           ETHUSDT    316
0.30       BTCUSDT     65
           ETHUSDT    136
dtype: int64

True
True
True
vbt.signals
True
True
>>> low_below_lband = data.get("Low") < bb.lowerband
>>> mask = low_below_lband.vbt.signals.first()
>>> mask.sum()
symbol
BTCUSDT    21
ETHUSDT    20
dtype: int64

BTCUSDT
>>> btc_low = data.get("Low", "BTCUSDT").rename("Low")  Give the column another name for the legend
>>> btc_lowerband = bb.lowerband["BTCUSDT"].rename("Lower Band")
>>> btc_mask = mask["BTCUSDT"].rename("Signals")

>>> fig = btc_low.vbt.plot()  First plotting method returns a figure, which needs to be passed to each subsequent plotting method
>>> btc_lowerband.vbt.plot(fig=fig)
>>> btc_mask.vbt.signals.plot_as_markers(
...     y=btc_low, 
...     trace_kwargs=dict(
...         marker=dict(
...             color="#DFFF00"
...         )
...     ),
...     fig=fig
... )  We're using btc_low as the Y-values at which to place the markers
>>> fig.show()

btc_low
after_false=True
False
>>> mask = low_below_lband.vbt.signals.first(after_false=True)
>>> mask.sum()
symbol
BTCUSDT    21
ETHUSDT    20
dtype: int64

False
True
after_false
True
>>> sample_low = pd.Series([10, 9, 8, 9, 8])
>>> sample_lband = pd.Series([np.nan, np.nan, 9, 8, 9])
>>> sample_mask = sample_low < sample_lband
>>> sample_mask.vbt.signals.first(after_false=True)  The first crossover shouldn't happen because we don't know what happens at those NaN, while the second crossover is perfectly valid
0    False
1    False
2     True
3    False
4     True
dtype: bool

>>> sample_mask[sample_lband.ffill().isnull()] = True  Forward fill the indicator values to keep only the first NaN values, and set the mask to True at those positions to make after_false effective again
>>> sample_mask.vbt.signals.first(after_false=True)
0    False
1    False
2    False
3    False
4     True
dtype: bool

True
after_false
>>> buffer = sample_lband.ffill().isnull().sum(axis=0).max()  Find the maximum length of each first consecutive series of NaN values across all columns
>>> buffer
2

>>> sample_buf_mask = sample_low.iloc[buffer:] < sample_lband.iloc[buffer:]
>>> sample_buf_mask = sample_buf_mask.vbt.signals.first(after_false=True)
>>> sample_mask = sample_low.vbt.wrapper.fill(False)
>>> sample_mask.loc[sample_buf_mask.index] = sample_buf_mask
>>> sample_mask
0    False
1    False
2    False
3    False
4     True
dtype: bool

False
vbt
>>> mask = data.get("Low").vbt.crossed_below(bb.lowerband, wait=1)  Wait one bar for confirmation
>>> mask.sum()
symbol
BTCUSDT    15
ETHUSDT    11
dtype: int64

wait
{name}_crossed_above
{name}_crossed_below
>>> mask = bb.lowerband_crossed_above(data.get("Low"), wait=1)
>>> mask.sum()
symbol
BTCUSDT    15
ETHUSDT    11
dtype: int64

&
|
~
^
and
or
not
mask1 and mask2
mask1 & mask2
mask1 or mask2
mask1 | mask2
not mask
~mask
>>> cond1 = data.get("Low") < bb.lowerband
>>> cond2 = bandwidth > 0.3
>>> cond3 = data.get("High") > bb.upperband
>>> cond4 = bandwidth < 0.15

>>> mask = (cond1 & cond2) | (cond3 & cond4)
>>> mask.sum()
symbol
BTCUSDT    25
ETHUSDT    13
dtype: int64

>>> cond1 = data.get("Low").vbt < bb.lowerband
>>> cond2 = bandwidth.vbt > vbt.Param([0.3, 0.3, 0.4, 0.4], name="cond2_th")  Test two thresholds in the second condition, and repeat them to compare each one to both thresholds in the fourth condition
>>> cond3 = data.get("High").vbt > bb.upperband
>>> cond4 = bandwidth.vbt < vbt.Param([0.1, 0.2, 0.1, 0.2], name="cond4_th")  Test two thresholds in the fourth condition, and tile them to compare each one to both thresholds in the second condition

>>> mask = (cond1.vbt & cond2).vbt | (cond3.vbt & cond4)  Notice how vbt is appended to each operand on the left. Adding it to any operand on the right is redundant.
>>> mask.sum()
cond2_th  cond4_th  symbol 
0.3       0.1       BTCUSDT    11
                    ETHUSDT    10
          0.2       BTCUSDT    28
                    ETHUSDT    27
0.4       0.1       BTCUSDT     9
                    ETHUSDT     5
          0.2       BTCUSDT    26
                    ETHUSDT    22
dtype: int64

vbt
symbol
>>> from itertools import product

>>> cond1 = data.get("Low").vbt < bb.lowerband
>>> cond2 = bandwidth.vbt > vbt.Param([0.3, 0.4], name="cond2_th")  Each DataFrame should contain only unique parameter combinations (and columns in general)
>>> cond3 = data.get("High").vbt > bb.upperband
>>> cond4 = bandwidth.vbt < vbt.Param([0.1, 0.2], name="cond4_th")

>>> i1 = np.split(np.arange(len(cond1.columns)), len(cond1.columns) // 2)  Split each position array into blocks with symbols
>>> i2 = np.split(np.arange(len(cond2.columns)), len(cond2.columns) // 2)
>>> i3 = np.split(np.arange(len(cond3.columns)), len(cond3.columns) // 2)
>>> i4 = np.split(np.arange(len(cond4.columns)), len(cond4.columns) // 2)

>>> i1
[array([0, 1])]
>>> i2
[array([0, 1]), array([2, 3])]
>>> i3
[array([0, 1])]
>>> i4
[array([0, 1]), array([2, 3])]

>>> i1, i2, i3, i4 = zip(*product(i1, i2, i3, i4))  Combine all blocks (= parameter combinations) using the Cartesian product

>>> i1
(array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]))
>>> i2
(array([0, 1]), array([0, 1]), array([2, 3]), array([2, 3]))
>>> i3
(array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]))
>>> i4
(array([0, 1]), array([2, 3]), array([0, 1]), array([2, 3]))

>>> i1 = np.asarray(i1).flatten()  Flatten each array with blocks to get an array that can be used in indexing
>>> i2 = np.asarray(i2).flatten()
>>> i3 = np.asarray(i3).flatten()
>>> i4 = np.asarray(i4).flatten()

>>> i1
[0 1 0 1 0 1 0 1]
>>> i2
[0 1 0 1 2 3 2 3]
>>> i3
[0 1 0 1 0 1 0 1]
>>> i4
[0 1 2 3 0 1 2 3]

>>> cond1 = cond1.iloc[:, i1]  Using the final positions, select the columns from each DataFrame, which will make each DataFrame have the same number of columns
>>> cond2 = cond2.iloc[:, i2]
>>> cond3 = cond3.iloc[:, i3]
>>> cond4 = cond4.iloc[:, i4]

>>> mask = (cond1.vbt & cond2).vbt | (cond3.vbt & cond4)  All columns have the same length but still different labels  let the vectorbt's broadcaster do the alignment job
>>> mask.sum()
cond2_th  cond4_th  symbol 
0.3       0.1       BTCUSDT    11
                    ETHUSDT    10
          0.2       BTCUSDT    28
                    ETHUSDT    27
0.4       0.1       BTCUSDT     9
                    ETHUSDT     5
          0.2       BTCUSDT    26
                    ETHUSDT    22
dtype: int64

>>> cond1 = data.get("Low").vbt < bb.lowerband
>>> cond2 = bandwidth.vbt > vbt.Param([0.3, 0.4], name="cond2_th")
>>> cond3 = data.get("High").vbt > bb.upperband
>>> cond4 = bandwidth.vbt < vbt.Param([0.1, 0.2], name="cond4_th")

>>> cond1, cond2, cond3, cond4 = vbt.pd_acc.x(cond1, cond2, cond3, cond4)  x is an alias method for cross. Another way: cond1.vbt.x(cond2, cond3, cond4).
>>> mask = (cond1.vbt & cond2).vbt | (cond3.vbt & cond4)

x
cross
cond1.vbt.x(cond2, cond3, cond4)
@res_talib_bbands
BBANDS
talib
cond2_th
cond4_th
mask
>>> MaskGenerator = vbt.IF.from_expr("""
... upperband, middleband, lowerband = @res_talib_bbands
... bandwidth = (upperband - lowerband) / middleband
... cond1 = low < lowerband
... cond2 = bandwidth > @p_cond2_th
... cond3 = high > upperband
... cond4 = bandwidth < @p_cond4_th
... @out_mask:(cond1 & cond2) | (cond3 & cond4)
... """)

>>> vbt.phelp(MaskGenerator.run, incl_doc=False)  Take a look at the arguments that the run method expects
Indicator.run(
    high,
    low,
    close,
    cond2_th,
    cond4_th,
    bbands_timeperiod=Default(value=5),
    bbands_nbdevup=Default(value=2),
    bbands_nbdevdn=Default(value=2),
    bbands_matype=Default(value=0),
    bbands_timeframe=Default(value=None),
    short_name='custom',
    hide_params=None,
    hide_default=True,
    **kwargs
)

>>> mask_generator = MaskGenerator.run(
...     high=data.get("High"),
...     low=data.get("Low"),
...     close=data.get("Close"),
...     cond2_th=[0.3, 0.4],
...     cond4_th=[0.1, 0.2],
...     bbands_timeperiod=vbt.Default(14),
...     param_product=True
... )  Run the indicator on the Cartesian product of all parameters
>>> mask_generator.mask.sum()
custom_cond2_th  custom_cond4_th  symbol 
0.3              0.1              BTCUSDT    11
                                  ETHUSDT    10
                 0.2              BTCUSDT    28
                                  ETHUSDT    27
0.4              0.1              BTCUSDT     9
                                  ETHUSDT     5
                 0.2              BTCUSDT    26
                                  ETHUSDT    22
dtype: int64

>>> cond1 = data.get("Low") < bb.lowerband
>>> cond2 = bandwidth > bandwidth.shift(1)  The shifted array holds the previous values

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    42
ETHUSDT    39
dtype: int64

>>> cond2 = bandwidth > bandwidth.rolling("7d").apply(lambda x: x[0])

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    33
ETHUSDT    28
dtype: int64

>>> def exactly_ago(sr):  By passing raw=False, the input will be a Pandas Series
...     if sr.index[0] == sr.index[-1] - pd.Timedelta("7d"):
...         return sr.iloc[0]
...     return np.nan

>>> cond_7d_ago = bandwidth.rolling("8d").apply(exactly_ago, raw=False)
>>> cond2 = bandwidth > cond_7d_ago

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    29
ETHUSDT    26
dtype: int64

raw=False
>>> from numba import njit

>>> @njit
... def exactly_ago_meta_nb(from_i, to_i, col, index, freq, arr):  The meta function must take three arguments: the current window start index, window end index, and column
...     if index[from_i] == index[to_i - 1] - freq:  Window end index is exclusive, thus use to_i - 1 to get the last index in the window
...         return arr[from_i, col]  Use the current column index to select the column. Remember to make all arrays two-dimensional.
...     return np.nan

>>> cond_7d_ago = vbt.pd_acc.rolling_apply(
...     "8d",
...     exactly_ago_meta_nb,
...     bandwidth.index.values,  Here come our three user-defined arguments
...     pd.Timedelta("7d").to_timedelta64(),
...     vbt.to_2d_array(bandwidth),
...     wrapper=bandwidth.vbt.wrapper  Wrapper contains the metadata we want to iterate over and to construct the final Pandas object
... )
>>> cond2 = bandwidth > cond_7d_ago

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    29
ETHUSDT    26
dtype: int64

to_i - 1
>>> cond2 = bandwidth > bandwidth.vbt.ago("7d")

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    29
ETHUSDT    26
dtype: int64

>>> bandwidth.iloc[-8]
symbol
BTCUSDT    0.125477
ETHUSDT    0.096458
Name: 2021-12-24 00:00:00+00:00, dtype: float64

>>> bandwidth.vbt.ago("7d").iloc[-1]
symbol
BTCUSDT    0.125477
ETHUSDT    0.096458
Name: 2021-12-31 00:00:00+00:00, dtype: float64

method="ffill"
any
all
max
min
>>> cond2 = data.get("Close").vbt.crossed_below(bb.middleband)
>>> cond2 = cond2.rolling(5, min_periods=1).max().astype(bool)

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    36
ETHUSDT    28
dtype: int64

min_periods
True
>>> cond2 = data.get("Close").vbt.crossed_below(bb.middleband)
>>> cond2 = cond2.vbt.rolling_any(5)

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    36
ETHUSDT    28
dtype: int64

reduce_func_nb
wrap_kwargs
False
>>> cond2 = data.get("Close").vbt.crossed_below(bb.middleband)
>>> cond2 = cond2.vbt.rolling_apply(
...     "5d", "any",  "any" translates to any_reduce_nb
...     minp=1, 
...     wrap_kwargs=dict(fillna=0, dtype=bool)
... )

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    36
ETHUSDT    28
dtype: int64

>>> anchor_points = data.wrapper.get_index_points(  Find the position of the first timestamp in each month using ArrayWrapper.get_index_ranges
...     every="MS", 
...     start=0,  Remove this and the next line to not include the first month if it's incomplete
...     exact_start=True
... )
>>> anchor_points
array([  0,  31,  59,  90, 120, 151, 181, 212, 243, 273, 304, 334])

>>> left_bound = np.full(len(data.wrapper.index), np.nan)  The left bound of each window will be the respective anchor point (month start). Create an integer array of the same size as our index, fill the anchor points at their positions, and forward fill them.
>>> left_bound[anchor_points] = anchor_points
>>> left_bound = vbt.dt.to_ns(vbt.nb.ffill_1d_nb(left_bound))
>>> left_bound = bandwidth.index[left_bound]
>>> left_bound
DatetimeIndex(['2021-01-01 00:00:00+00:00', '2021-01-01 00:00:00+00:00',
               '2021-01-01 00:00:00+00:00', '2021-01-01 00:00:00+00:00',
               '2021-01-01 00:00:00+00:00', '2021-01-01 00:00:00+00:00',
               ...
               '2021-12-01 00:00:00+00:00', '2021-12-01 00:00:00+00:00',
               '2021-12-01 00:00:00+00:00', '2021-12-01 00:00:00+00:00',
               '2021-12-01 00:00:00+00:00', '2021-12-01 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', ...)

>>> right_bound = data.wrapper.index  The right bound of each window is the current index
>>> right_bound
DatetimeIndex(['2021-01-01 00:00:00+00:00', '2021-01-02 00:00:00+00:00',
               '2021-01-03 00:00:00+00:00', '2021-01-04 00:00:00+00:00',
               '2021-01-05 00:00:00+00:00', '2021-01-06 00:00:00+00:00',
               ...
               '2021-12-26 00:00:00+00:00', '2021-12-27 00:00:00+00:00',
               '2021-12-28 00:00:00+00:00', '2021-12-29 00:00:00+00:00',
               '2021-12-30 00:00:00+00:00', '2021-12-31 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', ...)

>>> mask = (bandwidth <= 0.1).vbt.resample_between_bounds(  Use GenericAccessor.resample_between_bounds to map the mask values to their windows, and aggregate each window using the "any" operation
...     left_bound, 
...     right_bound,
...     "any",
...     closed_lbound=True,  Make both bounds inclusive to include every single timestamp in a month
...     closed_rbound=True,
...     wrap_kwargs=dict(fillna=0, dtype=bool)
... )
>>> mask.astype(int).vbt.ts_heatmap().show()

>>> min_data = vbt.BinanceData.pull(  Fetch hourly data for a better illustration
...     ["BTCUSDT", "ETHUSDT"], 
...     start="2021-01-01 UTC",  Don't forget about the UTC timezone for crypto
...     end="2021-02-01 UTC",
...     timeframe="1h"
... )
>>> index = min_data.wrapper.index
>>> tuesday_index = index[index.weekday == 1]
>>> tuesday_index
DatetimeIndex(['2021-01-05 00:00:00+00:00', '2021-01-05 01:00:00+00:00',
               '2021-01-05 02:00:00+00:00', '2021-01-05 03:00:00+00:00',
               '2021-01-05 04:00:00+00:00', '2021-01-05 05:00:00+00:00',
               ...
               '2021-01-26 18:00:00+00:00', '2021-01-26 19:00:00+00:00',
               '2021-01-26 20:00:00+00:00', '2021-01-26 21:00:00+00:00',
               '2021-01-26 22:00:00+00:00', '2021-01-26 23:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

>>> tuesday_1800_index = tuesday_index[tuesday_index.hour == 18]
>>> tuesday_1800_index
DatetimeIndex(['2021-01-05 18:00:00+00:00', '2021-01-12 18:00:00+00:00',
               '2021-01-19 18:00:00+00:00', '2021-01-26 18:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

>>> tuesday_1730_index = index[
...     (index.weekday == 1) & 
...     (index.hour == 17) & 
...     (index.minute == 30)
... ]
>>> tuesday_1730_index
DatetimeIndex([], dtype='datetime64[ns, UTC]', name='Open time', freq='H')

>>> index.get_indexer([pd.Timestamp("2021-01-07", tz=index.tz)])  Don't forget to provide the timezone if the index is timezone-aware!
array([144])

-1
>>> index.get_indexer([pd.Timestamp("2021-01-07 17:30:00", tz=index.tz)]) 
array([-1])

-1
method='ffill'
method='bfill'
>>> index[index.get_indexer(
...     [pd.Timestamp("2021-01-07 17:30:00", tz=index.tz)],
...     method="ffill"
... )]
DatetimeIndex(['2021-01-07 17:00:00+00:00'], ...)

>>> index[index.get_indexer(
...     [pd.Timestamp("2021-01-07 17:30:00", tz=index.tz)],
...     method="bfill"
... )]
DatetimeIndex(['2021-01-07 18:00:00+00:00'], ...)

True
>>> each_tuesday = pd.date_range(index[0], index[-1], freq="W-TUE")  Timezone is embedded into both timestamps and will be used by pd.date_range automatically
>>> each_tuesday_1730 = each_tuesday + pd.Timedelta(hours=17, minutes=30)  Adding a timedelta to a datetime will produce a new datetime
>>> each_tuesday_1730
DatetimeIndex(['2021-01-05 17:30:00+00:00', '2021-01-12 17:30:00+00:00',
               '2021-01-19 17:30:00+00:00', '2021-01-26 17:30:00+00:00'],
              dtype='datetime64[ns, UTC]', freq=None)

>>> positions = index.get_indexer(each_tuesday_1730, method="bfill")

>>> min_symbol_wrapper = min_data.get_symbol_wrapper()  Use Data.get_symbol_wrapper to get a Pandas wrapper where columns are symbols
>>> mask = min_symbol_wrapper.fill(False)  Use ArrayWrapper.fill to create an array with the same shape as the wrapper, and fill it with False values
>>> mask.iloc[positions] = True  Positions are integer indices of rows, thus use iloc to select the elements at those rows
>>> mask.sum()
symbol
BTCUSDT    4
ETHUSDT    4
dtype: int64

pd.date_range
False
iloc
>>> mask[mask.any(axis=1)].index.strftime("%A %T")  Details of the string format can be found here
Index(['Tuesday 18:00:00', 'Tuesday 18:00:00', 'Tuesday 18:00:00',
       'Tuesday 18:00:00'],
      dtype='object', name='Open time')

>>> tuesday_after_1700 = (index.weekday == 1) & (index.hour >= 17)
>>> wednesday_before_1700 = (index.weekday == 2) & (index.hour < 17)
>>> main_cond = tuesday_after_1700 | wednesday_before_1700
>>> mask = min_symbol_wrapper.fill(False)
>>> mask[main_cond] = True
>>> mask = mask.vbt.signals.first()
>>> mask[mask.any(axis=1)].index.strftime("%A %T")
Index(['Tuesday 17:00:00', 'Tuesday 17:00:00', 'Tuesday 17:00:00',
       'Tuesday 17:00:00'],
      dtype='object', name='Open time')

>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set(
...     True, 
...     every="W-TUE", 
...     at_time="17:30", 
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index.strftime("%A %T")
Index(['Tuesday 18:00:00', 'Tuesday 18:00:00', 'Tuesday 18:00:00',
       'Tuesday 18:00:00'],
      dtype='object', name='Open time')

>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set(
...     True, 
...     every="W-TUE", 
...     at_time="18:00", 
...     add_delta=pd.Timedelta(1, "ns"),  Add a nanosecond to exclude 18:00
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index.strftime("%A %T")
Index(['Tuesday 19:00:00', 'Tuesday 19:00:00', 'Tuesday 19:00:00',
       'Tuesday 19:00:00'],
      dtype='object', name='Open time')

>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set_between(
...     True, 
...     every="W-MON", 
...     start_time="12:00", 
...     end_time="17:00", 
...     add_end_delta=pd.Timedelta(days=1),  Add a day to get Wednesday
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index.strftime("%A %T")
Index(['Monday 12:00:00', 'Monday 13:00:00', 'Monday 14:00:00',
       'Monday 15:00:00', 'Monday 16:00:00', 'Monday 17:00:00',
       'Monday 18:00:00', 'Monday 19:00:00', 'Monday 20:00:00',
       ...
       'Tuesday 10:00:00', 'Tuesday 11:00:00', 'Tuesday 12:00:00',
       'Tuesday 13:00:00', 'Tuesday 14:00:00', 'Tuesday 15:00:00',
       'Tuesday 16:00:00'],
      dtype='object', name='Open time', length=116)

>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set(
...     True, 
...     on="January 7th 2021 UTC",  Human-readable datetime strings are accepted
...     indexer_method=None,  The default method is bfill, which takes the next timestamp if there was no exact match
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index
DatetimeIndex(['2021-01-07 00:00:00+00:00'], ...)

bfill
>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set_between(
...     True, 
...     start=["2021-01-01 12:00:00", "2021-01-07 12:00:00"],  The first range is built from the first element in start and end, and the second range is built from the second element. We could have also used human-readable datetime strings but those take time to map (to enable, see settings.datetime).
...     end=["2021-01-02 12:00:00", "2021-01-08 12:00:00"],
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index
DatetimeIndex(['2021-01-01 12:00:00+00:00', '2021-01-01 13:00:00+00:00',
               '2021-01-01 14:00:00+00:00', '2021-01-01 15:00:00+00:00',
               '2021-01-01 16:00:00+00:00', '2021-01-01 17:00:00+00:00',
               ...
               '2021-01-08 06:00:00+00:00', '2021-01-08 07:00:00+00:00',
               '2021-01-08 08:00:00+00:00', '2021-01-08 09:00:00+00:00',
               '2021-01-08 10:00:00+00:00', '2021-01-08 11:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

start
end
>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set_between(
...     True, 
...     every="W-MON",
...     split_every=False,  Otherwise, ranges will be built from each pair of week starts
...     add_end_delta="2h",
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index
DatetimeIndex(['2021-01-04 00:00:00+00:00', '2021-01-04 01:00:00+00:00',
               '2021-01-11 00:00:00+00:00', '2021-01-11 01:00:00+00:00',
               '2021-01-18 00:00:00+00:00', '2021-01-18 01:00:00+00:00',
               '2021-01-25 00:00:00+00:00', '2021-01-25 01:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

>>> @njit  Don't forget to decorate with @njit to make the function Numba-compiled
... def generate_mask_1d_nb(  Good convention is to append a suffix nb to Numba-compiled functions and 1d_nb to those that work only on one column of data
...     high, low,  Data arrays required by our logic
...     uband, mband, lband,  Bollinger Bands arrays required by our logic
...     cond2_th, cond4_th  Thresholds. Can be also defined as keyword arguments with default values.
... ):
...     out = np.full(high.shape, False)  Create a boolean array of the same shape as each of our arrays, and fill it with the default value False (no signal)
...     
...     for i in range(high.shape[0]):  Iterate over the rows (= timestamps)
...         Here comes the main logic. We perform all operations on one element at a time instead of arrays, which is great for memory.
...         bandwidth = (uband[i] - lband[i]) / mband[i]
...         cond1 = low[i] < lband[i]
...         cond2 = bandwidth > cond2_th
...         cond3 = high[i] > uband[i]
...         cond4 = bandwidth < cond4_th
...         signal = (cond1 and cond2) or (cond3 and cond4)  When working with single values, we need to replace & with and, | with or, and ~ with not
...         out[i] = signal  Write the current signal to the array
...         
...     return out

>>> mask = generate_mask_1d_nb(
...     data.get("High")["BTCUSDT"].values,  Since we're working with a one-dimensional Numba-compiled function, we must pass one-dimensional NumPy arrays
...     data.get("Low")["BTCUSDT"].values,
...     bb.upperband["BTCUSDT"].values,
...     bb.middleband["BTCUSDT"].values,
...     bb.lowerband["BTCUSDT"].values,
...     0.30,
...     0.15
... )
>>> symbol_wrapper = data.get_symbol_wrapper()
>>> mask = symbol_wrapper["BTCUSDT"].wrap(mask)  Since we generated the mask for the symbol BTCUSDT only, select the same column from the wrapper and wrap the array
>>> mask.sum()
25

@njit
nb
1d_nb
False
&
and
|
or
~
not
BTCUSDT
generate_mask_1d_nb
>>> @njit
... def generate_mask_nb(  Remove 1d from the name if the function takes two-dimensional arrays
...     high, low,
...     uband, mband, lband,
...     cond2_th, cond4_th
... ):
...     out = np.empty(high.shape, dtype=np.bool_)  Create an empty boolean array that will be gradually filled with results from generate_mask_1d_nb. When using np.empty instead of np.full, make sure to override each single value, otherwise the elements that haven't been overridden will remain uninitialized and basically garbage.
...     
...     for col in range(high.shape[1]):  Iterate over the columns (= assets)
...         out[:, col] = generate_mask_1d_nb(  Select the current column from each array and call our one-dimensional function
...             high[:, col], low[:, col],
...             uband[:, col], mband[:, col], lband[:, col],
...             cond2_th, cond4_th
...         )
...         
...     return out

>>> mask = generate_mask_nb(
...     vbt.to_2d_array(data.get("High")),  Use to_2d_array to cast any array to two dimensions and convert to NumPy
...     vbt.to_2d_array(data.get("Low")),
...     vbt.to_2d_array(bb.upperband),
...     vbt.to_2d_array(bb.middleband),
...     vbt.to_2d_array(bb.lowerband),
...     0.30,
...     0.15
... )
>>> mask = symbol_wrapper.wrap(mask)
>>> mask.sum()
symbol
BTCUSDT    25
ETHUSDT    13
dtype: int64

1d
generate_mask_1d_nb
np.empty
np.full
>>> MaskGenerator = vbt.IF(  Create the facade of the indicator and specify the apply function
...     input_names=["high", "low", "uband", "mband", "lband"],
...     param_names=["cond2_th", "cond4_th"],
...     output_names=["mask"]
... ).with_apply_func(generate_mask_1d_nb, takes_1d=True)  We could have also used generate_mask_nb and takes_1d=False 
>>> mask_generator = MaskGenerator.run(  Run the indicator. Notice that we don't have care about the proper type and shape of each array!
...     data.get("High"),
...     data.get("Low"),
...     bb.upperband,
...     bb.middleband,
...     bb.lowerband,
...     [0.3, 0.4],
...     [0.1, 0.2],
...     param_product=True  Test the Cartesian product of various threshold combinations
... )
>>> mask_generator.mask.sum()
custom_cond2_th  custom_cond4_th  symbol 
0.3              0.1              BTCUSDT    11
                                  ETHUSDT    10
                 0.2              BTCUSDT    28
                                  ETHUSDT    27
0.4              0.1              BTCUSDT     9
                                  ETHUSDT     5
                 0.2              BTCUSDT    26
                                  ETHUSDT    22
dtype: int64

generate_mask_nb
takes_1d=False
>>> @njit
... def value_ago_1d_nb(arr, ago):
...     out = np.empty(arr.shape, dtype=np.float_)  Use np.empty if we can guarantee to override each element in the array. Also remember to set the data type of the array to floating as soon as NaN values are involved!
...     for i in range(out.shape[0]):
...         if i - ago >= 0:  Before accessing the previous element, make sure that it's within the bounds of the array
...             out[i] = arr[i - ago]
...         else:
...             out[i] = np.nan  If the previous element is outside the bounds, set it to NaN
...     return out

>>> arr = np.array([1, 2, 3])
>>> value_ago_1d_nb(arr, 1)
array([nan, 1., 2.])

np.empty
NUMBA_BOUNDSCHECK
>>> @njit
... def any_in_window_1d_nb(arr, window):
...     out = np.empty(arr.shape, dtype=np.bool_)  We can make the empty array boolean because there are no NaN values involved
...     for i in range(out.shape[0]):
...         from_i = max(0, i + 1 - window)  Get the left bound of the window
...         to_i = i + 1  Get the right bound of the window
...         out[i] = np.any(arr[from_i:to_i])  Use the bounds to select all elements in the window and test whether there is a True value among them
...     return out

>>> arr = np.array([False, True, True, False, False])
>>> any_in_window_1d_nb(arr, 2)
array([False, True, True, True, False])

True
>>> @njit
... def any_in_var_window_1d_nb(arr, index, freq):  Take an array of the data type np.datetime64 as index and a constant of the type np.timedelta64 as frequency
...     out = np.empty(arr.shape, dtype=np.bool_)
...     from_i = 0
...     for i in range(out.shape[0]):
...         if index[from_i] <= index[i] - freq:  Test whether the previous left bound is still valid, that is, inside the current time interval
...             for j in range(from_i + 1, index.shape[0]):  If not, search for the current left bound using another loop that iterates over index
...                 if index[j] > index[i] - freq:
...                     from_i = j
...                     break  Once found, set the left bound to from_i and abort the second loop
...         to_i = i + 1
...         out[i] = np.any(arr[from_i:to_i])
...     return out

>>> arr = np.array([False, True, True, False, False])
>>> index = pd.date_range("2020", freq="5min", periods=len(arr)).values  Define an index with the 5-minute timeframe and convert it to a NumPy datetime array
>>> freq = pd.Timedelta("10min").to_timedelta64()  Define a frequency of 10 minutes and convert it to a NumPy timedelta value
>>> any_in_var_window_1d_nb(arr, index, freq)
array([False, True, True, True, False])

np.datetime64
np.timedelta64
from_i
>>> any_in_var_window_1d_nb(arr, vbt.dt.to_ns(index), vbt.dt.to_ns(freq))
array([False, True, True, True, False])

>>> vbt.dt.to_ns(index)  Index as timedelta in nanoseconds after 1970
array([1577836800000000000, 1577837100000000000, 1577837400000000000,
       1577837700000000000, 1577838000000000000])

>>> vbt.dt.to_ns(index - np.datetime64(0, "ns")) This is the same as converting datetime into timedelta by subtracting the Unix Epoch
array([1577836800000000000, 1577837100000000000, 1577837400000000000,
       1577837700000000000, 1577838000000000000])

>>> vbt.dt.to_ns(freq)  Frequency as timedelta in nanoseconds
600000000000

>>> vbt.dt.to_ns(freq) / 1000 / 1000 / 1000 / 60  Convert nanoseconds into minutes
10.0

False
-1
>>> from vectorbtpro.utils import datetime_nb as dt_nb  datetime_nb contains Numba-compiled functions for working datetimes and timedeltas, but mostly requiring them to have the integer representation!

>>> @njit
... def place_func_nb(c, index):  A placement function takes a context and optionally other user-defined arguments
...     last_i = -1  Create a variable to track the position of the last placed signal (if any)
...     for out_i in range(len(c.out)):  Iterate over the output array with a local index
...         i = c.from_i + out_i  Get the global index that we'll use to get the current timestamp. For example, if the array segment has 3 elements (len(out)) and the start index of the segment is 2 (from_i), then the first element corresponds to the position 2 (i) and the last element to the position 5.
...         weekday = dt_nb.weekday_nb(index[i])
...         hour = dt_nb.hour_nb(index[i])
...         if weekday == 1 and hour == 17:  Check whether the current timestamp is 17:00 on Tuesday
...             c.out[out_i] = True  If yes, place a signal using the local index, otherwise continue with the next timestamp
...             last_i = out_i
...     return last_i  Make sure that the returned index is local, not global!

>>> mask = vbt.pd_acc.signals.generate(  Call SignalsAccessor.generate as a class method of the accessor SignalsAccessor
...     symbol_wrapper.shape,  Provide the shape to iterate over
...     place_func_nb,
...     vbt.dt.to_ns(symbol_wrapper.index),  Provide the index in the integer representation (= the total number of nanoseconds)
...     wrapper=symbol_wrapper  Provide the wrapper to wrap the final NumPy array
... )
>>> mask.sum()
symbol
BTCUSDT    0
ETHUSDT    0
dtype: int64

len(out)
from_i
i
>>> @njit
... def place_func_nb(c, index):
...     last_i = -1
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         weekday = dt_nb.weekday_nb(index[i])
...         hour = dt_nb.hour_nb(index[i])
...         if weekday == 1 and hour == 17:
...             c.out[out_i] = True
...             last_i = out_i
...         else:
...             past_target_midnight = dt_nb.past_weekday_nb(index[i], 1)  Get the timestamp of the midnight on the previous Tuesday
...             past_target = past_target_midnight + 17 * dt_nb.h_ns  Get the timestamp of 17:00 on the previous Tuesday
...             if (i > 0 and index[i - 1] < past_target) and \
...                 index[i] > past_target:  Check if the previous timestamp was before and the current timestamp is after the target time
...                 c.out[out_i] = True
...                 last_i = out_i
...     return last_i

>>> mask = vbt.pd_acc.signals.generate(
...     symbol_wrapper.shape,
...     place_func_nb,
...     vbt.dt.to_ns(symbol_wrapper.index),
...     wrapper=symbol_wrapper
... )
>>> mask.sum()
symbol
BTCUSDT    52
ETHUSDT    52
dtype: int64

>>> mask.index[mask.any(axis=1)].strftime('%A %m/%d/%Y')  All the generated signals must be on Wednesdays
Index(['Wednesday 01/06/2021', ..., 'Wednesday 12/29/2021'],
      dtype='object', name='Open time')

mode
FactoryMode.Entries
from_apply_func
run
>>> @njit
... def place_func_nb(c, weekday, hour, index):  Each indicator function must first accept the input shape, then (optionally) any inputs, in-outputs, parameters, and only then additional arguments
...     last_i = -1
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         weekday_now = dt_nb.weekday_nb(index[i])
...         hour_now = dt_nb.hour_nb(index[i])
...         if weekday_now == weekday and hour_now == hour:
...             c.out[out_i] = True
...             last_i = out_i
...     return last_i

>>> EntryGenerator = vbt.SignalFactory(
...     mode="entries",
...     param_names=["weekday", "hour"]
... ).with_place_func(
...     entry_place_func_nb=place_func_nb,  When in the mode FactoryMode.Entries, we need to pass the placement function as entry_place_func_nb
...     entry_settings=dict(  Additionally, we need to provide settings to instruct the custom function to pass specific inputs, in-outputs, and parameters to the placement function. Without it, it will pass nothing!
...         pass_params=["weekday", "hour"],
...     ),
...     var_args=True  Enable variable arguments to be able to pass our index as an additional positional argument
... )
>>> entry_generator = EntryGenerator.run(
...     symbol_wrapper.shape,  Input shape is required if there are no inputs or in-outputs to determine it from
...     1, 
...     [0, 17],  Test two time combinations: 00:00 and 17:00
...     vbt.dt.to_ns(symbol_wrapper.index),  Can be passed thanks to var_args
...     input_index=symbol_wrapper.index,  Pass Pandas metadata for wrapping output arrays
...     input_columns=symbol_wrapper.columns
... )
>>> entry_generator.entries.sum()
custom_weekday  custom_hour   
1               0            0    52
                             1    52
                17           0     0
                             1     0
dtype: int64

FactoryMode.Entries
entry_place_func_nb
var_args
FactoryMode.Entries
plot
>>> entry_generator.plot(column=(2, 0, "BTCUSDT")).show()

from_i
to_i
0
from_i
len(index)
to_i
from_i
to_i
to_i
>>> @njit
... def exit_place_func_nb(c):
...     c.out[0] = True  Place an exit at the first position in the segment
...     return 0

>>> entries = symbol_wrapper.fill(False)
>>> entries.vbt.set(True, every="Q", inplace=True)
>>> entries.index[entries.any(axis=1)]
DatetimeIndex(['2021-03-31 00:00:00+00:00', '2021-06-30 00:00:00+00:00',
               '2021-09-30 00:00:00+00:00', '2021-12-31 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

>>> exits = entries.vbt.signals.generate_exits(exit_place_func_nb)  In contrast to SignalsAccessor.generate, this method is bound to a Pandas object with entries and is using its metadata
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-04-01 00:00:00+00:00', '2021-07-01 00:00:00+00:00',
               '2021-10-01 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

wait
>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     wait=0
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-03-31 00:00:00+00:00', '2021-06-30 00:00:00+00:00',
               '2021-09-30 00:00:00+00:00', '2021-12-31 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

>>> @njit
... def exit_place_func_nb(c, index, wait_td):
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if index[i] >= index[c.from_i] + wait_td:  Check whether a sufficient amount of time has passed since the entry. For c.from_i to be the index of the entry, we must set wait to zero.
...             return out_i  When returning the index, the generator will set the respective element to True automatically
...     return -1

>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     vbt.dt.to_ns(entries.index),  Our additional arguments come here
...     vbt.dt.to_ns(pd.Timedelta("7d")),
...     wait=0
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-04-07 00:00:00+00:00', '2021-07-07 00:00:00+00:00',
               '2021-10-07 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

c.from_i
wait
>>> entries = symbol_wrapper.fill(False)
>>> entries.vbt.set(True, every="5d", inplace=True)
>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     vbt.dt.to_ns(entries.index),
...     vbt.dt.to_ns(pd.Timedelta("7d")),
...     wait=0
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex([], dtype='datetime64[ns, UTC]', name='Open time', freq='D')

until_next
>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     vbt.dt.to_ns(entries.index),
...     vbt.dt.to_ns(pd.Timedelta("7d")),
...     wait=0,
...     until_next=False
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-01-08 00:00:00+00:00', '2021-01-13 00:00:00+00:00',
               '2021-01-18 00:00:00+00:00', '2021-01-23 00:00:00+00:00',
               '2021-01-28 00:00:00+00:00', '2021-02-02 00:00:00+00:00',
               ...
               '2021-12-04 00:00:00+00:00', '2021-12-09 00:00:00+00:00',
               '2021-12-14 00:00:00+00:00', '2021-12-19 00:00:00+00:00',
               '2021-12-24 00:00:00+00:00', '2021-12-29 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

entry1
entry2
exit1
entry3
exit2
entry1
entry2
exit1
entry3
exit2
entry2
skip_until_exit
>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     vbt.dt.to_ns(entries.index),
...     vbt.dt.to_ns(pd.Timedelta("7d")),
...     wait=0,
...     until_next=False,
...     skip_until_exit=True
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-01-08 00:00:00+00:00', '2021-01-18 00:00:00+00:00',
               '2021-01-28 00:00:00+00:00', '2021-02-07 00:00:00+00:00',
               '2021-02-17 00:00:00+00:00', '2021-02-27 00:00:00+00:00',
               ...
               '2021-11-04 00:00:00+00:00', '2021-11-14 00:00:00+00:00',
               '2021-11-24 00:00:00+00:00', '2021-12-04 00:00:00+00:00',
               '2021-12-14 00:00:00+00:00', '2021-12-24 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

skip_until_exit
until_next
FactoryMode.Exits
exit
>>> @njit
... def exit_place_func_nb(c, wait_td, index):  Don't forget to switch the order of parameters and user-defined arguments
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if index[i] >= index[c.from_i] + wait_td:
...             return out_i
...     return -1

>>> ExitGenerator = vbt.SignalFactory(
...     mode="exits",
...     param_names=["wait_td"]
... ).with_place_func(
...     exit_place_func_nb=exit_place_func_nb,
...     exit_settings=dict(
...         pass_params=["wait_td"],
...     ),
...     var_args=True,
...     wait=0,  Parameters for the generator can be passed as regular keyword arguments, also to the run method
...     until_next=False,
...     skip_until_exit=True,
...     param_settings=dict(  Convert the index level with timedeltas to strings to be able to select each value easily. Function post_index_func must take an original index and return a new index.
...         wait_td=dict(
...             post_index_func=lambda x: x.map(lambda y: str(pd.Timedelta(y)))
...         )
...     ),
... )
>>> exit_generator = ExitGenerator.run(
...     entries,  There is no need to provide the input shape and Pandas metadata because both can be derived from the array with entries
...     [
...         pd.Timedelta("3d").to_timedelta64(),  Test two timedelta combinations. The index and timedeltas must be converted to NumPy prior to passing. Here, we can use datetimes and timedeltas directly, that is, without converting them to integers.
...         pd.Timedelta("7d").to_timedelta64()
...     ],
...     symbol_wrapper.index.values
... )
>>> exit_generator.exits.sum()
custom_wait_td   symbol 
3 days 00:00:00  BTCUSDT    73
                 ETHUSDT    73
7 days 00:00:00  BTCUSDT    36
                 ETHUSDT    36
dtype: int64

run
post_index_func
>>> new_entries = exit_generator.entries.vbt.signals.first(  Use SignalsAccessor.first to select the first True in each partition
...     reset_by=exit_generator.exits,  The partition should reset if an exit signal is spotted
...     allow_gaps=True,  The partition is allowed to have False values
... )
>>> new_entries.index[new_entries[("7 days 00:00:00", "BTCUSDT")]]
DatetimeIndex(['2021-01-01 00:00:00+00:00', '2021-01-11 00:00:00+00:00',
               '2021-01-21 00:00:00+00:00', '2021-01-31 00:00:00+00:00',
               '2021-02-10 00:00:00+00:00', '2021-02-20 00:00:00+00:00',
               ...
               '2021-11-17 00:00:00+00:00', '2021-11-27 00:00:00+00:00',
               '2021-12-07 00:00:00+00:00', '2021-12-17 00:00:00+00:00',
               '2021-12-27 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

True
False
>>> @njit
... def entry_place_func_nb(c, low, close, th):
...     if c.from_i == 0:  Place the first entry signal at the first timestamp since there is no prior signal to run the threshold comparison against
...         c.out[0] = True
...         return 0
...     exit_i = c.from_i - c.wait  Get the (global) index of the latest opposite signal using c.from_i - c.wait
...     exit_price = close[exit_i, c.col]  Apply the percentage threshold to the initial close price
...     hit_price = exit_price * (1 - th)
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if low[i, c.col] <= hit_price:  Place an entry signal if the threshold has been crossed downward by the low price
...             return out_i
...     return -1

>>> @njit
... def exit_place_func_nb(c, high, close, th):  The same goes for the exit placement function, but with two differences: an exit is guaranteed to have an entry preceding it, and the threshold is now above the initial close price and should be crossed upward by the high price
...     entry_i = c.from_i - c.wait
...     entry_price = close[entry_i, c.col]
...     hit_price = entry_price * (1 + th)
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if high[i, c.col] >= hit_price:
...             return out_i
...     return -1

>>> entries, exits = vbt.pd_acc.signals.generate_both(  The accessor method is a class method because it isn't based on any other array and requires only the target shape
...     symbol_wrapper.shape,
...     entry_place_func_nb=entry_place_func_nb,
...     entry_place_args=(vbt.Rep("low"), vbt.Rep("close"), 0.1),  We need to distribute three price arrays across two functions: high, low, and close. Use the vectorbt's Rep template to substitute names by their broadcasted arrays
...     exit_place_func_nb=exit_place_func_nb,
...     exit_place_args=(vbt.Rep("high"), vbt.Rep("close"), 0.2),
...     wrapper=symbol_wrapper,
...     broadcast_named_args=dict(  Broadcast all arrays to the target shape
...         high=data.get("High"),
...         low=data.get("Low"),
...         close=data.get("Close")
...     ),
...     broadcast_kwargs=dict(post_func=np.asarray)  Don't forget to convert the broadcasted arrays to NumPy
... )

>>> fig = data.plot(
...     symbol="BTCUSDT", 
...     ohlc_trace_kwargs=dict(opacity=0.5), 
...     plot_volume=False
... )
>>> entries["BTCUSDT"].vbt.signals.plot_as_entries(
...     y=data.get("Close", "BTCUSDT"), fig=fig)
>>> exits["BTCUSDT"].vbt.signals.plot_as_exits(
...     y=data.get("Close", "BTCUSDT"), fig=fig)
>>> fig.show()  Plot the OHLC price data, entries, and exits. Make the OHLC graph more transparent to make the signals clearly visible.

c.from_i - c.wait
FactoryMode.Both
>>> BothGenerator = vbt.SignalFactory(
...     mode="both",
...     input_names=["high", "low", "close"],
...     param_names=["entry_th", "exit_th"]
... ).with_place_func(
...     entry_place_func_nb=entry_place_func_nb,
...     entry_settings=dict(
...         pass_inputs=["low", "close"],
...         pass_params=["entry_th"],
...     ),
...     exit_place_func_nb=exit_place_func_nb,
...     exit_settings=dict(
...         pass_inputs=["high", "close"],
...         pass_params=["exit_th"],
...     )
... )
>>> both_generator = BothGenerator.run(
...     data.get("High"),
...     data.get("Low"),
...     data.get("Close"),
...     [0.1, 0.2],
...     [0.2, 0.3],
...     param_product=True
... )
>>> fig = data.plot(
...     symbol="BTCUSDT", 
...     ohlc_trace_kwargs=dict(opacity=0.5), 
...     plot_volume=False
... )
>>> both_generator.plot(
...     column=(0.1, 0.3, "BTCUSDT"), 
...     entry_y=data.get("Close", "BTCUSDT"), 
...     exit_y=data.get("Close", "BTCUSDT"), 
...     fig=fig
... )
>>> fig.show()

FactoryMode.Exits
skip_until_exit
until_next
FactoryMode.Chain
new_entries
exits
>>> @njit
... def exit_place_func_nb(c, low, request_price, fill_price_out):
...     entry_req_price = request_price[c.from_i - c.wait, c.col]  Get the limit price defined at the entry
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if low[i, c.col] <= entry_req_price:  Check if that price has been hit, and if yes, write the signal and the fill price
...             fill_price_out[i, c.col] = entry_req_price
...             return out_i
...     return -1

>>> ChainGenerator = vbt.SignalFactory(
...     mode="chain",
...     input_names=["low", "request_price"],
...     in_output_names=["fill_price_out"]
... ).with_place_func(  Entry function and its accompanying information are already filled by vectorbt
...     exit_place_func_nb=exit_place_func_nb,
...     exit_settings=dict(
...         pass_inputs=["low", "request_price"],
...         pass_in_outputs=["fill_price_out"],
...     ),
...     fill_price_out=np.nan  Without a default, vectorbt will create the in-output array using np.empty. With a default, vectorbt will create the array using np.full and fill it with the default value. We need the second option since our function does not override each element.
... )

>>> fast_ma = vbt.talib("SMA").run(
...     data.get("Close"), 
...     vbt.Default(10), 
...     short_name="fast_ma"
... )
>>> slow_ma = vbt.talib("SMA").run(
...     data.get("Close"), 
...     vbt.Default(20), 
...     short_name="slow_ma"
... )
>>> entries = fast_ma.real_crossed_above(slow_ma)  Generate a signal array for limit order requests based on moving average crossovers
>>> entries.sum()
symbol
BTCUSDT    10
ETHUSDT     8
dtype: int64

>>> chain_generator = ChainGenerator.run(
...     entries,
...     data.get("Low"),
...     data.get("Close") * (1 - 0.1)  Use 10% below the close price as the limit order price
... )
>>> request_mask = chain_generator.new_entries  New entries contain cleaned order request signals. Many signals were ignored because they come before the limit price of some previous signals could be hit.
>>> request_mask.sum()
symbol
BTCUSDT    4
ETHUSDT    5
dtype: int64

>>> request_price = chain_generator.request_price  This is the input array we passed as the limit order price. We can use the request mask to get the price of each request order.
>>> request_price[request_mask.any(axis=1)]
symbol                       BTCUSDT   ETHUSDT
Open time                                     
2021-02-04 00:00:00+00:00  33242.994  1436.103
2021-03-11 00:00:00+00:00  51995.844  1643.202
2021-04-02 00:00:00+00:00  53055.009  1920.321
2021-06-07 00:00:00+00:00  30197.511  2332.845
2021-06-15 00:00:00+00:00  36129.636  2289.186
2021-07-05 00:00:00+00:00  30321.126  1976.877
2021-07-06 00:00:00+00:00  30798.009  2090.250
2021-07-27 00:00:00+00:00  35512.083  2069.541

>>> fill_mask = chain_generator.exits  Exits contain to-be-filled order signals generated by our exit placement function. Each of such signals has a request counterpart.
>>> fill_mask.sum()
symbol
BTCUSDT    3
ETHUSDT    4
dtype: int64

>>> fill_price = chain_generator.fill_price_out  This is the output array with the order fill price. We can use the fill mask to get the price of each to-be-filled order.
>>> fill_price[fill_mask.any(axis=1)]
symbol                       BTCUSDT   ETHUSDT
Open time                                     
2021-03-24 00:00:00+00:00        NaN  1643.202
2021-05-19 00:00:00+00:00  33242.994  1920.321
2021-06-08 00:00:00+00:00        NaN  2332.845
2021-06-18 00:00:00+00:00  36129.636       NaN
2021-07-13 00:00:00+00:00        NaN  1976.877
2021-07-19 00:00:00+00:00  30798.009       NaN

np.empty
np.full
BTCUSDT
2021-02-04
2021-05-19
ETHUSDT
2021-03-11
2021-03-24
fill_mask
fill_mask
FactoryMode.Exits
X
NX
CX
n
prob
>>> btcusdt_wrapper = symbol_wrapper["BTCUSDT"]
>>> mask = vbt.pd_acc.signals.generate_random(
...     btcusdt_wrapper.shape,
...     prob=1 / 10,
...     wrapper=btcusdt_wrapper,
...     seed=42  Make the calculation deterministic
... )
>>> mask_index = mask.index[mask]
>>> (mask_index[1:] - mask_index[:-1]).mean()  Compute the average distance between two neighboring signals in the mask
Timedelta('8 days 03:20:55.813953488')

>>> monday_mask = btcusdt_wrapper.fill(False)
>>> monday_mask.vbt.set(True, every="W-MON", inplace=True)  Set True on each Monday using BaseAccessor.set
>>> mask = monday_mask.vbt.signals.generate_random_exits(wait=0)  Generate exactly one signal after the previous Monday and before the next one. Using wait=0, we're allowing to place a signal right after Monday midnight.
>>> mask_index = mask.index[mask]
>>> mask_index.strftime("%W %A")  Print out the week number and the weekday of each signal
Index(['01 Tuesday', '02 Wednesday', '03 Wednesday', '04 Friday', '05 Friday',
       '06 Tuesday', '07 Thursday', '08 Tuesday', '09 Friday', '10 Saturday',
       '11 Friday', '12 Saturday', '13 Monday', '14 Friday', '15 Monday',
       ...
       '41 Wednesday', '42 Friday', '43 Thursday', '44 Sunday', '45 Sunday',
       '46 Sunday', '47 Saturday', '48 Saturday', '49 Tuesday', '50 Thursday',
       '51 Sunday', '52 Tuesday'],
      dtype='object', name='Open time')

True
wait=0
RAND
RPROB
n
prob
>>> prob = np.linspace(0, 1, len(symbol_wrapper.index))  Use numpy.linspace to fill the probability values between two extremes
>>> rprob = vbt.RPROB.run(
...     symbol_wrapper.shape,  We need to provide the input shape since the indicator doesn't take any input arrays
...     vbt.Default(vbt.to_2d_pr_array(prob)),  Use to_2d_pr_array to expand the array to two dimensions such that each value corresponds to a row rather than a column (refer to broadcasting rules). Wrap it with Default to hide the parameter.
...     seed=42,
...     input_index=symbol_wrapper.index,
...     input_columns=symbol_wrapper.columns
... )
>>> rprob.entries.astype(int).vbt.ts_heatmap().show()  Plot the final distribution

>>> rprob = vbt.RPROB.run(
...     symbol_wrapper.shape,
...     [0.5, vbt.to_2d_pr_array(prob)],
...     seed=42,
...     input_index=symbol_wrapper.index,
...     input_columns=symbol_wrapper.columns
... )
>>> rprob.entries.sum()
rprob_prob  symbol 
0.5         BTCUSDT    176
            ETHUSDT    187
array_0     BTCUSDT    183
            ETHUSDT    178
dtype: int64

entries
entry_ts
ts
stop
>>> new_entries, exits = entries.vbt.signals.generate_stop_exits(
...     data.get("Close"),
...     data.get("High"),
...     stop=0.1,
...     chain=True
... )
>>> new_entries[new_entries.any(axis=1)]
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-04 00:00:00+00:00     True    False
2021-03-10 00:00:00+00:00     True    False
...
2021-11-07 00:00:00+00:00     True    False
2021-12-02 00:00:00+00:00    False     True

>>> exits[exits.any(axis=1)]
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-06 00:00:00+00:00     True    False
2021-03-13 00:00:00+00:00     True    False
...
2021-10-15 00:00:00+00:00    False     True
2021-10-19 00:00:00+00:00     True    False

stop_ts
out_dict
out_dict
np.nan
>>> out_dict = {}
>>> new_entries, exits = entries.vbt.signals.generate_stop_exits(
...     data.get("Close"),
...     data.get("High"),
...     stop=0.1,
...     chain=True,
...     out_dict=out_dict
... )
>>> out_dict["stop_ts"][exits.any(axis=1)]
symbol                       BTCUSDT   ETHUSDT
Open time                                     
2021-02-06 00:00:00+00:00  40630.326       NaN
2021-03-13 00:00:00+00:00  61436.749       NaN
...
2021-10-15 00:00:00+00:00        NaN  3866.797
2021-10-19 00:00:00+00:00  63179.721       NaN

stop_ts
out_dict
follow_ts
wait
>>> stcx = vbt.STCX.run(  Indicators usually assume that we're interested in all optional arrays, so we don't need to pass anything. If you're not interested though, pass stop_ts=None to free up a bit of RAM.
...     entries,
...     data.get("Open"),
...     ts=data.get("Low"),
...     follow_ts=data.get("High"),
...     stop=-0.1,  For a downward crossover condition, specify a negative stop value
...     trailing=[False, True],  Test SL and TSL
...     wait=0  Waiting time of zero should only be specified when the entry price is the open price!
... )
>>> fig = data.plot(
...     symbol="BTCUSDT", 
...     ohlc_trace_kwargs=dict(opacity=0.5), 
...     plot_volume=False
... )
>>> stcx.plot(
...     column=(-0.1, True, "BTCUSDT"), 
...     entry_y="entry_ts",  When plotting, we can provide y as an attribute of the indicator instance
...     exit_y="stop_ts", 
...     fig=fig
... )
>>> fig.show()

stop_ts=None
y
entry_ts
entry_ts
>>> ohlcstcx = vbt.OHLCSTCX.run(
...     entries,
...     data.get("Close"),  Entry price. Here, we use the close price since we've generated the entry array using it.
...     data.get("Open"),  OHLC
...     data.get("High"),
...     data.get("Low"),
...     data.get("Close"),
...     sl_stop=vbt.Default(0.1),  Stop parameters
...     tsl_stop=vbt.Default(0.15),
...     is_entry_open=False  Entry price is the close price. Enable this flag if the entry price is the open price.
... )
>>> ohlcstcx.plot(column=("BTCUSDT")).show()  The indicator instance plots the candlestick data automatically!

wait=1
is_entry_open=True
wait
stop_type
stop_type_readable
>>> ohlcstcx.stop_type_readable[ohlcstcx.exits.any(axis=1)]
symbol                    BTCUSDT ETHUSDT
Open time                                
2021-02-22 00:00:00+00:00     TSL    None
2021-03-23 00:00:00+00:00    None     TSL
2021-03-24 00:00:00+00:00     TSL    None
2021-04-18 00:00:00+00:00      SL     TSL
2021-05-12 00:00:00+00:00      SL    None
2021-06-08 00:00:00+00:00    None      SL
2021-06-18 00:00:00+00:00      SL    None
2021-07-09 00:00:00+00:00    None     TSL
2021-07-19 00:00:00+00:00      SL    None
2021-09-07 00:00:00+00:00     TSL     TSL
2021-11-16 00:00:00+00:00     TSL     TSL
2021-12-03 00:00:00+00:00    None      SL
2021-12-29 00:00:00+00:00    None      SL
2021-12-31 00:00:00+00:00      SL    None

ts
follow_ts
entry_ts
follow_ts
entry_price
is_entry_open
high
open
close
>>> ohlcstcx = vbt.OHLCSTCX.run(
...     entries,
...     data.get("Close"),
...     sl_stop=vbt.Default(0.1),
...     tsl_stop=vbt.Default(0.15),
...     is_entry_open=False
... )
>>> ohlcstcx.plot(column=("BTCUSDT")).show()

>>> entry_pos_rank = entries.vbt.signals.pos_rank(allow_gaps=True)  Rank each entry signal by its position among all entry signals using SignalsAccessor.pos_rank
>>> short_entries = (entry_pos_rank >= 0) & (entry_pos_rank % 2 == 1)  Select only those entries whose position is odd, that is, doesn't divide by 2. Those will become our short entries.

>>> ohlcstcx = vbt.OHLCSTCX.run(
...     entries,
...     data.get("Close"),
...     data.get("Open"),
...     data.get("High"),
...     data.get("Low"),
...     data.get("Close"),
...     tsl_th=vbt.Default(0.2),  TTP information consists of two parts: a take profit threshold (tsl_th) that needs to be crossed upwards, and a trailing stop loss (tsl_stop) that becomes enabled once the threshold has been crossed
...     tsl_stop=vbt.Default(0.1),
...     reverse=vbt.Default(short_entries),  Short entry mask becomes our reversal mask
...     is_entry_open=False
... )
>>> ohlcstcx.plot(column=("BTCUSDT")).show()

tsl_th
tsl_stop
>>> long_entries = ohlcstcx.new_entries.vbt & (~short_entries)  We cannot use our original entries array here since the generator ignored some entries coming too early. But since new entries retain their positions, we can easily identify long signals by inverting the short array and combining it with the new entry array via the AND rule.
>>> long_exits = ohlcstcx.exits.vbt.signals.first_after(long_entries)  A nice property of the chained exits mode is that each exit is guaranteed to come right after its entry - there are no other entries in-between, thus we can use SignalsAccessor.first_after for this job
>>> short_entries = ohlcstcx.new_entries.vbt & short_entries
>>> short_exits = ohlcstcx.exits.vbt.signals.first_after(short_entries)

>>> fig = data.plot(
...     symbol="BTCUSDT", 
...     ohlc_trace_kwargs=dict(opacity=0.5), 
...     plot_volume=False
... )
>>> long_entries["BTCUSDT"].vbt.signals.plot_as_entries(
...     ohlcstcx.entry_price["BTCUSDT"],
...     trace_kwargs=dict(marker=dict(color="limegreen"), name="Long entries"), 
...     fig=fig
... )
>>> long_exits["BTCUSDT"].vbt.signals.plot_as_exits(
...     ohlcstcx.stop_price["BTCUSDT"],
...     trace_kwargs=dict(marker=dict(color="orange"), name="Long exits"),
...     fig=fig
... )
>>> short_entries["BTCUSDT"].vbt.signals.plot_as_entries(
...     ohlcstcx.entry_price["BTCUSDT"],
...     trace_kwargs=dict(marker=dict(color="magenta"), name="Short entries"),
...     fig=fig
... )
>>> short_exits["BTCUSDT"].vbt.signals.plot_as_exits(
...     ohlcstcx.stop_price["BTCUSDT"],
...     trace_kwargs=dict(marker=dict(color="red"), name="Short exits"),
...     fig=fig
... )
>>> fig.show()

entries
[0.1, -0.1, np.nan]
[11, 10, 12]
entries.astype(int) - exits.astype(int)
init_position
cash_deposits
size
save_returns
>>> import numpy as np
>>> import pandas as pd
>>> import vectorbtpro as vbt
>>> from vectorbtpro.portfolio import nb as pf_nb
>>> from vectorbtpro.portfolio import enums as pf_enums

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(3, 1),  Target shape must be two-dimensional, 3 and (3,) are not allowed
...     group_lens=np.array([1]),  We must provide a one-dimensional group lengths array, which without column grouping is just an array of ones (one group per column)
...     size=np.array([[0.1], [-0.1], [np.nan]]),  Make size and price arrays of the same shape as target_shape for now
...     price=np.array([[11], [10], [12]])
... )
>>> sim_out.order_records
array([(0, 0, 0, 0.1, 11., 0., 0), (1, 0, 1, 0.1, 10., 0., 1)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})

3
(3,)
target_shape
>>> print(vbt.prettify(sim_out))
SimulationOutput(
    order_records=<numpy.ndarray object at 0x7f88606d5710 of shape (2,)>,
    log_records=<numpy.ndarray object at 0x7f8860907fa8 of shape (0,)>,
    cash_deposits=<numpy.ndarray object at 0x7f8860907f50 of shape (1, 1)>,
    cash_earnings=<numpy.ndarray object at 0x7f8860a355b0 of shape (1, 1)>,
    call_seq=None,
    in_outputs=FSInOutputs(
        returns=<numpy.ndarray object at 0x7f88a1976fa8 of shape (0, 0)>
    )
)

None
>>> def print_orders(target_shape, order_records):
...     wrapper = vbt.ArrayWrapper.from_shape(target_shape)
...     print(vbt.Orders(wrapper, order_records).records_readable)

>>> print_orders((3, 1), sim_out.order_records)
   Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0   0.1   11.0   0.0   Buy
1         1       0          1   0.1   10.0   0.0  Sell

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(3, 1),
...     group_lens=np.array([1]),
...     size=np.array([0.1, -0.1, np.nan]),
...     price=np.array([11, 10, 12]),
...     fees=0.01
... )
>>> print_orders((3, 1), sim_out.order_records)
   Order Id  Column  Timestamp  Size  Price   Fees  Side
0         0       0          0   0.1   11.0  0.011   Buy
1         1       0          1   0.1   10.0  0.010  Sell

(3,)
(3, 1)
>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(3, 3),  Shape (n, m) resulting from imaginary broadcasting all the passed arrays
...     group_lens=np.array([1, 1, 1]),  Three groups, one per column
...     size=np.array([[np.inf, np.nan, -np.inf]]),  Arrays with the shape (1, m) broadcast along columns. Size of np.inf will buy as much as possible at every timestamp, size of np.nan will be ignored at every timestamp, and size of -np.inf will short sell as much as possible at every timestamp.
...     price=np.array([11, 10, 12]),  Arrays with the shape (n,) or (n, 1) broadcast along rows
...     fees=0.01  Scalars and arrays with the shape (1,) or (1, 1) broadcast along columns and rows
... )
>>> print_orders((3, 3), sim_out.order_records)
   Order Id  Column  Timestamp    Size  Price      Fees  Side
0         0       0          0  9.0009   11.0  0.990099   Buy
1         0       2          0  9.0009   11.0  0.990099  Sell

(n, m)
(1, m)
np.inf
np.nan
-np.inf
(n,)
(n, 1)
(1,)
(1, 1)
>>> size, price, fees = vbt.broadcast_arrays(  
...     np.array([[np.inf, np.nan, -np.inf]]),
...     np.array([11, 10, 12]),
...     0.01
... )
>>> size
array([[ inf,  nan, -inf],
       [ inf,  nan, -inf],
       [ inf,  nan, -inf]])

>>> price
array([[11, 11, 11],
       [10, 10, 10],
       [12, 12, 12]])

>>> fees
array([[0.01, 0.01, 0.01],
       [0.01, 0.01, 0.01],
       [0.01, 0.01, 0.01]])

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=size.shape,  Use broadcast_arrays
...     group_lens=np.full(size.shape[1], 1),
...     size=size,
...     price=price,
...     fees=fees
... )
>>> print_orders(size.shape, sim_out.order_records)
   Order Id  Column  Timestamp    Size  Price      Fees  Side
0         0       0          0  9.0009   11.0  0.990099   Buy
1         0       2          0  9.0009   11.0  0.990099  Sell

(3, 3)
np.broadcast_arrays
group_lens
>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(1, 2),  One row (bar) and two columns (assets)
...     group_lens=np.array([2]),  Build a group with cash sharing consisting of two columns
...     size=np.array([[np.inf, np.inf]]),  Provide order information per element by creating an array of the same shape as target_shape
...     price=np.array([[10, 5]])
... )
>>> print_orders((1, 2), sim_out.order_records)
   Order Id  Column  Timestamp  Size  Price  Fees Side
0         0       0          0  10.0   10.0   0.0  Buy

target_shape
>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(2, 2),
...     group_lens=np.array([2]),
...     size=np.array([[0, 1], [1, 0]]),
...     size_type=pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 5], [11, 4]])
... )
>>> print_orders((2, 2), sim_out.order_records)
   Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       1          0  20.0    5.0   0.0   Buy
1         1       1          1  20.0    4.0   0.0  Sell

1
>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(2, 2),
...     group_lens=np.array([2]),
...     size=np.array([[0, 1], [1, 0]]),
...     size_type=pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 5], [11, 4]]),
...     call_seq=np.array([[0, 1], [1, 0]])  First timestamp: first process the first asset then the second. Second timestamp: first process the second asset then the first. Note that a call sequence must provide exactly one position per timestamp and asset, thus its shape must follow target_shape.
... )
>>> print_orders((2, 2), sim_out.order_records)
   Order Id  Column  Timestamp       Size  Price  Fees  Side
0         0       0          1   7.272727   11.0   0.0   Buy
1         0       1          0  20.000000    5.0   0.0   Buy
2         1       1          1  20.000000    4.0   0.0  Sell

target_shape
None
auto_call_seq
>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(2, 2),
...     group_lens=np.array([2]),
...     size=np.array([[0, 1], [1, 0]]),
...     size_type=pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 5], [11, 4]]),
...     auto_call_seq=True  Here
... )
>>> print_orders((2, 2), sim_out.order_records)
   Order Id  Column  Timestamp       Size  Price  Fees  Side
0         0       0          1   7.272727   11.0   0.0   Buy
1         0       1          0  20.000000    5.0   0.0   Buy
2         1       1          1  20.000000    4.0   0.0  Sell

n
n
>>> from vectorbtpro.portfolio.call_seq import build_call_seq

>>> call_seq = build_call_seq(
...     target_shape=(2, 2), 
...     group_lens=np.array([2]), 
...     call_seq_type=pf_enums.CallSeqType.Default  If we want for vectorbt to sort the call sequence using auto_call_seq, it must be of the type Default
... )

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(2, 2),
...     group_lens=np.array([2]),
...     size=np.array([[0, 1], [1, 0]]),
...     size_type=pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 5], [11, 4]]),
...     call_seq=call_seq,
...     auto_call_seq=True
... )
>>> sim_out.call_seq
array([[0, 1],
       [1, 0]])

auto_call_seq
Default
None
auto_call_seq
returns
>>> data = vbt.YFData.pull("BTC-USD", end="2022-01-01")
>>> symbol_wrapper = data.get_symbol_wrapper()

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=symbol_wrapper.shape_2d,  Use the shape of the data as the target shape. Make sure that the columns are symbols and the shape is two-dimensional.
...     group_lens=np.array([1]),
...     open=data.get("Open").values,  Pass OHLC data as separate arrays. Passing the open, high, and low price is optional.
...     high=data.get("High").values,
...     low=data.get("Low").values,
...     close=data.get("Close").values,
...     save_returns=True  Tell vectorbt to calculate the portfolio value and return at each timestamp
... )
>>> returns = symbol_wrapper.wrap(sim_out.in_outputs.returns)  Extract the return series and convert it into a Pandas object
>>> returns
Date
2014-09-17 00:00:00+00:00    0.000000
2014-09-18 00:00:00+00:00   -0.071926
2014-09-19 00:00:00+00:00   -0.069843
...                               ...
2021-12-29 00:00:00+00:00   -0.024042
2021-12-30 00:00:00+00:00    0.015791
2021-12-31 00:00:00+00:00   -0.018476
Freq: D, Name: BTC-USD, Length: 2663, dtype: float64

>>> data.get("Close").vbt.to_returns()  Buy-and-hold produces the same returns as the close price itself
Date
2014-09-17 00:00:00+00:00    0.000000
2014-09-18 00:00:00+00:00   -0.071926
2014-09-19 00:00:00+00:00   -0.069843
...                               ...
2021-12-29 00:00:00+00:00   -0.024042
2021-12-30 00:00:00+00:00    0.015791
2021-12-31 00:00:00+00:00   -0.018476
Freq: D, Name: Close, Length: 2663, dtype: float64

group_lens
>>> mult_data = vbt.YFData.pull(  Fetch two columns (assets) of data
...     ["BTC-USD", "ETH-USD"], 
...     end="2022-01-01",
...     missing_index="drop"
... )
>>> mult_symbol_wrapper = mult_data.get_symbol_wrapper()

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([2]),  Build a single group out of two columns
...     close=mult_data.get("Close").values,
...     size=np.array([[0.5, 0.5]]),  Define an allocation per column, such that the array nicely broadcasts against the target shape
...     size_type=pf_enums.SizeType.TargetPercent,
...     save_returns=True
... )
>>> returns = mult_symbol_wrapper\
...     .replace(columns=["group"], ndim=1)\
...     .wrap(sim_out.in_outputs.returns)  Our wrapper holds metadata per column while the returned array is per group - replace the columns with the single group we created above
>>> returns
Date
2017-11-09 00:00:00+00:00    0.000000
2017-11-10 00:00:00+00:00   -0.070482
2017-11-11 00:00:00+00:00    0.006159
...                               ...
2021-12-29 00:00:00+00:00   -0.034684
2021-12-30 00:00:00+00:00    0.019652
2021-12-31 00:00:00+00:00   -0.013406
Freq: D, Name: group, Length: 1514, dtype: float64

init_cash
group_lens
>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(1, 4),
...     group_lens=np.array([2, 1, 1]),
...     init_cash=100,  Make $100 the starting cash of all three groups
...     size=np.array([[0.5, 0.5, 1.0, 1.0]]),  Evenly split the cash among the two columns in the first group. Remember that size and other order information must be provided per column (asset), not per group!
...     size_type=pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 11, 10, 11]]),
... )
>>> print_orders((1, 4), sim_out.order_records)
   Order Id  Column  Timestamp       Size  Price  Fees Side
0         0       0          0   5.000000   10.0   0.0  Buy
1         0       1          0   4.545455   11.0   0.0  Buy
2         0       2          0  10.000000   10.0   0.0  Buy
3         0       3          0   9.090909   11.0   0.0  Buy

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=(1, 4),
...     group_lens=np.array([2, 1, 1]),
...     init_cash=np.array([200, 100, 100]),  Provide the initial cash as an array with values per group
...     size=np.array([[0.5, 0.5, 1.0, 1.0]]),
...     size_type=pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 11, 10, 11]]),
... )
>>> print_orders((1, 4), sim_out.order_records)
   Order Id  Column  Timestamp       Size  Price  Fees Side
0         0       0          0  10.000000   10.0   0.0  Buy
1         0       1          0   9.090909   11.0   0.0  Buy
2         0       2          0  10.000000   10.0   0.0  Buy
3         0       3          0   9.090909   11.0   0.0  Buy

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([1, 1]),
...     init_position=np.array([1, 1]),  Initial position must be provided per column (asset)
...     close=mult_data.get("Close").values,
...     save_returns=True
... )
>>> returns = mult_symbol_wrapper.wrap(sim_out.in_outputs.returns)
>>> returns
symbol                      BTC-USD   ETH-USD
Date                                         
2017-11-09 00:00:00+00:00       NaN       NaN
2017-11-10 00:00:00+00:00 -0.073554 -0.067411
2017-11-11 00:00:00+00:00 -0.039368  0.051555
...                             ...       ...
2021-12-29 00:00:00+00:00 -0.024042 -0.045348
2021-12-30 00:00:00+00:00  0.015791  0.023514
2021-12-31 00:00:00+00:00 -0.018476 -0.008406

[1514 rows x 2 columns]

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([1, 1]),
...     init_position=np.array([1, 1]),
...     init_price=mult_data.get("Open").values[0],  Entry price of the initial position must also be provided per column (asset)
...     close=mult_data.get("Close").values,
...     save_returns=True
... )
>>> returns = mult_symbol_wrapper.wrap(sim_out.in_outputs.returns)
>>> returns
symbol                      BTC-USD   ETH-USD
Date                                         
2017-11-09 00:00:00+00:00 -0.040182  0.029950
2017-11-10 00:00:00+00:00 -0.073554 -0.067411
2017-11-11 00:00:00+00:00 -0.039368  0.051555
...                             ...       ...
2021-12-29 00:00:00+00:00 -0.024042 -0.045348
2021-12-30 00:00:00+00:00  0.015791  0.023514
2021-12-31 00:00:00+00:00 -0.018476 -0.008406

[1514 rows x 2 columns]

target_shape
group_lens.shape
FlexArray1dLike
FlexArray2dLike
FlexArray1d
FlexArray2d
target_shape
Array1d
Array2d
init_cash
cash_deposits
>>> cash_deposits = symbol_wrapper.fill(0)  Create an array with the same shape as our data, fill it with zeros, and set the elements at an annual frequency to 100
>>> cash_deposits.vbt.set(100, every="YS", inplace=True)

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=symbol_wrapper.shape_2d,
...     group_lens=np.array([1]),
...     cash_deposits=cash_deposits.values,
...     close=data.get("Close").values
... )
>>> print_orders(symbol_wrapper.shape_2d, sim_out.order_records)
   Order Id  Column  Timestamp      Size         Price  Fees Side
0         0       0          0  0.218659    457.334015   0.0  Buy
1         1       0        106  0.318219    314.248993   0.0  Buy
2         2       0        471  0.230238    434.334015   0.0  Buy
3         3       0        837  0.100168    998.325012   0.0  Buy
4         4       0       1202  0.007322  13657.200195   0.0  Buy
5         5       0       1567  0.026018   3843.520020   0.0  Buy
6         6       0       1932  0.013889   7200.174316   0.0  Buy
7         7       0       2298  0.003404  29374.152344   0.0  Buy

cash_deposits
>>> cash_deposits = mult_symbol_wrapper\
...     .replace(columns=["group"], ndim=1)\
...     .fill(0)  Create an array with one column for our group
>>> cash_deposits.vbt.set(100, every="YS", inplace=True)
>>> size = mult_symbol_wrapper.fill(np.nan)  Create a size array to allocate 50% of the deposited cash to each asset
>>> size.vbt.set(0.5, every="YS", inplace=True)
>>> size.iloc[0] = 0.5  We have also initial cash, thus allocate at the first timestamp too

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([2]),
...     cash_deposits=cash_deposits.values,
...     close=mult_data.get("Close").values,
...     size=size.values,
...     size_type=pf_enums.SizeType.TargetPercent
... )
>>> print_orders(mult_symbol_wrapper.shape_2d, sim_out.order_records)
   Order Id  Column  Timestamp      Size         Price  Fees  Side
0         0       0          0  0.006999   7143.580078   0.0   Buy
1         1       0         53  0.004569  13657.200195   0.0   Buy
2         2       0        418  0.010971   3843.520020   0.0   Buy
3         3       0        783  0.001263   7200.174316   0.0   Buy
4         4       0       1149  0.003404  29374.152344   0.0   Buy
5         0       1          0  0.155820    320.884003   0.0   Buy
6         1       1         53  0.048663    772.640991   0.0   Buy
7         2       1        418  0.410697    140.819412   0.0   Buy
8         3       1        783  0.695013    130.802002   0.0   Buy
9         4       1       1149  0.108007    730.367554   0.0  Sell

>>> size = symbol_wrapper.fill(np.nan)  Create a size array to sell 10% of the current position at an annual frequency
>>> size.vbt.set(-0.1, every="YS", inplace=True)

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=symbol_wrapper.shape_2d,
...     group_lens=np.array([1]),
...     init_position=1,
...     cash_deposits=-np.inf,  Withdraw as much as possible at each timestamp. Remember that most arrays can be provided as a scalar to be applied on each element using flexible indexing.
...     close=data.get("Close").values,
...     size=size.values,
...     size_type=pf_enums.SizeType.Percent,  Our size means the percentage of the current position
...     direction=pf_enums.Direction.LongOnly  Make each order long-only to avoid going into the short direction
... )
>>> print_orders(symbol_wrapper.shape_2d, sim_out.order_records)
   Order Id  Column  Timestamp      Size         Price  Fees  Side
0         0       0        106  0.100000    314.248993   0.0  Sell
1         1       0        471  0.090000    434.334015   0.0  Sell
2         2       0        837  0.081000    998.325012   0.0  Sell
3         3       0       1202  0.072900  13657.200195   0.0  Sell
4         4       0       1567  0.065610   3843.520020   0.0  Sell
5         5       0       1932  0.059049   7200.174316   0.0  Sell
6         6       0       2298  0.053144  29374.152344   0.0  Sell

>>> cash_deposits = symbol_wrapper.wrap(sim_out.cash_deposits)  Access and wrap the cash deposits
>>> print(cash_deposits[cash_deposits != 0])  Positive cash deposits mean money inflow, negative cash deposits mean money outflow
Date
2014-09-17 00:00:00+00:00    -100.000000
2015-01-02 00:00:00+00:00     -31.424899
2016-01-02 00:00:00+00:00     -39.090061
2017-01-02 00:00:00+00:00     -80.864326
2018-01-02 00:00:00+00:00    -995.609894
2019-01-02 00:00:00+00:00    -252.173348
2020-01-02 00:00:00+00:00    -425.163093
2021-01-02 00:00:00+00:00   -1561.062890
Name: BTC-USD, dtype: float64

-np.inf
cash_earnings
cash_dividends
>>> aapl_data = vbt.YFData.pull("AAPL", end="2022-01-01")
>>> aapl_wrapper = aapl_data.get_symbol_wrapper()
>>> size = aapl_wrapper.fill()  Create a size array for buying the maximum amount at the first bar (np.inf) and then holding (np.nan)
>>> size.iloc[0] = np.inf

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=aapl_wrapper.shape_2d,
...     group_lens=np.array([1]),
...     close=aapl_data.get("Close").values,
...     cash_dividends=aapl_data.get("Dividends").values,
...     size=size.values
... )
>>> print_orders(aapl_wrapper.shape_2d, sim_out.order_records)
   Order Id  Column  Timestamp        Size     Price  Fees Side
0         0       0          0  996.754204  0.100326   0.0  Buy

>>> cash_earnings = aapl_wrapper.wrap(sim_out.cash_earnings)
>>> print(cash_earnings[cash_earnings != 0])  Positive cash earnings mean money inflow, negative cash earnings mean money outflow
Date
1987-05-11 00:00:00+00:00      0.534260
1987-08-10 00:00:00+00:00      0.534260
1987-11-17 00:00:00+00:00      0.711683
...                                 ...
2021-05-07 00:00:00+00:00    219.285925
2021-08-06 00:00:00+00:00    219.285925
2021-11-05 00:00:00+00:00    219.285925
Name: AAPL, Length: 73, dtype: float64

np.inf
np.nan
target_shape
max_order_records
max_log_records
>>> target_shape = (1000000, 1)
>>> np.random.seed(42)  Set a seed to make the results reproducible
>>> rand_price = np.random.randint(8, 12, size=target_shape)  Quickly generate a random time series acting as a price
>>> size = np.full(target_shape, np.nan)
>>> size[0] = np.inf
>>> size[-1] = -np.inf

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=target_shape,
...     group_lens=np.array([1]),
...     price=rand_price,
...     size=size,
...     max_order_records=2  We expect to fill just two orders
... )
>>> print_orders(target_shape, sim_out.order_records)
   Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0  10.0   10.0   0.0   Buy
1         1       0     999999  20.0    8.0   0.0  Sell

>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=target_shape,
...     group_lens=np.array([1]),
...     price=rand_price,
...     size=size,
...     max_order_records=1  Reduce the maximum count of order records to 1
... )
IndexError: order_records index out of range. Set a higher max_order_records.

max_order_records
>>> sim_out = pf_nb.from_orders_nb(
...     target_shape=target_shape,
...     group_lens=np.array([1]),
...     price=rand_price,
...     size=size,
...     max_order_records=0
... )
>>> print_orders(target_shape, sim_out.order_records)
Empty DataFrame
Columns: [Order Id, Column, Timestamp, Size, Price, Fees, Side]
Index: []

max_order_records
max_log_records
skipna
>>> %%timeit
>>> pf_nb.from_orders_nb(
...     target_shape=target_shape,
...     group_lens=np.array([1]),
...     price=rand_price,
...     size=size,
...     max_order_records=2
... )
42.1 ms ± 641 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> %%timeit
>>> pf_nb.from_orders_nb(
...     target_shape=target_shape,
...     group_lens=np.array([1]),
...     price=rand_price,
...     size=size,
...     max_order_records=2,
...     ffill_val_price=False,
...     skipna=True
... )
2.18 ms ± 80.3 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

@njit
>>> f_py = vbt.jit_reg.resolve_option(
...     task_id=pf_nb.from_orders_nb, 
...     option=False  Option gets resolved using resolve_jitted_option
... )

>>> f_no_cache = vbt.jit_reg.resolve_option(
...     task_id=pf_nb.from_orders_nb, 
...     option=dict(cache=False)
... )

>>> f_parallel = vbt.jit_reg.resolve_option(
...     task_id=pf_nb.from_orders_nb, 
...     option=dict(parallel=True)
... )

from_orders_nb
>>> f_chunked = vbt.ch_reg.resolve_option(
...     setup_id_or_func=pf_nb.from_orders_nb, 
...     option=True  Option gets resolved using resolve_chunked_option
... )
>>> print(vbt.prettify(f_chunked.options))
Config(
    n_chunks=None,
    size=ArraySizer(
        arg_query='group_lens',
        single_type=None,
        axis=0
    ),
    min_size=None,
    chunk_len=None,
    chunk_meta=None,
    skip_single_chunk=None,
    arg_take_spec={...},
    template_context=None,
    engine=None,
    engine_kwargs={},
    merge_func=<function merge_sim_outs at 0x7f88873f1ea0>,
    merge_kwargs={...},
    return_raw_chunks=False,
    silence_warnings=None,
    disable=None
)

from_orders_nb
group_lens
[1]
[1]
>>> sim_out = f_chunked(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([1, 1]),
...     close=mult_data.get("Close").values,
...     _n_chunks=2,  Add an underscore to an argument to override a setting with the same name in chunked
...     _engine="dask"
... )
>>> print_orders(mult_symbol_wrapper.shape_2d, sim_out.order_records)
   Order Id  Column  Timestamp      Size        Price  Fees Side
0         0       0          0  0.013999  7143.580078   0.0  Buy
1         0       1          0  0.311639   320.884003   0.0  Buy

>>> pf = vbt.Portfolio.from_orders(
...     close=[11, 10, 12],
...     size=[0.1, -0.1, np.nan]
... )
>>> pf.orders.records_readable
   Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0   0.1   11.0   0.0   Buy
1         1       0          1   0.1   10.0   0.0  Sell

>>> pf = vbt.Portfolio.from_orders(10, 1)  Any array-like argument gets converted into a NumPy array
>>> pf.orders.records_readable
   Order Id  Column  Timestamp  Size  Price  Fees Side
0         0       0          0   1.0   10.0   0.0  Buy

None
None
>>> vbt.phelp(vbt.Portfolio.from_orders, incl_doc=False)
Portfolio.from_orders(
    close,
    size=None,
    size_type=None,
    direction=None,
    price=None,
    fees=None,
    ...
    jitted=None,
    chunked=None,
    wrapper_kwargs=None,
    freq=None,
    bm_close=None,
    **kwargs
)

>>> vbt.settings.portfolio["price"]
inf

>>> vbt.settings.portfolio["fixed_fees"] = 1

>>> pf = vbt.Portfolio.from_orders(
...     close=pd.Series([11, 10, 12]),
...     size=pd.Series([0.1, -0.1, np.nan])
... )
>>> pf.orders.records_readable
   Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0   0.1   11.0   1.0   Buy
1         1       0          1   0.1   10.0   1.0  Sell
                                               ^
                                              here

>>> vbt.settings.portfolio.reset()

>>> vbt.settings.portfolio["fixed_fees"]
0.0

price
np.array(np.inf)
None
>>> print(vbt.prettify(pf_enums.SizeType))
SizeTypeT(
    Amount=0,
    Value=1,
    Percent=2,
    TargetAmount=3,
    TargetValue=4,
    TargetPercent=5
)

>>> vbt.map_enum_fields("targetamount", pf_enums.SizeType)
3

>>> vbt.map_enum_fields([
...     "amount",
...     "targetamount",
...     "targetpercent"
... ], pf_enums.SizeType)
[0, 3, 5]

>>> vbt.map_enum_fields(3, pf_enums.SizeType)
3

>>> vbt.map_enum_fields("Target Amount", pf_enums.SizeType)
3

>>> pf = vbt.Portfolio.from_orders(
...     close=pd.Series([10, 11]),
...     size=pd.Series([1, -0.5]),
...     size_type=pd.Series(["amount", "percent"]),
...     direction="longonly"
... )
>>> pf.orders.records_readable
   Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0   1.0   10.0   0.0   Buy
1         1       0          1   0.5   11.0   0.0  Sell

>>> close = pd.Series(
...     [11, 10, 12], 
...     index=pd.date_range("2020-01-01", "2020-01-03")
... )
>>> size = pd.DataFrame(
...     [[-np.inf, np.nan, np.inf]],
...     columns=pd.Index(["short", "nan", "long"], name="size")
... )
>>> fees = 0.01

>>> broadcasted = vbt.broadcast(dict(
...     close=close,
...     size=size,
...     fees=0.01
... ))
>>> broadcasted["close"]
size        short  nan  long
2020-01-01     11   11    11
2020-01-02     10   10    10
2020-01-03     12   12    12

>>> broadcasted["size"]
size        short  nan  long
2020-01-01   -inf  NaN   inf
2020-01-02   -inf  NaN   inf
2020-01-03   -inf  NaN   inf

>>> broadcasted["fees"]
size        short   nan  long
2020-01-01   0.01  0.01  0.01
2020-01-02   0.01  0.01  0.01
2020-01-03   0.01  0.01  0.01

keep_flex=True
>>> broadcasted, wrapper = vbt.broadcast(dict(
...     close=close,
...     size=size,
...     fees=0.01
... ), keep_flex=True, return_wrapper=True)
>>> broadcasted["close"]
[[11]
 [10]
 [12]]

>>> broadcasted["size"]
[[-inf  nan  inf]]

>>> broadcasted["fees"]
[0.01]

>>> wrapper.fill()  Create a dummy array to reveal the broadcasted Pandas metadata
size        short  nan  long
2020-01-01    NaN  NaN   NaN
2020-01-02    NaN  NaN   NaN
2020-01-03    NaN  NaN   NaN

fees
init_cash
cash_deposits
close
target_shape
>>> init_position = 1
>>> new_init_position = vbt.broadcast_array_to(init_position, wrapper.shape_2d[1])
>>> new_init_position
array([1, 1, 1])

>>> pf = vbt.Portfolio.from_orders(
...     close=close,
...     size=size,
...     fees=fees,
...     init_position=init_position
... )
>>> pf.orders.records_readable
   Order Id Column  Timestamp       Size  Price      Fees  Side
0         0  short 2020-01-01  10.981098   11.0  1.207921  Sell
1         0   long 2020-01-01   9.000900   11.0  0.990099   Buy

>>> pf.value
size             short    nan        long
2020-01-01  109.792079  111.0  110.009901
2020-01-02  119.773177  110.0  100.009001
2020-01-03   99.810981  112.0  120.010801

broadcast_kwargs
>>> pf = vbt.Portfolio.from_orders(
...     close=close,
...     size=size,
...     fees=fees,
...     init_position=init_position,
...     broadcast_kwargs=dict(columns_from=["a", "b", "c"])
... )
>>> pf.value
                     a      b           c
2020-01-01  109.792079  111.0  110.009901
2020-01-02  119.773177  110.0  100.009001
2020-01-03   99.810981  112.0  120.010801

>>> pf = vbt.Portfolio.from_orders(
...     close=close,
...     size=vbt.Param([-np.inf, np.inf]),  We could have also passed a regular pd.Index for the same effect
...     fees=vbt.Param([0, 0.01]),
...     init_position=init_position
... )
>>> pf.value
size                          -inf                     inf            
fees              0.00        0.01        0.00        0.01
2020-01-01  111.000000  109.792079  111.000000  110.009901
2020-01-02  121.090909  119.773177  100.909091  100.009001
2020-01-03  100.909091   99.810981  121.090909  120.010801

pd.Index
>>> mult_close = mult_data.get("Close")
>>> mult_close
symbol                          BTC-USD      ETH-USD
Date                                                
2017-11-09 00:00:00+00:00   7143.580078   320.884003
2017-11-10 00:00:00+00:00   6618.140137   299.252991
2017-11-11 00:00:00+00:00   6357.600098   314.681000
...                                 ...          ...
2021-12-29 00:00:00+00:00  46444.710938  3628.531738
2021-12-30 00:00:00+00:00  47178.125000  3713.852051
2021-12-31 00:00:00+00:00  46306.445312  3682.632812

[1514 rows x 2 columns]

>>> mult_price = pd.concat((
...     mult_data.get("Open"), 
...     mult_data.get("Close")
... ), axis=1, keys=pd.Index(["open", "close"], name="price"))
>>> mult_price
price                              open                      close  \\
symbol                          BTC-USD      ETH-USD       BTC-USD   
Date                                                                 
2017-11-09 00:00:00+00:00   7446.830078   308.644989   7143.580078   
2017-11-10 00:00:00+00:00   7173.729980   320.670990   6618.140137   
2017-11-11 00:00:00+00:00   6618.609863   298.585999   6357.600098   
...                                 ...          ...           ...   
2021-12-29 00:00:00+00:00  47623.871094  3797.436279  46444.710938   
2021-12-30 00:00:00+00:00  46490.605469  3632.219727  47178.125000   
2021-12-31 00:00:00+00:00  47169.371094  3713.430176  46306.445312   

price                                   
symbol                         ETH-USD  
Date                                    
2017-11-09 00:00:00+00:00   320.884003  
2017-11-10 00:00:00+00:00   299.252991  
2017-11-11 00:00:00+00:00   314.681000  
...                                ...  
2021-12-29 00:00:00+00:00  3628.531738  
2021-12-30 00:00:00+00:00  3713.852051  
2021-12-31 00:00:00+00:00  3682.632812  

[1514 rows x 4 columns]

symbol
>>> pf = vbt.Portfolio.from_orders(close=mult_close, price=mult_price)
>>> pf.value
price                            open                    close             
symbol                        BTC-USD      ETH-USD     BTC-USD      ETH-USD
Date                                                                       
2017-11-09 00:00:00+00:00   95.927798   103.965402  100.000000   100.000000
2017-11-10 00:00:00+00:00   88.871910    96.957022   92.644585    93.258931
2017-11-11 00:00:00+00:00   85.373240   101.955648   88.997394    98.066902
...                               ...          ...         ...          ...
2021-12-29 00:00:00+00:00  623.684312  1175.632804  650.160150  1130.792345
2021-12-30 00:00:00+00:00  633.532987  1203.276315  660.426908  1157.381490
2021-12-31 00:00:00+00:00  621.827608  1193.161381  648.224627  1147.652355

[1514 rows x 4 columns]

ArrayLike
FlexArray
flex
>>> print(vbt.prettify(f_chunked.options["arg_take_spec"]["close"]))
FlexArraySlicer(
    single_type=None,
    ignore_none=True,
    mapper={
        'should_cache': True,
        'chunk_meta_cache': {...},
        'arg_query': {
            'pattern': '(group_lens|group_map)',
            'flags': 0
        }
    },
    axis=1
)

close
target_shape
group_lens
cash_deposits
>>> print(vbt.prettify(f_chunked.options["arg_take_spec"]["cash_deposits"]))
FlexArraySlicer(
    single_type=None,
    ignore_none=True,
    mapper=None,
    axis=1
)

portfolio_ch.flex_array_gl_slicer
group_by
group_by=True
cash_sharing
cash_sharing
>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close")
... )
>>> pf.value  Without grouping, each column manages its own initial cash
symbol                        BTC-USD      ETH-USD
Date                                              
2017-11-09 00:00:00+00:00  100.000000   100.000000
2017-11-10 00:00:00+00:00   92.644585    93.258931
2017-11-11 00:00:00+00:00   88.997394    98.066902
...                               ...          ...
2021-12-29 00:00:00+00:00  650.160150  1130.792345
2021-12-30 00:00:00+00:00  660.426908  1157.381490
2021-12-31 00:00:00+00:00  648.224627  1147.652355

[1514 rows x 2 columns]

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"),
...     group_by=True
... )
>>> pf.value  With grouping but without cash sharing, each column still manages its own initial cash, but during the post-analysis phase vectorbt concatenates both columns to analyze them as a single group, thus we see the first value being $200 instead of $100
Date
2017-11-09 00:00:00+00:00     200.000000
2017-11-10 00:00:00+00:00     185.903516
2017-11-11 00:00:00+00:00     187.064296
...                                  ...
2021-12-29 00:00:00+00:00    1780.952495
2021-12-30 00:00:00+00:00    1817.808397
2021-12-31 00:00:00+00:00    1795.876982
Freq: D, Name: group, Length: 1514, dtype: float64

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"),
...     group_by=True,
...     cash_sharing=True
... )
>>> pf.value  With grouping and with cash sharing, both columns manage the same initial cash. The first column went all in and left the second column without the required funds, such that the value effectively reflects only the first asset
Date
2017-11-09 00:00:00+00:00    100.000000
2017-11-10 00:00:00+00:00     92.644585
2017-11-11 00:00:00+00:00     88.997394
...                                 ...
2021-12-29 00:00:00+00:00    650.160150
2021-12-30 00:00:00+00:00    660.426908
2021-12-31 00:00:00+00:00    648.224627
Freq: D, Name: group, Length: 1514, dtype: float64

group_by=True
mult_close
mult_price
group_by=True
symbol
group_by
BTC-USD
(Open, BTC-USD)
(Close, BTC-USD)
ETH-USD
(Open, BTC-USD)
(Open, ETH-USD)
(Close, BTC-USD)
(Close, ETH-USD)
group_by
>>> pf = vbt.Portfolio.from_orders(
...     close=mult_close, 
...     price=mult_price,
...     group_by=pd.Index(["group1", "group1", "group2", "group2"])  Pass any collection of group labels manually, preferably a Pandas Index. This collection will be converted into a column level. Must have the same length as the number of columns in the target shape.
... )
>>> pf.value
                                group1       group2
Date                                               
2017-11-09 00:00:00+00:00   199.893199   200.000000
2017-11-10 00:00:00+00:00   185.828932   185.903516
2017-11-11 00:00:00+00:00   187.328888   187.064296
...                                ...          ...
2021-12-29 00:00:00+00:00  1799.317116  1780.952495
2021-12-30 00:00:00+00:00  1836.809302  1817.808397
2021-12-31 00:00:00+00:00  1814.988988  1795.876982

[1514 rows x 2 columns]

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_close, 
...     price=mult_price,
...     group_by=["price"]  Pass the names of all column levels except the one with the assets. Only works when there are multiple levels, that is, the column index is a pd.MultiIndex. The to-be-grouped column levels will be visible in the final column hierarchy.
... )
>>> pf.value
price                             open        close
Date                                               
2017-11-09 00:00:00+00:00   199.893199   200.000000
2017-11-10 00:00:00+00:00   185.828932   185.903516
2017-11-11 00:00:00+00:00   187.328888   187.064296
...                                ...          ...
2021-12-29 00:00:00+00:00  1799.317116  1780.952495
2021-12-30 00:00:00+00:00  1836.809302  1817.808397
2021-12-31 00:00:00+00:00  1814.988988  1795.876982

[1514 rows x 2 columns]

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_close, 
...     price=mult_price,
...     group_by=vbt.ExceptLevel("symbol")  Pass the column level with the assets to ExceptLevel to group all column levels except this one. This will work with any column index - the best option!
... )
>>> pf.value
price                             open        close
Date                                               
2017-11-09 00:00:00+00:00   199.893199   200.000000
2017-11-10 00:00:00+00:00   185.828932   185.903516
2017-11-11 00:00:00+00:00   187.328888   187.064296
...                                ...          ...
2021-12-29 00:00:00+00:00  1799.317116  1780.952495
2021-12-30 00:00:00+00:00  1836.809302  1817.808397
2021-12-31 00:00:00+00:00  1814.988988  1795.876982

pd.MultiIndex
group_by=True
group_by='symbol'
call_seq
>>> size = mult_symbol_wrapper.fill(np.nan)
>>> size.vbt.set(0.5, every="MS", inplace=True)
>>> size.iloc[0] = 0.5

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"), 
...     size=size,
...     size_type="targetpercent",
...     group_by=vbt.ExceptLevel("symbol"),
...     cash_sharing=True,
...     call_seq="auto"
... )
>>> allocations = pf.get_asset_value(group_by=False).vbt / pf.value
>>> allocations.vbt.plot(  Plot how much each asset contributes to the portfolio value
...    trace_kwargs=dict(stackgroup="one"),
...    use_gl=False
... ).show()

attach_call_seq
>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"), 
...     size=size,
...     size_type="targetpercent",
...     group_by=vbt.ExceptLevel("symbol"),
...     cash_sharing=True,
...     call_seq="auto",
...     attach_call_seq=True  Here
... )
>>> pf.call_seq
symbol                     BTC-USD  ETH-USD
Date                                       
2017-11-09 00:00:00+00:00        0        1
2017-11-10 00:00:00+00:00        1        0
2017-11-11 00:00:00+00:00        1        0
...                            ...      ...
2021-12-29 00:00:00+00:00        1        0
2021-12-30 00:00:00+00:00        1        0
2021-12-31 00:00:00+00:00        1        0

[1514 rows x 2 columns]

np.inf
init_cash
init_cash
>>> size = mult_symbol_wrapper.fill(np.nan)
>>> size.vbt.set(1, every="YS", inplace=True)
>>> size.iloc[0] = 1

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"), 
...     size=size,
...     init_cash="auto"  Use InitCashMode.Auto
... )
>>> pf.init_cash  Calculated after the simulation
symbol
BTC-USD    61218.626953
ETH-USD     2095.513962
Name: init_cash, dtype: float64

>>> pf2 = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"), 
...     size=size,
...     init_cash=pf.init_cash
... )
>>> pf2.cash.loc[~size.isnull().all(axis=1)]  Print out the cash balance after each investment
symbol                          BTC-USD      ETH-USD
Date                                                
2017-11-09 00:00:00+00:00  54075.046875  1774.629959
2018-01-01 00:00:00+00:00  40417.846680  1001.988968
2019-01-01 00:00:00+00:00  36574.326660   861.169556
2020-01-01 00:00:00+00:00  29374.152344   730.367554
2021-01-01 00:00:00+00:00      0.000000     0.000000

>>> pf2.orders.records_readable
   Order Id   Column                 Timestamp  Size         Price  Fees Side
0         0  BTC-USD 2017-11-09 00:00:00+00:00   1.0   7143.580078   0.0  Buy
1         1  BTC-USD 2018-01-01 00:00:00+00:00   1.0  13657.200195   0.0  Buy
2         2  BTC-USD 2019-01-01 00:00:00+00:00   1.0   3843.520020   0.0  Buy
3         3  BTC-USD 2020-01-01 00:00:00+00:00   1.0   7200.174316   0.0  Buy
4         4  BTC-USD 2021-01-01 00:00:00+00:00   1.0  29374.152344   0.0  Buy
5         0  ETH-USD 2017-11-09 00:00:00+00:00   1.0    320.884003   0.0  Buy
6         1  ETH-USD 2018-01-01 00:00:00+00:00   1.0    772.640991   0.0  Buy
7         2  ETH-USD 2019-01-01 00:00:00+00:00   1.0    140.819412   0.0  Buy
8         3  ETH-USD 2020-01-01 00:00:00+00:00   1.0    130.802002   0.0  Buy
9         4  ETH-USD 2021-01-01 00:00:00+00:00   1.0    730.367554   0.0  Buy

cash_deposits
cash_earnings
in_outputs.returns
>>> size = symbol_wrapper.fill(np.nan)
>>> size.vbt.set(-0.1, every="YS", inplace=True)

>>> pf = vbt.Portfolio.from_orders(
...     close=data.get("Close"),
...     size=size,
...     size_type="percent",
...     direction="longonly",
...     init_position=1,
...     cash_deposits=-np.inf
... )
>>> pf.cash_deposits[pf.cash_deposits != 0]
Date
2014-09-17 00:00:00+00:00    -100.000000
2015-01-02 00:00:00+00:00     -31.424899
2016-01-02 00:00:00+00:00     -39.090061
2017-01-02 00:00:00+00:00     -80.864326
2018-01-02 00:00:00+00:00    -995.609894
2019-01-02 00:00:00+00:00    -252.173348
2020-01-02 00:00:00+00:00    -425.163093
2021-01-02 00:00:00+00:00   -1561.062890
dtype: float64

None
True
max_order_records
max_log_records
>>> vbt.Portfolio.from_orders(True)
AssertionError: Data type of 'close' must be <class 'numpy.number'>, not bool

close
jitted
>>> big_target_shape = (1000, 1000)
>>> big_rand_price = np.random.randint(8, 12, size=big_target_shape)
>>> big_size = np.full(big_target_shape, 1)
>>> big_size[1::2] = -1

>>> %%timeit
>>> vbt.Portfolio.from_orders(
...     close=big_rand_price, 
...     size=big_size
... )
113 ms ± 2.34 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> %%timeit
>>> vbt.Portfolio.from_orders(
...     close=big_rand_price, 
...     size=big_size,
...     jitted=dict(parallel=True)
... )
97 ms ± 3.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

chunked
>>> %%timeit
>>> vbt.Portfolio.from_orders(
...     close=big_rand_price, 
...     size=big_size,
...     chunked=dict(engine="dask", n_chunks=4)
... )
67.2 ms ± 3.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> trade1 = dict(
...     timestamp="2022-01-22 12:39:26",
...     price=0.0027702,
...     size=4.99,
...     fixed_fees=1.01571e-05  Commission paid for each trade is provided in absolute terms, thus name it "fixed fees" to avoid confusion with "fees" that are usually given in %. Also, make sure that this number is provided in the quote currency (BTC in our example).
... )
>>> trade2 = dict(
...     timestamp="2022-01-29 02:12:50",
...     price=0.00243,
...     size=-1.72,
...     fixed_fees=3.0549e-06
... )
>>> trade3 = dict(
...     timestamp="2022-01-29 02:52:54",
...     price=0.0024299,
...     size=-3.27,
...     fixed_fees=5.8102e-06
... )

>>> trades = pd.DataFrame([trade1, trade2, trade3])  Put the trades into a DataFrame for easier handling
>>> trades["timestamp"] = pd.to_datetime(trades["timestamp"], utc=True)  Timezone must be the same as coming from Binance - UTC
>>> trades.set_index("timestamp", inplace=True)
>>> trades
                             price  size  fixed_fees
timestamp                                           
2022-01-22 12:39:26+00:00  0.00277  4.99    0.000010
2022-01-29 02:52:54+00:00  0.00243 -1.72    0.000003
2022-01-29 02:52:54+00:00  0.00243 -3.27    0.000006

>>> solbtc_data = vbt.BinanceData.pull(  Pull the hourly SOL/BTC data ranging from the day before the first trade to the day after the last trade (this time buffer is optional)
...     "SOLBTC", 
...     start=trades.index[0] - pd.Timedelta(days=1), 
...     end=trades.index[-1] + pd.Timedelta(days=1),
...     timeframe="1h"
... )

>>> resampler = vbt.Resampler(  Datetime index of the pulled data acts as a backbone to our time-series analysis, thus we need to put our trades to timestamps present in this index. For this, we need to create an instance of Resampler mapping the trade timestamps to the pulled bars.
...     source_index=trades.index, 
...     target_index=solbtc_data.wrapper.index,
...     source_freq=None,
...     target_freq="1h"
... )

>>> Since some trades (second and third in our example) may fall into the same hour and Portfolio.from_orders cannot execute multiple orders at the same bar, we need to aggregate their information
>>> from numba import njit

>>> @njit
... def avg_price_reduce_meta_nb(from_i, to_i, col, size, price):  Implement a meta function that reduces the price of all the trades that fall into the same bar using the size-weighted average
...     _size = size[from_i:to_i, col]
...     _price = price[from_i:to_i, col]
...     return np.sum(_price * _size) / np.sum(_size)

>>> price = pd.Series.vbt.resample_to_index(  Pass the resampler and the meta function to GenericAccessor.resample_to_index to resample the trade price
...     resampler, 
...     avg_price_reduce_meta_nb,
...     vbt.to_2d_array(trades["size"]),
...     vbt.to_2d_array(trades["price"]),
...     wrapper=trades["price"].vbt.wrapper,
... )
>>> price.loc[~price.isnull()]  Display the aggregated price points for validation
Open time
2022-01-22 12:00:00+00:00    0.00277
2022-01-29 02:00:00+00:00    0.00243
Freq: 158H, Name: price, dtype: float64

>>> size = trades["size"].vbt.resample_to_index(  Size and fixed fees can be easily aggregated using the sum operation
...     resampler, 
...     vbt.nb.sum_reduce_nb
... )
>>> size.loc[~size.isnull()]
Open time
2022-01-22 12:00:00+00:00    4.99
2022-01-29 02:00:00+00:00   -4.99
Freq: 158H, Name: size, dtype: float64

>>> fixed_fees = trades["fixed_fees"].vbt.resample_to_index(
...     resampler, 
...     vbt.nb.sum_reduce_nb
... )
>>> fixed_fees.loc[~fixed_fees.isnull()]
Open time
2022-01-22 12:00:00+00:00    0.000010
2022-01-29 02:00:00+00:00    0.000009
Freq: 158H, Name: fixed_fees, dtype: float64

>>> pf = vbt.Portfolio.from_orders(  Backtest the trades by setting the initial "cash" to 0.1 BTC and skipping missing data points
...     open=solbtc_data.get("Open"),
...     high=solbtc_data.get("High"),
...     low=solbtc_data.get("Low"),
...     close=solbtc_data.get("Close"),
...     price=price,
...     size=size,
...     fixed_fees=fixed_fees,
...     init_cash=0.1,
...     ffill_val_price=False,
...     skipna=True
... )
>>> pf.orders.records_readable  Filled orders have the same information as our trades
   Order Id  Column                 Timestamp  Size    Price      Fees  Side
0         0       0 2022-01-22 12:00:00+00:00  4.99  0.00277  0.000010   Buy
1         1       0 2022-01-29 02:00:00+00:00  4.99  0.00243  0.000009  Sell

>>> pf.plot().show()  Plot the portfolio

size
price
fixed_fees
@jitclass
>>> import vectorbtpro as vbt

>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,  Debt is non-zero only when leveraging or shorting
...     locked_cash=0.0,  Locked cash is non-zero only when leveraging or shorting
...     free_cash=100.0  Free cash deviates from the regular cash balance only when leveraging or shorting
... )
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state,
...     size=1.0,
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=1.0,
    price=15.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=85.0,
    position=1.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=85.0
)

>>> vbt.pf_enums.OrderSide._fields[order_result.side]
'Buy'

>>> vbt.pf_enums.OrderStatus._fields[order_result.status]
'Filled'

-1
>>> import numpy as np

>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state,  Use the previous account state as input
...     size=np.inf,  Infinity means using up the entire balance
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.666666666666667,
    price=15.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=0.0,
    position=6.666666666666667,
    debt=0.0,
    locked_cash=0.0,
    free_cash=0.0
)

5.67
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state,
...     size=np.inf,
...     price=15.0,
...     size_granularity=1
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=6.0,
    price=15.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=10.0,
    position=6.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=10.0
)

>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     new_account_state,  Use the account state from the previous operation
...     size=np.inf,
...     price=15.0,
...     size_granularity=1
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=np.nan,
    price=np.nan,
    fees=np.nan,
    side=-1,
    status=1,
    status_info=5
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=10.0,
    position=6.0,
    debt=0.0,
    free_cash=10.0
)

>>> vbt.pf_enums.OrderStatus._fields[order_result.status]
'Ignored'

>>> vbt.pf_enums.OrderStatusInfo._fields[order_result.status_info]
'SizeZero'

>>> vbt.pf_enums.status_info_desc[order_result.status_info]  There is a list with more elaborative descriptions of different statuses
'Size is zero'

>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=1000.0, 
...     price=15.0,
...     allow_partial=False
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=np.nan,
    price=np.nan,
    fees=np.nan,
    side=-1,
    status=2,
    status_info=12
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=100.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=100.0
)

>>> vbt.pf_enums.OrderStatus._fields[order_result.status]
'Rejected'

>>> vbt.pf_enums.status_info_desc[order_result.status_info]
'Final size is less than requested'

>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=np.inf, 
...     price=15.0,
...     fees=0.01,  0.01 = 1%. Paid always in cash. To specify fixed fees, use fixed_fees instead.
...     slippage=0.01,  0.01 = 1%. Applied on the price. By artificially increasing the price, you always put yourself at a disadvantage, but this might be wanted to make backtesting more realistic.
...     percent=0.5  0.01 = 1%. Applied on the resources used to open or increase a position. 
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=3.2676534980230696,
    price=15.15,
    fees=0.4950495049504937,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=50.0,
    position=3.2676534980230696,
    debt=0.0,
    locked_cash=0.0,
    free_cash=50.0
)

fixed_fees
price_area
price_area_vio_mode
>>> price_area = vbt.pf_enums.PriceArea(
...     open=10,
...     high=14,
...     low=8,
...     close=12
... )
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state,
...     size=np.inf,
...     price=np.inf,
...     price_area=price_area,
...     price_area_vio_mode=vbt.pf_enums.PriceAreaVioMode.Error
... )
ValueError: Adjusted order price is above the highest price

>>> account_state = vbt.pf_enums.AccountState(
...     cash=0.0,
...     position=10.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=0.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state,
...     size=2.0,
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=2.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=30.0,
    position=8.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=30.0
)

>>> vbt.pf_enums.OrderSide._fields[order_result.side]
'Sell'

>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=np.inf,  Short-sell as much as possible
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=6.666666666666667,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=200.0,
    position=-6.666666666666667,
    debt=100.0,
    locked_cash=100.0,
    free_cash=0.0
)

leverage
>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=np.inf,
...     price=15.0,
...     leverage=2
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=13.333333333333334,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=300.0,
    position=-13.333333333333334,
    debt=200.0,
    locked_cash=100.0,
    free_cash=0.0
)

>>> order_result, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=np.inf,
...     price=15.0,
...     leverage=2
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=np.nan,
    price=np.nan,
    fees=np.nan,
    side=-1,
    status=2,
    status_info=6
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=300.0,
    position=-13.333333333333334,
    debt=200.0,
    locked_cash=100.0,
    free_cash=0.0
)

>>> vbt.pf_enums.OrderStatus._fields[order_result.status]
'Rejected'

>>> vbt.pf_enums.status_info_desc[order_result.status_info]
'Not enough cash'

>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=1000, 
...     price=15.0, 
...     leverage=np.inf
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=1000.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=15100.0,
    position=-1000.0,
    debt=15000.0,
    locked_cash=100.0,
    free_cash=0.0
)

>>> new_account_state.debt / new_account_state.locked_cash
150.0

>>> new_account_state.cash + new_account_state.position * order_result.price
100.0

>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=10.0, 
...     price=15.0,
...     leverage=2
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=250.0,
    position=-10.0,
    debt=150.0,
    locked_cash=75.0,
    free_cash=25.0
)

>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=30.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=30.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=100.0,
    position=-5.0,
    debt=75.0,
    locked_cash=37.5,
    free_cash=-12.5
)

debt
locked_cash
free_cash
>>> new_account_state2.debt / abs(new_account_state2.position)
15.0

>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=10.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=200.0,
    position=-5.0,
    debt=75.0,
    locked_cash=37.5,
    free_cash=87.5
)

>>> st0 = account_state
>>> st1 = new_account_state2
>>> avg_entry_price = st1.debt / abs(st1.position)  The remaining shares are still valued at $15
>>> new_equity = st1.cash + st1.position * avg_entry_price
>>> new_equity - st0.cash
25.0

>>> order_result, new_account_state3 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state2, 
...     size=5.0, 
...     price=10.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state3)
AccountState(
    cash=150.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=150.0
)

>>> order_result, new_account_state3 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state2, 
...     size=5.0, 
...     price=100.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=2.0,
    price=100.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state3)
AccountState(
    cash=0.0,
    position=-3.0,
    debt=45.0,
    locked_cash=22.5,
    free_cash=-67.5
)

debt + locked_cash + free_cash
leverage
debt
locked_cash
debt / locked_cash
debt / locked_cash + 1
>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0
... )
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=20, 
...     price=10.0,
...     leverage=np.inf
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=20,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=-100.0,
    position=20.0,
    debt=100.0,
    locked_cash=100.0,
    free_cash=0.0
)

>>> new_account_state.debt / new_account_state.locked_cash + 1
2.0

>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=10, 
...     price=10.0,
...     leverage=np.inf
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=0.0,
    position=10.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=0.0
)

leverage_mode
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=10, 
...     price=10.0,
...     leverage=3,
...     leverage_mode=vbt.pf_enums.LeverageMode.Eager
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=0.0,
    position=10.0,
    debt=66.66666666666667,
    locked_cash=33.333333333333336,
    free_cash=66.66666666666666
)

>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=10, 
...     price=20.0,
...     leverage=2
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10,
    price=20.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=-100.0,
    position=10.0,
    debt=100.0,
    locked_cash=100.0,
    free_cash=0.0
)

>>> order_result, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=5.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=5.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=-75.0,
    position=5.0,
    debt=50.0,
    locked_cash=50.0,
    free_cash=-25.0
)

>>> st0 = account_state
>>> st1 = new_account_state2
>>> avg_entry_price = (st1.debt + st1.locked_cash) / abs(st1.position)  The remaining shares are still valued at $20
>>> new_equity = st1.cash + st1.position * avg_entry_price
>>> new_equity - st0.cash
-75.0

>>> order_result, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=40.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=40.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=100.0,
    position=5.0,
    debt=50.0,
    locked_cash=50.0,
    free_cash=150.0
)

>>> order_result, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=40.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=40.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=300.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=300.0
)

>>> account_state = vbt.pf_enums.AccountState(
...     cash=200.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=200.0
... )
>>> _, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=10, 
...     price=20.0
... )
>>> _, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=10.0, 
...     price=40.0
... )
>>> new_account_state2.free_cash - account_state.free_cash
200.0

>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0
... )

>>> _, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     direction=vbt.pf_enums.Direction.LongOnly,
...     size=np.inf, 
...     price=10.0,
...     leverage=10
... )
>>> _, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     direction=vbt.pf_enums.Direction.LongOnly,
...     size=np.inf, 
...     price=15.0
... )
>>> vbt.pprint(new_account_state)
AccountState(
    cash=600.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=600.0
)

>>> _, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     direction=vbt.pf_enums.Direction.ShortOnly,
...     size=np.inf, 
...     price=10.0,
...     leverage=10
... )
>>> _, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=new_account_state, 
...     direction=vbt.pf_enums.Direction.ShortOnly,
...     size=np.inf, 
...     price=5.0
... )
>>> vbt.pprint(new_account_state)
AccountState(
    cash=600.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=600.0
)

Direction.Both
>>> account_state = vbt.pf_enums.AccountState(
...     cash=0.0,
...     position=10.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=0.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=np.inf, 
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=20.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=300.0,
    position=-10.0,
    debt=150.0,
    locked_cash=150.0,
    free_cash=0.0
)

>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state, 
...     size=np.inf, 
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=20.0,
    price=15.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=0.0,
    position=10.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=0.0
)

direction
>>> account_state = vbt.pf_enums.AccountState(
...     cash=0.0,
...     position=10.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=0.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=np.inf, 
...     price=15.0, 
...     direction=vbt.pf_enums.Direction.LongOnly
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=150.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=150.0
)

direction
>>> account_state = vbt.pf_enums.AccountState(
...     cash=0.0,
...     position=10.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=0.0
... )
>>> order_result, new_account_state = vbt.pf_nb.long_sell_nb(
...     account_state=account_state, 
...     size=np.inf, 
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=150.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=150.0
)

>>> from numba import njit

>>> @njit
... def pipeline_1_nb(close, entries, exits, init_cash=100):
...     account_state = vbt.pf_enums.AccountState(  Initial account state
...         cash=float(init_cash),
...         position=0.0,
...         debt=0.0,
...         locked_cash=0.0,
...         free_cash=float(init_cash)
...     )
...     for i in range(close.shape[0]):
...         if entries[i]:
...             _, account_state = vbt.pf_nb.buy_nb(  Execute the order and return a new account state
...                 account_state=account_state,
...                 size=1 / close[i],
...                 price=close[i]
...             )
...         if exits[i]:
...             _, account_state = vbt.pf_nb.sell_nb(
...                 account_state=account_state,
...                 size=1 / close[i],
...                 price=close[i]
...             )
...     return account_state.cash + account_state.position * close[-1]  Calculate the final portfolio value

>>> data = vbt.YFData.pull("BTC-USD", end="2022-01-01")
>>> sma_50 = vbt.talib("SMA").run(data.get("Close"), 50)
>>> sma_200 = vbt.talib("SMA").run(data.get("Close"), 200)
>>> entries = sma_50.real_crossed_above(sma_200)
>>> exits = sma_50.real_crossed_below(sma_200)

>>> pipeline_1_nb(
...     data.get("Close").values, 
...     entries.values, 
...     exits.values
... )
210.71073253390762

_nb
>>> vbt.Portfolio.from_orders(
...     data.get("Close"), 
...     size=entries.astype(int) - exits.astype(int), 
...     size_type="value"
... ).final_value
210.71073253390762

>>> order = vbt.pf_enums.Order()
>>> vbt.pprint(order)
Order(
    size=np.inf,
    price=np.inf,
    size_type=0,
    direction=2,
    fees=0.0,
    fixed_fees=0.0,
    slippage=0.0,
    min_size=np.nan,
    max_size=np.nan,
    size_granularity=np.nan,
    leverage=1.0,
    leverage_mode=0,
    reject_prob=0.0,
    price_area_vio_mode=0,
    allow_partial=True,
    raise_reject=False,
    log=False
)

>>> order.direction
2

>>> order[3]
2

>>> tuple(order)  Convert to a regular tuple
(inf,
 inf,
 0,
 2,
 0.0,
 0.0,
 0.0,
 nan,
 nan,
 nan,
 1.0,
 0,
 0.0,
 0,
 True,
 False,
 False)

>>> @njit
... def create_order_nb():
...     return vbt.pf_enums.Order()  Using the default values only

>>> create_order_nb()
Order(size=inf, price=inf, ...)

>>> @njit
... def create_order_nb(size, price):
...     return vbt.pf_enums.Order(size=size, price=price)  Overriding the default values of arguments on the left side

>>> create_order_nb(1, 15)
Order(size=1, price=15, ...)

>>> @njit
... def create_order_nb(size, price, direction):
...     return vbt.pf_enums.Order(size=size, price=price, direction=direction)  Overriding the default values of arguments on different positions

>>> create_order_nb(1, 15, 2)
Failed in nopython mode pipeline (step: nopython frontend)

>>> @njit
... def create_order_nb(size, price, direction):
...     return vbt.pf_nb.order_nb(size=size, price=price, direction=direction)

>>> create_order_nb(1, 15, 2)
Order(size=1.0, price=15.0, ..., direction=2, ...)

>>> vbt.pf_nb.close_position_nb(15)  Uses size of zero and size type of TargetAmount
Order(size=0.0, price=15.0, size_type=6, ...)

TargetAmount
>>> exec_state = vbt.pf_enums.ExecState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0,
...     val_price=15.0,
...     value=100.0
... )
>>> vbt.pf_nb.execute_order_nb(
...     exec_state,
...     vbt.pf_nb.order_nb(price=-15)
... )
ValueError: order.price must be finite and 0 or greater

price_area
-np.inf
np.inf
np.inf
>>> price_area = vbt.pf_enums.PriceArea(
...     open=10,
...     high=14,
...     low=8,
...     close=12
... )
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(  Price gets replaced by the open price. Order executed.
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=-np.inf),
...     price_area=price_area
... )
>>> order_result.price
10.0

>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(  Price gets replaced by the close price (default). Order executed.
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=np.inf),
...     price_area=price_area
... )
>>> order_result.price
12.0

>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(  Price gets replaced by np.nan since the price area is not defined. Order ignored.
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=np.inf)
... )
>>> order_result.price
nan

np.nan
Amount
TargetAmount
Value
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(
...         size=3, 
...         size_type=vbt.pf_enums.SizeType.TargetAmount
...     ),
...     price_area=price_area
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=3.0,
    price=12.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_exec_state)
ExecState(
    cash=64.0,
    position=3.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=64.0,
    val_price=15.0,
    value=100.0
)

direction
size
LongOnly
Both
ShortOnly
ShortOnly
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(
...         size=1.0, 
...         size_type=vbt.pf_enums.SizeType.TargetPercent
...     ),
...     price_area=price_area
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=6.666666666666667,
    price=12.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_exec_state)
ExecState(
    cash=20.0,
    position=6.666666666666667,
    debt=0.0,
    locked_cash=0.0,
    free_cash=20.0,
    val_price=15.0,
    value=100.0
)

val_price
value
100 / 15 = 6.67
update_value
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(
...         size=1.0, 
...         size_type=vbt.pf_enums.SizeType.TargetPercent,
...         fixed_fees=10,
...         slippage=0.01
...     ),
...     price_area=price_area,
...     update_value=True
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=6.666666666666667,
    price=12.120000000000001,
    fees=10.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_exec_state)
ExecState(
    cash=9.199999999999989,
    position=6.666666666666667,
    debt=0.0,
    locked_cash=0.0,
    free_cash=9.199999999999989,
    val_price=12.120000000000001,
    value=90.0
)

>>> @njit
... def pipeline_2_nb(open, close, target_pct, init_cash=100):
...     asset_value_out = np.empty(close.shape, dtype=np.float_)  Create two empty arrays with a floating data type. Remember that creating an array with np.empty will produce an array with uninitialized (garbage) values that you should override.
...     value_out = np.empty(close.shape, dtype=np.float_)
...     exec_state = vbt.pf_enums.ExecState(  Our initial order execution state
...         cash=float(init_cash),
...         position=0.0,
...         debt=0.0,
...         locked_cash=0.0,
...         free_cash=float(init_cash),
...         val_price=np.nan,
...         value=np.nan
...     )
...
...     for i in range(close.shape[0]):
...         if not np.isnan(target_pct[i]):  There is no need to run order execution when target percentage is np.nan (= do not rebalance)
...             val_price = open[i]
...             value = exec_state.cash + val_price * exec_state.position  Calculate the portfolio value at the beginning of the bar (= valuation)
...
...             exec_state = vbt.pf_enums.ExecState(  Create a new existing order execution state after the valuation
...                 cash=exec_state.cash,
...                 position=exec_state.position,
...                 debt=exec_state.debt,
...                 locked_cash=exec_state.locked_cash,
...                 free_cash=exec_state.free_cash,
...                 val_price=val_price,
...                 value=value
...             )
...             order = vbt.pf_nb.order_nb(  Create a new order tuple using the close price as order price
...                 size=target_pct[i],
...                 price=close[i],
...                 size_type=vbt.pf_enums.SizeType.TargetPercent
...             )
...             _, exec_state = vbt.pf_nb.execute_order_nb(  Execute the order and return the order result and the new execution state
...                 exec_state=exec_state,
...                 order=order
...             )
...
...         asset_value_out[i] = exec_state.position * close[i]  Fill the arrays (you should fill each single element!)
...         value_out[i] = exec_state.cash + exec_state.position * close[i]
...         
...     return asset_value_out, value_out

np.empty
np.nan
>>> symbol_wrapper = data.get_symbol_wrapper()  We cannot use data.wrapper because it contains OHLC as columns. What we need is a wrapper that has symbols as columns, to fill the array with target percentages.
>>> target_pct = symbol_wrapper.fill()
>>> target_pct.vbt.set(0.5, every="MS", inplace=True)  Fill the array with NaNs and set all data points at the beginning of each month to 0.5.

>>> asset_value, value = pipeline_2_nb(
...     data.get("Open").values, 
...     data.get("Close").values, 
...     target_pct.values
... )
>>> asset_value = symbol_wrapper.wrap(asset_value)  Use the same wrapper to convert the NumPy array into Pandas Series
>>> value = symbol_wrapper.wrap(value)
>>> allocations = (asset_value / value).rename(None)  Divide the asset value by the portfolio value to derive the allocation
>>> allocations.vbt.scatterplot(trace_kwargs=dict(
...     marker=dict(
...         color=allocations, 
...         colorscale="Temps", 
...         size=3,
...         cmin=0.3,
...         cmid=0.5,
...         cmax=0.7
...     )
... )).show()

data.wrapper
0.5
>>> pf = vbt.Portfolio.from_orders(
...     data, 
...     size=target_pct, 
...     size_type="targetpercent"
... )
>>> pf.allocations.vbt.scatterplot(trace_kwargs=dict(
...     marker=dict(
...         color=allocations, 
...         colorscale="Temps", 
...         size=3,
...         cmin=0.3,
...         cmid=0.5,
...         cmax=0.7
...     )
... )).show()

>>> order_records = np.empty(2, dtype=vbt.pf_enums.order_dt)
>>> order_count = 0

>>> order_records
array([(4585679916398730403, ..., 4583100142070297783),
       (4582795628349012822, ..., 4576866499094039639)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})

>>> exec_state = vbt.pf_enums.ExecState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0,
...     val_price=15.0,
...     value=100.0
... )
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=15.0)
... )
>>> if order_result.status == vbt.pf_enums.OrderStatus.Filled:  Check that the order has been filled
...     order_records["id"][order_count] = order_count  Order ids start with 0 and follow the counter
...     order_records["col"][order_count] = 0
...     order_records["idx"][order_count] = 678  Index of the current bar
...     order_records["size"][order_count] = order_result.size
...     order_records["price"][order_count] = order_result.price
...     order_records["fees"][order_count] = order_result.fees
...     order_records["side"][order_count] = order_result.side
...     order_count += 1

>>> order_records[0]
(0, 0, 678, 6.66666667, 15., 0., 0)

>>> order_count
1

>>> order_result, new_exec_state2 = vbt.pf_nb.execute_order_nb(
...     exec_state=new_exec_state,
...     order=vbt.pf_nb.order_nb(size=-np.inf, price=16.0)
... )
>>> if order_result.status == vbt.pf_enums.OrderStatus.Filled:
...     order_records["id"][order_count] = order_count  Don't forget to increment the order id
...     order_records["col"][order_count] = 0
...     order_records["idx"][order_count] = 679
...     order_records["size"][order_count] = order_result.size
...     order_records["price"][order_count] = order_result.price
...     order_records["fees"][order_count] = order_result.fees
...     order_records["side"][order_count] = order_result.side
...     order_count += 1

>>> order_records[1]
(1, 0, 679, 13.33333333, 16., 0., 1)

>>> order_count
2

>>> order_records
array([(0, 0, 678,  6.66666667, 15., 0., 0),
       (1, 0, 679, 13.33333333, 16., 0., 1)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})

>>> order_records = np.empty((2, 1), dtype=vbt.pf_enums.order_dt)
>>> order_counts = np.full(1, 0, dtype=np.int_)

>>> order_result1, new_exec_state1 = vbt.pf_nb.process_order_nb(
...     0, 0, 678,  Current group, column, and index (row)
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=15.0),
...     order_records=order_records,
...     order_counts=order_counts
... )
>>> order_result2, new_exec_state2 = vbt.pf_nb.process_order_nb(
...     0, 0, 679,
...     exec_state=new_exec_state,
...     order=vbt.pf_nb.order_nb(size=-np.inf, price=16.0),
...     order_records=order_records,
...     order_counts=order_counts
... )

>>> order_records
array([(0, 0, 678,  6.66666667, 15., 0., 0),
       (1, 0, 679, 13.33333333, 16., 0., 1)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})

>>> order_counts
array([2])

>>> order_records = np.empty((2, 1), dtype=vbt.pf_enums.order_dt)
>>> order_counts = np.full(1, 0, dtype=np.int_)
>>> log_records = np.empty((2, 1), dtype=vbt.pf_enums.log_dt)
>>> log_counts = np.full(1, 0, dtype=np.int_)

>>> order_result1, new_exec_state1 = vbt.pf_nb.process_order_nb(
...     0, 0, 678,
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=15.0, log=True),  Logging of each order must be explicitly enabled
...     order_records=order_records,
...     order_counts=order_counts,
...     log_records=log_records,
...     log_counts=log_counts
... )
>>> order_result2, new_exec_state2 = vbt.pf_nb.process_order_nb(
...     0, 0, 679,
...     exec_state=new_exec_state,
...     order=vbt.pf_nb.order_nb(size=-np.inf, price=16.0, log=True),
...     order_records=order_records,
...     order_counts=order_counts,
...     log_records=log_records,
...     log_counts=log_counts
... )

>>> log_records
array([[(0, 0, 0, 678, ..., 0., 15., 100., 0)],
       [(1, 0, 0, 679, ..., 0., 15., 100., 1)]],
      dtype={'names':['id','group',...,'res_status_info','order_id'], ...})

>>> @njit
... def pipeline_3_nb(open, close, target_pct, init_cash=100):
...     order_records = np.empty(close.shape, dtype=vbt.pf_enums.order_dt)  Since we don't know the number of orders in advance, let's prepare for the worst-case scenario: one record at each bar. Remember that order records must be aligned column-wise.
...     order_counts = np.full(close.shape[1], 0, dtype=np.int_)
...
...     for col in range(close.shape[1]):  Iterate over columns in close and run our logic on each one
...         exec_state = vbt.pf_enums.ExecState(
...             cash=float(init_cash),
...             position=0.0,
...             debt=0.0,
...             locked_cash=0.0,
...             free_cash=float(init_cash),
...             val_price=np.nan,
...             value=np.nan
...         )
...
...         for i in range(close.shape[0]):
...             if not np.isnan(target_pct[i, col]):  Since every array passed to the pipeline now must be two-dimensional, don't forget to specify the column when accessing an array element. Also, in indexing, first comes the row and then the column 
...                 val_price = open[i, col]
...                 value = exec_state.cash + val_price * exec_state.position
...
...                 exec_state = vbt.pf_enums.ExecState(
...                     cash=exec_state.cash,
...                     position=exec_state.position,
...                     debt=exec_state.debt,
...                     locked_cash=exec_state.locked_cash,
...                     free_cash=exec_state.free_cash,
...                     val_price=val_price,
...                     value=value
...                 )
...                 order = vbt.pf_nb.order_nb(
...                     size=target_pct[i, col],
...                     price=close[i, col],
...                     size_type=vbt.pf_enums.SizeType.TargetPercent
...                 )
...                 _, exec_state = vbt.pf_nb.process_order_nb(
...                     col, col, i,  
...                     exec_state=exec_state,
...                     order=order,
...                     order_records=order_records,
...                     order_counts=order_counts
...                 )
...         
...     return vbt.nb.repartition_nb(order_records, order_counts)  Use repartition_nb to flatten the final order records array (= concatenate records of all columns into a one-dimensional array)

close
every
>>> import pandas as pd

>>> every = pd.Index(["MS", "Q", "Y"], name="every")

>>> open = data.get("Open").vbt.tile(3, keys=every)  Use BaseAccessor.tile to populate columns and append a new column level for our parameter combinations
>>> close = data.get("Close").vbt.tile(3, keys=every)
>>> close
every                                MS             Q             Y
Date                                                               
2014-09-17 00:00:00+00:00    457.334015    457.334015    457.334015
2014-09-18 00:00:00+00:00    424.440002    424.440002    424.440002
2014-09-19 00:00:00+00:00    394.795990    394.795990    394.795990
...                                 ...           ...           ...
2021-12-29 00:00:00+00:00  46444.710938  46444.710938  46444.710938
2021-12-30 00:00:00+00:00  47178.125000  47178.125000  47178.125000
2021-12-31 00:00:00+00:00  46306.445312  46306.445312  46306.445312

[2663 rows x 3 columns]

>>> target_pct = symbol_wrapper.fill().vbt.tile(3, keys=every)
>>> target_pct.vbt.set(0.5, every="MS", columns=["MS"], inplace=True)  Change the corresponding column only
>>> target_pct.vbt.set(0.5, every="Q", columns=["Q"], inplace=True)
>>> target_pct.vbt.set(0.5, every="Y", columns=["Y"], inplace=True)

>>> order_records = pipeline_3_nb(
...     open.values, 
...     close.values, 
...     target_pct.values
... )
>>> order_records
array([( 0, 0,   14, 1.29056570e-01,   383.61499023, 0., 0),  << first column
       ( 1, 0,   45, 1.00206092e-02,   325.74899292, 0., 0),
       ( 2, 0,   75, 7.10912824e-03,   379.24499512, 0., 1),
       ...
       (84, 0, 2571, 7.79003416e-04, 48116.94140625, 0., 0),
       (85, 0, 2602, 3.00678739e-03, 61004.40625   , 0., 1),
       (86, 0, 2632, 6.84410394e-04, 57229.828125  , 0., 0),
       ( 0, 1,   13, 1.32947604e-01,   386.94400024, 0., 0),  << second column
       ( 1, 1,  105, 1.16132613e-02,   320.19299316, 0., 0),
       ( 2, 1,  195, 1.83187063e-02,   244.22399902, 0., 0),
       ...
       (27, 1, 2478, 7.74416872e-03, 35040.8359375 , 0., 0),
       (28, 1, 2570, 2.08567037e-03, 43790.89453125, 0., 1),
       (29, 1, 2662, 1.72637091e-03, 46306.4453125 , 0., 1),
       ( 0, 2,  105, 1.60816173e-01,   320.19299316, 0., 0),  << third column
       ( 1, 2,  470, 2.34573523e-02,   430.56698608, 0., 1),
       ( 2, 2,  836, 3.81744650e-02,   963.74298096, 0., 1),
       ...
       ( 5, 2, 1931, 2.83026812e-02,  7193.59912109, 0., 1),
       ( 6, 2, 2297, 3.54188390e-02, 29001.72070312, 0., 1),
       ( 7, 2, 2662, 1.14541249e-02, 46306.4453125 , 0., 1)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})

>>> vbt.phelp(vbt.Portfolio)
Portfolio.__init__(
    self,
    wrapper,
    order_records,
    *,
    close,
    open=None,
    high=None,
    low=None,
    log_records=None,
    cash_sharing=False,
    init_cash='auto',
    ...
)

*
order_records
>>> pf = vbt.Portfolio(
...     close.vbt.wrapper,
...     order_records,
...     open=open,
...     close=close,
...     init_cash=100  Make sure to use the same initial cash as during the simulation
... )

>>> pf.sharpe_ratio
every
MS    1.267804
Q     1.309943
Y     1.393820
Name: sharpe_ratio, dtype: float64

target_pct
close
close
>>> per_row_arr = np.array([1, 2, 3])
>>> per_col_arr = np.array([4, 5])
>>> per_elem_arr = np.array([
...     [6, 7],
...     [8, 9],
...     [10, 11]
... ])

>>> vbt.flex_select_1d_pr_nb(per_row_arr, 2)  Get the value under the third row
3

>>> vbt.flex_select_1d_pc_nb(per_col_arr, 1)  Get the value under the second column
5

>>> vbt.flex_select_nb(per_elem_arr, 2, 1)  Get the value under the third row and second column
11

per_row_arr
per_col_arr
>>> per_row_arr_2d = per_row_arr[:, None]  Create the second axis of length one: from (3,) to (3, 1)
>>> per_row_arr_2d
array([[1],
       [2],
       [3]])

>>> vbt.flex_select_nb(per_row_arr_2d, 2, 1)
3

>>> per_col_arr_2d = per_col_arr[None]  Create the first axis of length one: from (3,) to (1, 3)
>>> per_col_arr_2d
array([[4, 5]])

>>> vbt.flex_select_nb(per_col_arr_2d, 2, 1)
5

(3,)
(3, 1)
(3,)
(1, 3)
>>> target_shape = (3, 2)

>>> vbt.broadcast_array_to(per_row_arr, target_shape[0])[2]
3

>>> vbt.broadcast_array_to(per_col_arr, target_shape[1])[1]
5

>>> vbt.broadcast_array_to(per_row_arr_2d, target_shape)[2, 1]
3

>>> vbt.broadcast_array_to(per_col_arr_2d, target_shape)[2, 1]
5

>>> vbt.flex_select_1d_pr_nb(per_row_arr, 100, rotate_rows=True)  Resolves to index 100 % 3 == 1 and element 2
2

>>> vbt.flex_select_1d_pc_nb(per_col_arr, 100, rotate_cols=True)  Resolves to index 100 % 2 == 0 and element 4
4

>>> vbt.flex_select_nb(per_elem_arr, 100, 100, rotate_rows=True, rotate_cols=True)
8

target_shape
>>> @njit
... def pipeline_4_nb(
...     target_shape, 
...     open, 
...     close, 
...     target_pct, 
...     init_cash=100,
...     rotate_cols=False
... ):
...     init_cash_ = vbt.to_1d_array_nb(np.asarray(init_cash))  This line allows us to pass a one-dimensional flexible array also as a scalar. Note how we write the result to a new variable (with a trailing underscore) and then use it in indexing.
...     open_ = vbt.to_2d_array_nb(np.asarray(open))  Same for two-dimensional flexible arrays
...     close_ = vbt.to_2d_array_nb(np.asarray(close))
...     target_pct_ = vbt.to_2d_array_nb(np.asarray(target_pct))
...     order_records = np.empty(target_shape, dtype=vbt.pf_enums.order_dt)
...     order_counts = np.full(target_shape[1], 0, dtype=np.int_)
...
...     for col in range(target_shape[1]):
...         init_cash_elem = vbt.flex_select_1d_pc_nb(
...             init_cash_, col, rotate_cols=rotate_cols)  Select the current element of the initial cash array. Remember that indexing functions with the suffix 1d are expecting one-dimensional arrays.
...
...         exec_state = vbt.pf_enums.ExecState(
...             cash=float(init_cash_elem),
...             position=0.0,
...             debt=0.0,
...             locked_cash=0.0,
...             free_cash=float(init_cash_elem),
...             val_price=np.nan,
...             value=np.nan
...         )
...
...         for i in range(target_shape[0]):
...             open_elem = vbt.flex_select_nb(
...                 open_, i, col, rotate_cols=rotate_cols)  Since all three arrays are not guaranteed to have the full shape anymore, we must switch to flexible indexing instead of doing open[i, col]. Remember that indexing functions without the suffix 1d are expecting two-dimensional arrays.
...             close_elem = vbt.flex_select_nb(
...                 close_, i, col, rotate_cols=rotate_cols)
...             target_pct_elem = vbt.flex_select_nb(
...                 target_pct_, i, col, rotate_cols=rotate_cols)
...
...             if not np.isnan(target_pct_elem):
...                 value = exec_state.cash + open_elem * exec_state.position
...
...                 exec_state = vbt.pf_enums.ExecState(
...                     cash=exec_state.cash,
...                     position=exec_state.position,
...                     debt=exec_state.debt,
...                     locked_cash=exec_state.locked_cash,
...                     free_cash=exec_state.free_cash,
...                     val_price=open_elem,
...                     value=value
...                 )
...                 order = vbt.pf_nb.order_nb(
...                     size=target_pct_elem,
...                     price=close_elem,
...                     size_type=vbt.pf_enums.SizeType.TargetPercent
...                 )
...                 _, exec_state = vbt.pf_nb.process_order_nb(
...                     col, col, i,
...                     exec_state=exec_state,
...                     order=order,
...                     order_records=order_records,
...                     order_counts=order_counts
...                 )
...         
...     return vbt.nb.repartition_nb(order_records, order_counts)

1d
open[i, col]
1d
>>> target_shape = vbt.broadcast_shapes(  We need to build the target shape to iterate over. This also works as a broadcasting check.
...     data.get("Open").values.shape,
...     data.get("Close").values.shape,
...     target_pct.values.shape
... )
>>> target_shape
(2663, 3)

>>> order_records = pipeline_4_nb(
...     target_shape,
...     data.get("Open").values,
...     data.get("Close").values,
...     target_pct.values
... )
>>> len(order_records)
125

>>> target_shape = vbt.broadcast_shapes(
...     data.get("Open").values.shape,
...     data.get("Close").values.shape
... )
>>> target_shape
(2663,)

>>> target_shape = vbt.to_2d_shape(target_shape)  Target shape must always be two-dimensional
>>> target_shape
(2663, 1)

>>> order_records = pipeline_4_nb(
...     target_shape,
...     data.get("Open").values,
...     data.get("Close").values,
...     0.5
... )
len(order_records)
2663

>>> np.product(symbol_wrapper.shape_2d)
2663

>>> mult_data = vbt.YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     end="2022-01-01",
...     missing_index="drop"
... )

>>> mult_symbol_wrapper = mult_data.get_symbol_wrapper()
>>> mult_target_pct = pd.concat([
...     mult_symbol_wrapper.fill().vbt.set(0.5, every=every[i])
...     for i in range(len(every))
... ], axis=1, keys=every)  That's another way of constructing the target percentage array

>>> target_shape = vbt.broadcast_shapes(
...     vbt.tile_shape(mult_data.get("Open").values.shape, len(every)),  Since broadcasting doesn't support rotations, we can use tile_shape to tile the shape of open and close manually (don't tile the actual array!)
...     vbt.tile_shape(mult_data.get("Close").values.shape, len(every)), 
...     mult_target_pct.values.shape
... )
>>> target_shape
(1514, 6)

>>> order_records = pipeline_4_nb(
...     target_shape,
...     mult_data.get("Open").values,  There is no need to tile any array thanks to rotational indexing
...     mult_data.get("Close").values,
...     mult_target_pct.values,
...     rotate_cols=True
... )
>>> len(order_records)
142

open
close
>>> columns = pd.Index(["BTC-USD", "ETH-USD", "BNB-USD", "SOL-USD", "XRP-USD"])
>>> mono_grouper = vbt.Grouper(columns, group_by=[0, 0, 0, 1, 1])
>>> mono_grouper.get_group_lens()  Using Grouper.get_group_lens
array([3, 2])

0
1
>>> dist_grouper = vbt.Grouper(columns, group_by=[0, 1, 0, 1, 1])
>>> dist_grouper.get_group_lens()
ValueError: group_by must form monolithic groups

>>> group_lens = mono_grouper.get_group_lens()

>>> group_end_idxs = np.cumsum(group_lens)  Get the end column index of each group (excluding)
>>> group_start_idxs = group_end_idxs - group_lens  Get the start column index of each group (including)

>>> for group in range(len(group_lens)):  Iterate over all groups
...     from_col = group_start_idxs[group]
...     to_col = group_end_idxs[group]
...     Define here your logic per group
...
...     for col in range(from_col, to_col):  Iterate over all columns in the group
...         pass  Define here your logic per column in the group

>>> mono_grouper.get_group_map()
(array([0, 1, 2, 3, 4]), array([3, 2]))

>>> dist_grouper.get_group_map()
(array([0, 2, 1, 3, 4]), array([2, 3]))

2
3
>>> group_map = dist_grouper.get_group_map()

>>> group_idxs, group_lens = group_map
>>> group_start_idxs = np.cumsum(group_lens) - group_lens  Get the start index of each group in the first array

>>> for group in range(len(group_lens)):
...     group_len = group_lens[group]
...     start_idx = group_start_idxs[group]
...     col_idxs = group_idxs[start_idx : start_idx + group_len]  Get the column indices of the group in the first array
...     Define here your logic per group
... 
...     for k in range(len(col_idxs)):  Iterate over all column indices in the group
...         col = col_idxs[k]
...         Define here your logic per column in the group

[2, 0, 1]
>>> position = np.array([0.0, -10.0, 10.0])
>>> val_price = np.array([10.0, 5.0, 15.0])
>>> debt = np.array([0.0, 100.0, 0.0])
>>> locked_cash = np.array([0.0, 100.0, 0.0])
>>> order_value = np.empty(3, dtype=np.float_)

>>> for col in range(len(position)):
...     exec_state = vbt.pf_enums.ExecState(
...         cash=200.0,  Cash-related information is defined per group using a constant
...         position=position[col],  Position-related information is defined per column using an array
...         debt=debt[col],
...         locked_cash=locked_cash[col],
...         free_cash=0.0,
...         val_price=val_price[col],
...         value=100.0
...     )
...     order_value[col] = vbt.pf_nb.approx_order_value_nb(
...         exec_state=exec_state,
...         size=0.,
...         size_type=vbt.pf_enums.SizeType.TargetAmount,
...         direction=vbt.pf_enums.Direction.Both
...     )

>>> order_value  Positive number means outbound cash flow, negative number means inbound cash flow
array([0., 50., -150.])

>>> from vectorbtpro.utils.array_ import insert_argsort_nb

>>> call_seq = np.array([0, 1, 2])  We should always start with a simple range
>>> insert_argsort_nb(order_value, call_seq)
>>> call_seq
array([2, 0, 1])

>>> for k in range(len(call_seq)):  When working with group lengths
...     c = call_seq[k]
...     col = from_col + c

>>> for k in range(len(call_seq)):  When working with a group map
...     c = call_seq[k]
...     col = col_idxs[c]

k
c
col
auto_call_seq
>>> @njit
... def pipeline_5_nb(
...     target_shape,  Second number in the target shape tracks assets (x parameter combinations) - it doesn't track groups!
...     group_lens,  The group lengths array must have the same number of elements as we have groups, while the sum of this array must yield the number of columns in target_shape
...     open,
...     close, 
...     target_pct,  Target allocations must be provided per asset, thus the array should broadcast against target_shape
...     init_cash=100,
...     auto_call_seq=True,
...     rotate_cols=False
... ):
...     init_cash_ = vbt.to_1d_array_nb(np.asarray(init_cash))
...     open_ = vbt.to_2d_array_nb(np.asarray(open))
...     close_ = vbt.to_2d_array_nb(np.asarray(close))
...     target_pct_ = vbt.to_2d_array_nb(np.asarray(target_pct))
...     alloc = np.empty(target_shape, dtype=np.float_)  Allocations must be filled per asset, thus the same shape as target_shape
...
...     group_end_idxs = np.cumsum(group_lens)
...     group_start_idxs = group_end_idxs - group_lens
...
...     for group in range(len(group_lens)):  Iterate over groups
...         group_len = group_lens[group]
...         from_col = group_start_idxs[group]
...         to_col = group_end_idxs[group]
...
...         Here comes the creation of various arrays that should exist only per group, such as the cash balance, position size per asset, and other state information. Remember that different groups represent independent, isolated tests, and shouldn't be connected by any means!
...         init_cash_elem = vbt.flex_select_1d_pc_nb(
...             init_cash_, group, rotate_cols=rotate_cols)
...     
...         last_position = np.full(group_len, 0.0, dtype=np.float_)  We can't create a single instance of ExecState like we did before because an order execution state contains information per asset, thus we need to keep track of its fields using separate variables (constants for data per group, arrays for data per asset)
...         last_debt = np.full(group_len, 0.0, dtype=np.float_)
...         last_locked_cash = np.full(group_len, 0.0, dtype=np.float_)
...         cash_now = float(init_cash_elem)
...         free_cash_now = float(init_cash_elem)
...
...         order_value = np.empty(group_len, dtype=np.float_)  We could have also created those two arrays at each bar, but frequent array creation slows down the simulation. A better practice is to create an array only once and re-fill it as many times as we want.
...         call_seq = np.empty(group_len, dtype=np.int_)
... 
...         for i in range(target_shape[0]):  Iterate over time steps (bars) as we did in previous pipelines
...             Calculate the value of each group (= portfolio value) by iterating over the assets of the group and adding their position value to the current cash balance
...             value_now = cash_now
...             for c in range(group_len):
...                 col = from_col + c
...                 open_elem = vbt.flex_select_nb(
...                     open_, i, col, rotate_cols=rotate_cols)
...                 value_now += last_position[c] * open_elem  Last position array exists only for this group, thus we're using c, not col!
...         
...             Prepare the order value and call sequence arrays
...             for c in range(group_len):
...                 col = from_col + c
...                 open_elem = vbt.flex_select_nb(
...                     open_, i, col, rotate_cols=rotate_cols)
...                 target_pct_elem = vbt.flex_select_nb(
...                     target_pct_, i, col, rotate_cols=rotate_cols)
...                 exec_state = vbt.pf_enums.ExecState(
...                     cash=cash_now,
...                     position=last_position[c],
...                     locked_cash=last_locked_cash[c],
...                     debt=last_debt[c],
...                     free_cash=free_cash_now,
...                     val_price=open_elem,
...                     value=value_now,
...                 )
...                 order_value[c] = vbt.pf_nb.approx_order_value_nb(  Approximate the value of a potential order. We're at the beginning of the bar, thus we use the open price.
...                     exec_state=exec_state,
...                     size=target_pct_elem,
...                     size_type=vbt.pf_enums.SizeType.TargetPercent,
...                     direction=vbt.pf_enums.Direction.Both
...                 )
...                 call_seq[c] = c  Write the current column index within this group. This will connect order values to column indices.
... 
...             if auto_call_seq:
...                 vbt.pf_nb.insert_argsort_nb(order_value, call_seq)  Sort both arrays in-place such that column indices associated with a lower order value appear first in the call sequence
... 
...             for k in range(len(call_seq)):  Next we want to execute orders, hence iterate over the newly sorted call sequence
...                 c = call_seq[k]  Get the associated column index within the group
...                 col = from_col + c  Get the associated global column index
...
...                 open_elem = vbt.flex_select_nb(
...                     open_, i, col, rotate_cols=rotate_cols)
...                 close_elem = vbt.flex_select_nb(
...                     close_, i, col, rotate_cols=rotate_cols)
...                 target_pct_elem = vbt.flex_select_nb(
...                     target_pct_, i, col, rotate_cols=rotate_cols)
...
...                 if not np.isnan(target_pct_elem):  Perform the same logic as in the previous pipeline
...                     order = vbt.pf_nb.order_nb(
...                         size=target_pct_elem,
...                         price=close_elem,
...                         size_type=vbt.pf_enums.SizeType.TargetPercent,
...                         direction=vbt.pf_enums.Direction.Both
...                     )
...                     exec_state = vbt.pf_enums.ExecState(
...                         cash=cash_now,
...                         position=last_position[c],
...                         debt=last_debt[c],
...                         locked_cash=last_locked_cash[c],
...                         free_cash=free_cash_now,
...                         val_price=open_elem,
...                         value=value_now,
...                     )
...                     _, new_exec_state = vbt.pf_nb.process_order_nb(
...                         group=group,
...                         col=col,
...                         i=i,
...                         exec_state=exec_state,
...                         order=order
...                     )
...                     cash_now = new_exec_state.cash
...                     free_cash_now = new_exec_state.free_cash
...                     value_now = new_exec_state.value
...                     last_position[c] = new_exec_state.position
...                     last_debt[c] = new_exec_state.debt
...                     last_locked_cash[c] = new_exec_state.locked_cash
...
...             Calculate the group value once again, but now using the updated position and the close price. Note that we are running this outside of the np.nan check since we want to track the allocation at each single bar.
...             value_now = cash_now
...             for c in range(group_len):
...                 col = from_col + c
...                 close_elem = vbt.flex_select_nb(
...                     close_, i, col, rotate_cols=rotate_cols)
...                 value_now += last_position[c] * close_elem
...
...             Calculate the current allocation of each asset, and write it to the output array
...             for c in range(group_len):
...                 col = from_col + c
...                 close_elem = vbt.flex_select_nb(
...                     close_, i, col, rotate_cols=rotate_cols)
...                 alloc[i, col] = last_position[c] * close_elem / value_now
... 
...     return alloc

target_shape
target_shape
target_shape
c
col
np.nan
>>> mult_target_pct = mult_symbol_wrapper.fill()
>>> mult_target_pct.vbt.set([[0.7, 0.3]], every="MS", inplace=True)
>>> grouper = vbt.Grouper(mult_symbol_wrapper.columns, group_by=True)
>>> group_lens = grouper.get_group_lens()

>>> target_shape = vbt.broadcast_shapes(
...     mult_data.get("Open").values.shape,
...     mult_data.get("Close").values.shape,
...     mult_target_pct.values.shape
... )
>>> target_shape
(1514, 2)

>>> alloc = pipeline_5_nb(
...     target_shape,
...     group_lens,
...     mult_data.get("Open").values,
...     mult_data.get("Close").values,
...     mult_target_pct.values
... )
>>> alloc = mult_symbol_wrapper.wrap(alloc)
>>> alloc.vbt.plot(
...    trace_kwargs=dict(stackgroup="one"),
...    use_gl=False
... ).show()

>>> alloc = pipeline_5_nb(
...     target_shape,
...     group_lens,
...     mult_data.get("Open").values,
...     mult_data.get("Close").values,
...     mult_target_pct.values,
...     auto_call_seq=False
... )
>>> alloc = mult_symbol_wrapper.wrap(alloc)
>>> alloc.vbt.plot(
...    trace_kwargs=dict(stackgroup="one"),
...    use_gl=False
... ).show()

group_by=False
>>> groups = pd.Index([0, 0, 1, 1], name="group")
>>> target_alloc = pd.Index([0.7, 0.3, 0.5, 0.5], name="target_alloc")

>>> final_columns = vbt.stack_indexes((  Build a new column hierarchy with three levels: groups, weights, and assets. Each level must have the same length.
...     groups,
...     target_alloc, 
...     vbt.tile_index(mult_symbol_wrapper.columns, 2)
... ))
>>> final_wrapper = mult_symbol_wrapper.replace(  Create a new (grouped) wrapper with the new column hierarchy
...     columns=final_columns,
...     group_by="group"
... )
>>> mult_target_pct = final_wrapper.fill(group_by=False)  Create the target percentage array of the same shape as the new wrapper and fill with NaN
>>> mult_target_pct.vbt.set(target_alloc.values[None], every="MS", inplace=True)
>>> group_lens = final_wrapper.grouper.get_group_lens()

>>> n_groups = final_wrapper.grouper.get_group_count()
>>> target_shape = vbt.broadcast_shapes(
...     vbt.tile_shape(mult_data.get("Open").values.shape, n_groups),  We're using rotational indexing - tile the shapes of open and close by the number of groups for the shapes to broadcast nicely and build the target shape
...     vbt.tile_shape(mult_data.get("Close").values.shape, n_groups), 
...     mult_target_pct.values.shape
... )
>>> target_shape
(1514, 4)

>>> alloc = pipeline_5_nb(
...     target_shape,
...     group_lens,
...     mult_data.get("Open").values,
...     mult_data.get("Close").values, 
...     mult_target_pct.values,
...     rotate_cols=True
... )
>>> alloc = mult_target_pct.vbt.wrapper.wrap(alloc)
>>> alloc
group                                       0                   1          
target_alloc                    0.7       0.3       0.5       0.5
symbol                      BTC-USD   ETH-USD   BTC-USD   ETH-USD
Date                                                             
2017-11-09 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
2017-11-10 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
2017-11-11 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
2017-11-12 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
2017-11-13 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
...                             ...       ...       ...       ...
2021-12-27 00:00:00+00:00  0.703817  0.296183  0.504464  0.495536
2021-12-28 00:00:00+00:00  0.703452  0.296548  0.504026  0.495974
2021-12-29 00:00:00+00:00  0.708035  0.291965  0.509543  0.490457
2021-12-30 00:00:00+00:00  0.706467  0.293533  0.507650  0.492350
2021-12-31 00:00:00+00:00  0.704346  0.295654  0.505099  0.494901

[1514 rows x 4 columns]

open
close
>>> from collections import namedtuple

>>> SimContext = namedtuple("SimContext", [
...     "open",  Arguments passed to the simulator
...     "high",
...     "low",
...     "close",
...     "init_cash",
...     "col",  Loop variables
...     "i",
...     "price_area",  State information, either unpacked (marginally faster) or in form of named tuples (more convenient)
...     "exec_state"
... ])

>>> @njit
... def pipeline_6_nb(
...     open, high, low, close, 
...     order_func_nb, order_args=(), 
...     init_cash=100
... ):
...     order_records = np.empty(close.shape, dtype=vbt.pf_enums.order_dt)
...     order_counts = np.full(close.shape[1], 0, dtype=np.int_)
...
...     for col in range(close.shape[1]):
...         exec_state = vbt.pf_enums.ExecState(
...             cash=float(init_cash),
...             position=0.0,
...             debt=0.0,
...             locked_cash=0.0,
...             free_cash=float(init_cash),
...             val_price=np.nan,
...             value=np.nan
...         )
...
...         for i in range(close.shape[0]):
...             val_price = open[i, col]
...             value = exec_state.cash + val_price * exec_state.position
...
...             price_area = vbt.pf_enums.PriceArea(
...                 open[i, col],
...                 high[i, col],
...                 low[i, col],
...                 close[i, col]
...             )
...             exec_state = vbt.pf_enums.ExecState(
...                 cash=exec_state.cash,
...                 position=exec_state.position,
...                 debt=exec_state.debt,
...                 locked_cash=exec_state.locked_cash,
...                 free_cash=exec_state.free_cash,
...                 val_price=val_price,
...                 value=value
...             )
...             sim_ctx = SimContext(  Initialize the simulation context (= creates a named tuple)
...                 open=open,
...                 high=high,
...                 low=low,
...                 close=close,
...                 init_cash=init_cash,
...                 col=col,
...                 i=i,
...                 price_area=price_area,
...                 exec_state=exec_state
...             )
...             order = order_func_nb(sim_ctx, *order_args)  Call the UDF by first passing the context and then any user-defined arguments
...             _, exec_state = vbt.pf_nb.process_order_nb(
...                 col, col, i,
...                 exec_state=exec_state,
...                 order=order,
...                 price_area=price_area,
...                 order_records=order_records,
...                 order_counts=order_counts
...             )
...         
...     return vbt.nb.repartition_nb(order_records, order_counts)

>>> @njit  Don't forget to decorate your order function with @njit as well!
... def signal_order_func_nb(c, entries, exits):
...     if entries[c.i, c.col] and c.exec_state.position == 0:  We can access any information similarly to accessing attributes of any Python object
...         return vbt.pf_nb.order_nb()
...     if exits[c.i, c.col] and c.exec_state.position > 0:
...         return vbt.pf_nb.close_position_nb()
...     return vbt.pf_nb.order_nothing_nb()

>>> broadcasted_args = vbt.broadcast(  Neither prices nor signals utilize flexible indexing, thus we need to broadcast them to the full shape
...     dict(
...         open=data.get("Open").values,
...         high=data.get("High").values,
...         low=data.get("Low").values,
...         close=data.get("Close").values,
...         entries=entries.values,  These two arrays were generated for the first pipeline
...         exits=exits.values
...     ),
...     min_ndim=2
... )

>>> pipeline_6_nb(
...     broadcasted_args["open"],
...     broadcasted_args["high"], 
...     broadcasted_args["low"], 
...     broadcasted_args["close"], 
...     signal_order_func_nb,
...     order_args=(
...         broadcasted_args["entries"],
...         broadcasted_args["exits"]
...     )
... )
array([( 0, 0,  300, 0.34786966,   287.46398926, 0., 0),
       ( 1, 0,  362, 0.34786966,   230.64399719, 0., 1),
       ( 2, 0,  406, 0.26339233,   304.61801147, 0., 0),
       ( 3, 0, 1290, 0.26339233,  6890.52001953, 0., 1),
       ( 4, 0, 1680, 0.33210511,  5464.86669922, 0., 0),
       ( 5, 0, 1865, 0.33210511,  9244.97265625, 0., 1),
       ( 6, 0, 1981, 0.31871477,  9633.38671875, 0., 0),
       ( 7, 0, 2016, 0.31871477,  6681.06298828, 0., 1),
       ( 8, 0, 2073, 0.2344648 ,  9081.76171875, 0., 0),
       ( 9, 0, 2467, 0.2344648 , 35615.87109375, 0., 1),
       (10, 0, 2555, 0.17333543, 48176.34765625, 0., 0)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})

@njit
np.cumsum
-
+
*
/
args
*args
fastmath=True
>>> test_data = vbt.RandomOHLCData.pull(
...     start="2020-01-01", 
...     end="2021-01-01",
...     freq="1T",  Set tick frequency to 1 minute
...     std=0.0001,  Set tick volatility to 0.01%
...     symmetric=True  Use symmetric returns (50% negative return == 100% positive return)
... )
>>> test_data.resample("1d").plot().show()  Plot to ensure that the generated data is realistic. Here we're resampling to daily frequency for faster plotting.

>>> test_open = test_data.get("Open").values[:, None]  Get a column, extract the NumPy array, and expand the array to two dimensions (omit the last step if your data is already two-dimensional!)
>>> test_high = test_data.get("High").values[:, None]
>>> test_low = test_data.get("Low").values[:, None]
>>> test_close = test_data.get("Close").values[:, None]
>>> test_entries = np.full(test_data.get_symbol_wrapper().shape, False)[:, None]
>>> test_exits = np.full(test_data.get_symbol_wrapper().shape, False)[:, None]
>>> test_entries[0::2] = True  Place an entry at each second bar starting from the first bar
>>> test_exits[1::2] = True  Place an exit at each second bar starting from the second bar
>>> del test_data  Delete the data object to release memory

>>> test_entries.shape
(527041, 1)

>>> %%timeit  This magic command works only in a Jupyter environment and only if you place this command at the beginning of the cell. If you're running the code as a script, use the timeit module.
>>> pipeline_6_nb(
...     test_open, 
...     test_high, 
...     test_low, 
...     test_close, 
...     signal_order_func_nb,
...     order_args=(
...         test_entries, 
...         test_exits
...     )
... )
79.4 ms ± 290 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

timeit
>>> @njit
... def subopt_signal_order_func_nb(c, entries, exits):
...     _ = np.empty(0)  Here
...
...     if entries[c.i, c.col] and c.exec_state.position == 0:
...         return vbt.pf_nb.order_nb()
...     if exits[c.i, c.col] and c.exec_state.position > 0:
...         return vbt.pf_nb.close_position_nb()
...     return vbt.pf_nb.order_nothing_nb()

>>> %%timeit
>>> pipeline_6_nb(
...     test_open, 
...     test_high, 
...     test_low, 
...     test_close, 
...     subopt_signal_order_func_nb,
...     order_args=(
...         test_entries, 
...         test_exits
...     )
... )
130 ms ± 675 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

range
numba.prange
parallel=True
@njit
>>> rom numba import prange

>>> arr = np.random.uniform(size=(1000000, 10))

>>> @njit
... def expanding_max_nb(arr):
...     out = np.empty_like(arr, dtype=np.float_)
...     for col in range(arr.shape[1]):
...         maxv = -np.inf
...         for i in range(arr.shape[0]):
...             if arr[i, col] > maxv:
...                 maxv = arr[i, col]
...             out[i, col] = maxv
...     return out

>>> %timeit expanding_max_nb(arr)
40.7 ms ± 558 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> @njit(parallel=True)  Here's the first change
... def parallel_expanding_max_nb(arr):
...     out = np.empty_like(arr, dtype=np.float_)
...     for col in prange(arr.shape[1]):  Here's the second change
...         maxv = -np.inf
...         for i in range(arr.shape[0]):
...             if arr[i, col] > maxv:
...                 maxv = arr[i, col]
...             out[i, col] = maxv
...     return out

>>> %timeit parallel_expanding_max_nb(arr)
26.6 ms ± 437 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

cache=True
@njit
order_func_nb
__pycache__
rm -rf __pycache__
>>> from numba.pycc import CC
>>> cc = CC('pipeline_5')  Initialize a new module

>>> sig = "f8[:, :](" \ Function should return a two-dimensional array of 64-bit floating-point data type (allocations)
...       "UniTuple(i8, 2), " \ Tuple with two 64-bit integers (target_shape)
...       "i8[:], " \ One-dimensional array of 64-bit integer data type (group_lens)
...       "f8[:, :], " \ Two-dimensional array of 64-bit floating-point data type (open, close, and target_pct)
...       "f8[:, :], " \
...       "f8[:, :], " \
...       "f8[:], " \ One-dimensional array of 64-bit floating-point data type (init_cash)
...       "b1, " \ Boolean value (auto_call_seq and rotate_cols)
...       "b1" \
...       ")"

>>> cc.export('pipeline_5_nb', sig)(pipeline_5_nb)   Register the function with the provided signature. You can bind multiple signatures to the same function.
>>> cc.compile() Compile the module

target_shape
group_lens
open
close
target_pct
init_cash
auto_call_seq
rotate_cols
pipeline_5
pipeline_5.cpython-37m-darwin.so
pipeline_5_nb
>>> import pipeline_5

>>> alloc = pipeline_5.pipeline_5_nb(
...     shape_2d,
...     group_lens,
...     vbt.to_2d_array(open_2d).astype("f8", copy=False),  This makes sure that the array has a proper dimensionality and data type
...     vbt.to_2d_array(close_2d).astype("f8", copy=False), 
...     vbt.to_2d_array(target_pct_2d).astype("f8", copy=False),
...     vbt.to_1d_array(100).astype("f8", copy=False),  Keyword arguments with default values must be provided explicitly
...     True,
...     False
... )

init_cash
100
>>> import numpy as np
>>> import pandas as pd
>>> from itertools import product

>>> price = np.array([21, 20, 22, 21])
>>> returns = (price[1:] - price[:-1]) / price[:-1]
>>> permutations = list(product([False, True], repeat=len(returns)))
>>> total_return = np.prod(1 + np.where(permutations, returns, -returns), axis=1) - 1
>>> pd.Series(total_return, index=permutations).sort_values(ascending=False)
(False, True, False)     0.204762
(False, True, True)      0.100000
(True, True, False)      0.095238
(True, True, True)       0.000000
(False, False, False)   -0.014286
(False, False, True)    -0.100000
(True, False, False)    -0.103896
(True, False, True)     -0.181818
dtype: float64

2^n
2^365
7.515336e+109
init_cash
size
size
Portfolio.from_orders
BTC-USD
ETH-USD
2020-01-01
BTC-USD
ETH-USD
365 * 2 = 730
signal_func_nb
>>> import numpy as np
>>> import pandas as pd
>>> from numba import njit
>>> import vectorbtpro as vbt

>>> @njit
... def signal_func_nb(c):
...     return False, False, False, False

>>> close = pd.DataFrame({
...     "BTC-USD": [20594.29, 20719.41, 19986.60, 21084.64], 
...     "ETH-USD": [1127.51, 1125.37, 1051.32, 1143.20],
...     "DOT-USD": [7.88, 7.74, 7.41, 7.78], 
...     "BNB-USD": [216.90, 219.67, 214.23, 228.92]
... })

>>> pf = vbt.Portfolio.from_signals(
...     close=close, 
...     signal_func_nb=signal_func_nb
... )
>>> pf.order_records
array([], dtype={...})

@njit
signal_func_nb
jitted=False
from_signals
>>> @njit
... def signal_func_nb(c):
...     print(c.col, c.i)
...     return False, False, False, False

>>> pf = vbt.Portfolio.from_signals(
...     close=close[["BTC-USD", "ETH-USD"]], 
...     signal_func_nb=signal_func_nb
... )
0 0
0 1
0 2
0 3
1 0
1 1
1 2
1 3

BTC-USD
ETH-USD
>>> @njit
... def signal_func_nb(c):
...     print(c.group, c.col, c.i)
...     return False, False, False, False

>>> pf = vbt.Portfolio.from_signals(
...     close=close, 
...     signal_func_nb=signal_func_nb,
...     group_by=[0, 0, 1, 1],
...     cash_sharing=True
... )
0 0 0
0 1 0
0 0 1
0 1 1
0 0 2
0 1 2
0 0 3
0 1 3
1 2 0
1 3 0
1 2 1
1 3 1
1 2 2
1 3 2
1 2 3
1 3 3

c.index[c.i]
True
False
True
True
upon_long_conflict
>>> vbt.pf_nb.resolve_signal_conflict_nb(
...     position_now=20,
...     is_entry=True,
...     is_exit=True,
...     direction=vbt.pf_enums.Direction.LongOnly,
...     conflict_mode=vbt.pf_enums.ConflictMode.Adjacent
... )
(True, False)

upon_dir_conflict
>>> vbt.pf_nb.resolve_dir_conflict_nb(
...     position_now=20,
...     is_long_entry=True,
...     is_short_entry=True,
...     upon_dir_conflict=vbt.pf_enums.DirectionConflictMode.Short,
... )
(False, True)

upon_opposite_entry
>>> vbt.pf_nb.resolve_opposite_entry_nb(
...     position_now=20,
...     is_long_entry=False,
...     is_long_exit=False,
...     is_short_entry=True,
...     is_short_exit=False,
...     upon_opposite_entry=vbt.pf_enums.OppositeEntryMode.Close,
...     accumulate=False,  This argument can be True, False, or any of AccumulationMode 
... )
(False, True, False, False, 0)

True
False
>>> vbt.pf_nb.signal_to_size_nb(
...     position_now=20,
...     val_price_now=20594.29,  The latest asset price known
...     value_now=411885.80,  The latest group value known
...     is_long_entry=False,
...     is_long_exit=True,
...     is_short_entry=False,
...     is_short_exit=False,
...     size=0.1,  Default value for this element
...     size_type=vbt.pf_enums.SizeType.ValuePercent,  Default value for this element
...     accumulate=False  Default value for this element
... )
(-20.0, 0, 0)

size
>>> vbt.pf_nb.signal_to_size_nb(
...     position_now=20,
...     val_price_now=20594.29,
...     value_now=411885.80,
...     is_long_entry=False,
...     is_long_exit=False,
...     is_short_entry=True,  Reverse the long position
...     is_short_exit=False,
...     size=0.1,
...     size_type=vbt.pf_enums.SizeType.ValuePercent,
...     accumulate=False
... )
(-22.0, 0, 2)

size * value_now / val_price_now = 2.0
order_type
price
-np.inf
np.inf
c.limit_info_dt
c.limit_info_dt["init_price"][c.col] = new_price
2020-01-01
2020-01-02
>>> vbt.pf_nb.check_limit_expired_nb(
...     creation_idx=0,
...     i=1,
...     tif=vbt.dt.to_ns(pd.Timedelta("36h")),  Index and time deltas must be converted into nanoseconds
...     index=vbt.dt.to_ns(pd.date_range("2020-01-01", periods=3)),
...     freq=vbt.dt.to_ns(pd.Timedelta("1d"))
... )
(False, True)

>>> vbt.pf_nb.check_limit_expired_nb(
...     creation_idx=0,
...     i=1,
...     tif=vbt.dt.to_ns(pd.Timedelta("24h")),
...     index=vbt.dt.to_ns(pd.date_range("2020-01-01", periods=3)),
...     freq=vbt.dt.to_ns(pd.Timedelta("1d"))
... )
(True, True)

9.5
>>> vbt.pf_nb.check_limit_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     price=9.5,
...     size=2.0
... )
(9.5, False, True)

11
>>> vbt.pf_nb.check_limit_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     price=11.0,
...     size=2.0
... )
(10.0, True, True)

upon_adj_limit_conflict
upon_opp_limit_conflict
>>> vbt.pf_nb.resolve_pending_conflict_nb(
...     is_pending_long=True,
...     is_user_long=False,
...     upon_adj_conflict=vbt.pf_enums.PendingConflictMode.KeepIgnore,
...     upon_opp_conflict=vbt.pf_enums.PendingConflictMode.CancelIgnore,
... )
(False, False)

stop_entry_price
10 * (1 - 0.1) = 9
>>> vbt.pf_nb.check_stop_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     is_position_long=True,
...     init_price=10.0,
...     stop=0.1
... )
(9.0, False, True)

>>> vbt.pf_nb.check_stop_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     is_position_long=True,
...     init_price=12.0,
...     stop=0.1
... )
(10.0, True, True)

hit_below
False
can_use_ohlc
stop_exit_type
StopExitType.Reverse
>>> vbt.pf_nb.generate_stop_signal_nb(
...     position_now=20,
...     stop_exit_type=vbt.pf_enums.StopExitType.Reverse
... )
(False, False, True, False, 0)

True
False
stop_exit_price
StopExitPrice.Close
>>> vbt.pf_nb.resolve_stop_exit_price_nb(
...     stop_price=9.0,
...     close=10.5,
...     stop_exit_price=vbt.pf_enums.StopExitPrice.Stop
... )
9.0

>>> vbt.pf_nb.resolve_stop_exit_price_nb(
...     stop_price=9.0,
...     close=10.5,
...     stop_exit_price=9.5
... )
9.5

upon_stop_update
StopUpdateMode.Override
>>> vbt.pf_nb.should_update_stop_nb(
...     new_stop=0.1,
...     upon_stop_update=vbt.pf_enums.StopUpdateMode.Override
... )
True

>>> vbt.pf_nb.should_update_stop_nb(
...     new_stop=np.nan,
...     upon_stop_update=vbt.pf_enums.StopUpdateMode.Override
... )
False

StopUpdateMode.OverrideNaN
>>> vbt.pf_nb.should_update_stop_nb(
...     new_stop=np.nan,
...     upon_stop_update=vbt.pf_enums.StopUpdateMode.OverrideNaN
... )
True

upon_stop_update
upon_adj_stop_conflict
upon_opp_stop_conflict
upon_adj_stop_conflict
PendingConflictMode.CancelExecute
2 * 2 * 3 = 12
2 * 2 * 2 * 2 = 16
BTCUSDT
ETHUSDT
>>> data = vbt.BinanceData.pull(["BTCUSDT", "ETHUSDT"])

>>> sub_data = data.loc["2021-02-18":"2021-02-24"]

>>> sub_data.plot(symbol="BTCUSDT").show()

>>> pf = vbt.Portfolio.from_signals(sub_data)  Even though the first argument expects the closing price (close), we can pass the entire data instance, from which the OHLC features will be extracted automatically
>>> pf.orders.count()
symbol
BTCUSDT    0
ETHUSDT    0
Name: count, dtype: int64

close
False
>>> X = True  Shortcuts for better readability
>>> O = False

>>> entries = pd.Series([X, O, O, O, O, O, O])
>>> exits   = pd.Series([O, O, O, X, O, O, O])
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,  We could have also specified it as a two-dimensional NumPy array with one column
...     exits=exits
... )
>>> pf.orders.records_readable
   Order Id   Column              Signal Index            Creation Index  \\
0         0  BTCUSDT 2021-02-18 00:00:00+00:00 2021-02-18 00:00:00+00:00   
1         1  BTCUSDT 2021-02-21 00:00:00+00:00 2021-02-21 00:00:00+00:00   
2         0  ETHUSDT 2021-02-18 00:00:00+00:00 2021-02-18 00:00:00+00:00   
3         1  ETHUSDT 2021-02-21 00:00:00+00:00 2021-02-21 00:00:00+00:00   

                 Fill Index      Size     Price  Fees  Side    Type Stop Type  
0 2021-02-18 00:00:00+00:00  0.001940  51552.60   0.0   Buy  Market      None  
1 2021-02-21 00:00:00+00:00  0.001940  57408.57   0.0  Sell  Market      None  
2 2021-02-18 00:00:00+00:00  0.051557   1939.61   0.0   Buy  Market      None  
3 2021-02-21 00:00:00+00:00  0.051557   1933.53   0.0  Sell  Market      None  

BTCUSDT
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00194  0.051557
2021-02-20 00:00:00+00:00  0.00194  0.051557
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000

ETHUSDT
BTCUSDT
>>> entries = pd.Series([X, O, O, O, O, O, O])
>>> exits = pd.DataFrame({
...     0: [O, O, O, X, O, O, O],
...     1: [O, O, X, O, O, O, O],
... })  We could have also used symbols as column names. Defining rows and columns as a simple range of values (from 0 to n) will ignore the labels and broadcast only using shapes.
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,
...     exits=exits
... )
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00194  0.051557
2021-02-20 00:00:00+00:00  0.00194  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000

0
n
ETHUSDT
BTCUSDT
>>> exits = sub_data.symbol_wrapper.fill(False)  Create an array with the same number of columns as we have symbols, and fill it with False
>>> exits.loc["2021-02-21", "BTCUSDT"] = True
>>> exits.loc["2021-02-20", "ETHUSDT"] = True
>>> exits
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    False    False
2021-02-19 00:00:00+00:00    False    False
2021-02-20 00:00:00+00:00    False     True
2021-02-21 00:00:00+00:00     True    False
2021-02-22 00:00:00+00:00    False    False
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False

False
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=vbt.index_dict({0: True}),
...     exits=vbt.index_dict({
...         vbt.idx("2021-02-21", "BTCUSDT"): True,
...         vbt.idx("2021-02-20", "ETHUSDT"): True
...     })
... )
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00194  0.051557
2021-02-20 00:00:00+00:00  0.00194  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000

entries
exits
direction
Direction.LongOnly
signal_direction
direction
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,
...     exits=exits,
...     direction="both"  Can also be provided in the numeric format as Direction.Both
... )
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00194  0.051557
2021-02-20 00:00:00+00:00  0.00194 -0.051557
2021-02-21 00:00:00+00:00 -0.00194 -0.051557
2021-02-22 00:00:00+00:00 -0.00194 -0.051557
2021-02-23 00:00:00+00:00 -0.00194 -0.051557
2021-02-24 00:00:00+00:00 -0.00194 -0.051557

Direction.Both
BTCUSDT
ETHUSDT
>>> direction = pd.DataFrame([["longonly", "shortonly"]])
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,
...     exits=exits,
...     direction=direction
... )
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194 -0.051557
2021-02-19 00:00:00+00:00  0.00194 -0.051557
2021-02-20 00:00:00+00:00  0.00194  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000

ETHUSDT
>>> L = vbt.pf_enums.Direction.LongOnly  Shortcuts for better readability, numeric format
>>> S = vbt.pf_enums.Direction.ShortOnly

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),  Select one symbol of data
...     entries=  pd.Series([X, O, O, O, X, O, O]),
...     exits=    pd.Series([O, O, O, X, O, O, X]),
...     direction=pd.Series([L, L, L, L, S, S, S])
... )
>>> pf.assets
Open time
2021-02-18 00:00:00+00:00    0.001940
2021-02-19 00:00:00+00:00    0.001940
2021-02-20 00:00:00+00:00    0.001940
2021-02-21 00:00:00+00:00    0.000000
2021-02-22 00:00:00+00:00   -0.002059
2021-02-23 00:00:00+00:00   -0.002059
2021-02-24 00:00:00+00:00    0.000000
Freq: D, dtype: float64

short_entries
short_exits
entries
exits
direction
>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=      pd.Series([X, O, O, O, O, O, O]),
...     exits=        pd.Series([O, O, O, X, O, O, O]),
...     short_entries=pd.Series([O, O, O, O, X, O, O]),
...     short_exits=  pd.Series([O, O, O, O, O, O, X]),
... )
>>> pf.assets
Open time
2021-02-18 00:00:00+00:00    0.001940
2021-02-19 00:00:00+00:00    0.001940
2021-02-20 00:00:00+00:00    0.001940
2021-02-21 00:00:00+00:00    0.000000
2021-02-22 00:00:00+00:00   -0.002059
2021-02-23 00:00:00+00:00   -0.002059
2021-02-24 00:00:00+00:00    0.000000
Freq: D, dtype: float64

>>> @njit
... def signal_func_nb(c):
...     ts = c.index[c.i]  c.index returns an array with timestamps in the nanosecond format while c.i returns the current row. By applying the latter on the former, we can get the current timestamp.
...     if vbt.dt_nb.matches_date_nb(ts, 2021, 2, 18):  Use matches_date_nb to check whether the current day matches a date
...         return True, False, False, False  Signal functions must return a set of direction-aware signals
...     if vbt.dt_nb.matches_date_nb(ts, 2021, 2, 21):
...         return False, True, False, False
...     if vbt.dt_nb.matches_date_nb(ts, 2021, 2, 22):
...         return False, False, True, False
...     if vbt.dt_nb.matches_date_nb(ts, 2021, 2, 24):
...         return False, False, False, True
...     return False, False, False, False

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     signal_func_nb=signal_func_nb
... )
>>> pf.assets
Open time
2021-02-18 00:00:00+00:00    0.001940
2021-02-19 00:00:00+00:00    0.001940
2021-02-20 00:00:00+00:00    0.001940
2021-02-21 00:00:00+00:00    0.000000
2021-02-22 00:00:00+00:00   -0.002059
2021-02-23 00:00:00+00:00   -0.002059
2021-02-24 00:00:00+00:00    0.000000
Freq: D, dtype: float64

c.index
c.i
signal_args
>>> @njit
... def signal_func_nb(c, entries, exits, short_entries, short_exits):
...     long_entry = entries[c.i]
...     long_exit = exits[c.i]
...     short_entry = short_entries[c.i]
...     short_exit = short_exits[c.i]
...     return long_entry, long_exit, short_entry, short_exit

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         pd.Series([X, O, O, O, O, O, O]).values,
...         pd.Series([O, O, O, X, O, O, O]).values,
...         pd.Series([O, O, O, O, X, O, O]).values,
...         pd.Series([O, O, O, O, O, O, X]).values
...     )
... )
>>> pf.assets
Open time
2021-02-18 00:00:00+00:00    0.001940
2021-02-19 00:00:00+00:00    0.001940
2021-02-20 00:00:00+00:00    0.001940
2021-02-21 00:00:00+00:00    0.000000
2021-02-22 00:00:00+00:00   -0.002059
2021-02-23 00:00:00+00:00   -0.002059
2021-02-24 00:00:00+00:00    0.000000
Freq: D, dtype: float64

>>> @njit
... def signal_func_nb(c, entries, exits, short_entries, short_exits):
...     long_entry = vbt.pf_nb.select_nb(c, entries)
...     long_exit = vbt.pf_nb.select_nb(c, exits)
...     short_entry = vbt.pf_nb.select_nb(c, short_entries)
...     short_exit = vbt.pf_nb.select_nb(c, short_exits)
...     return long_entry, long_exit, short_entry, short_exit

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.to_2d_array(pd.Series([X, O, O, O, O, O, O])),
...         vbt.to_2d_array(pd.Series([O, O, O, X, O, O, O])),
...         vbt.to_2d_array(pd.Series([O, O, O, O, X, O, O])),
...         vbt.to_2d_array(pd.Series([O, O, O, O, O, O, X]))
...     )
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.001940  0.051557
2021-02-20 00:00:00+00:00  0.001940  0.051557
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00 -0.002059 -0.056080
2021-02-23 00:00:00+00:00 -0.002059 -0.056080
2021-02-24 00:00:00+00:00  0.000000  0.000000

broadcast_named_args
signal_args
>>> entries = pd.Series({pd.Timestamp("2021-02-18 00:00:00+00:00"): True})
>>> exits = pd.Series({pd.Timestamp("2021-02-21 00:00:00+00:00"): True})
>>> short_entries = pd.Series({pd.Timestamp("2021-02-22 00:00:00+00:00"): True})
>>> short_exits = pd.Series({pd.Timestamp("2021-02-24 00:00:00+00:00"): True})

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("entries"),  entries is recognized as a key in broadcast_named_args
...         vbt.Rep("exits"),
...         vbt.Rep("short_entries"),
...         vbt.Rep("short_exits")
...     ),
...     broadcast_named_args=dict(
...         entries=entries,
...         exits=exits,
...         short_entries=short_entries,
...         short_exits=short_exits
...     )
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.001940  0.051557
2021-02-20 00:00:00+00:00  0.001940  0.051557
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00 -0.002059 -0.056080
2021-02-23 00:00:00+00:00 -0.002059 -0.056080
2021-02-24 00:00:00+00:00  0.000000  0.000000

entries
broadcast_named_args
entries
exits
short_entries
short_exits
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("entries"),
...         vbt.Rep("exits"),
...         vbt.Rep("short_entries"),
...         vbt.Rep("short_exits")
...     ),
...     broadcast_named_args=dict(
...         entries=      vbt.index_dict({"2021-02-18": True, "_def": False}),
...         exits=        vbt.index_dict({"2021-02-21": True, "_def": False}),
...         short_entries=vbt.index_dict({"2021-02-22": True, "_def": False}),
...         short_exits=  vbt.index_dict({"2021-02-24": True, "_def": False})
...     )
... )

wait
wait
>>> @njit
... def signal_func_nb(c, fast_sma, slow_sma, wait):
...     curr_wait = vbt.pf_nb.select_nb(c, wait)  Since the confirmation period is an array-like parameter, get the value defined for the current row and column
...     i_wait = c.i - curr_wait  Get the row where the crossover should have taken place
...     if i_wait < 0:  If the current period is smaller than the confirmation period, return no signal
...         return False, False, False, False
...
...     if vbt.nb.iter_crossed_above_nb(fast_sma, slow_sma, i_wait, c.col):  Check whether the faster SMA has crossed the slower SMA at the current bar
...         cross_confirmed = True
...         for j in range(i_wait + 1, c.i + 1):  If true, loop over the confirmation period (including the current bar) and check whether the faster SMA has always stayed above the slower SMA for confirmation
...             if not vbt.nb.iter_above_nb(fast_sma, slow_sma, j, c.col):
...                 cross_confirmed = False
...                 break
...         if cross_confirmed:
...             return True, False, False, False
...
...     if vbt.nb.iter_crossed_below_nb(fast_sma, slow_sma, i_wait, c.col):  The same for the below-crossover
...         cross_confirmed = True
...         for j in range(i_wait + 1, c.i + 1):
...             if not vbt.nb.iter_below_nb(fast_sma, slow_sma, j, c.col):
...                 cross_confirmed = False
...                 break
...         if cross_confirmed:
...             return False, False, True, False
...
...     return False, False, False, False

>>> fast_sma = data.run("sma", 20, short_name="fast_sma").real
>>> slow_sma = data.run("sma", 50, short_name="slow_sma").real
>>> pf = vbt.Portfolio.from_signals(
...     data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("fast_sma"),
...         vbt.Rep("slow_sma"),
...         vbt.Rep("wait")
...     ),
...     broadcast_named_args=dict(
...         fast_sma=fast_sma,
...         slow_sma=slow_sma,
...         wait=0  Waiting period is a flexible parameter, that is, it must broadcast together with other arrays
...     )
... )
>>> pf.orders.count()
fast_sma_timeperiod  slow_sma_timeperiod  symbol 
20                   50                   BTCUSDT    40
                                          ETHUSDT    32
Name: count, dtype: int64

>>> n_crossed_above = fast_sma.vbt.crossed_above(slow_sma).sum()
>>> n_crossed_below = fast_sma.vbt.crossed_below(slow_sma).sum()
>>> n_crossed_above + n_crossed_below
fast_sma_timeperiod  slow_sma_timeperiod  symbol 
20                   50                   BTCUSDT    40
                                          ETHUSDT    32
Name: count, dtype: int64

>>> pf = vbt.Portfolio.from_signals(
...     data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("fast_sma"),
...         vbt.Rep("slow_sma"),
...         vbt.Rep("wait")
...     ),
...     broadcast_named_args=dict(
...         fast_sma=fast_sma,
...         slow_sma=slow_sma,
...         wait=vbt.Param([0, 1, 7, 30])
...     )
... )
>>> pf.orders.count()
wait  fast_sma_timeperiod  slow_sma_timeperiod  symbol 
0     20                   50                   BTCUSDT    40
                                                ETHUSDT    32
1     20                   50                   BTCUSDT    38
                                                ETHUSDT    32
7     20                   50                   BTCUSDT    36
                                                ETHUSDT    30
30    20                   50                   BTCUSDT    14
                                                ETHUSDT    16
Name: count, dtype: int64

>>> @njit
... def signal_func_nb(c, fast_sma, slow_sma, wait, temp_coi):  The temporary array is taken as a regular argument
...     if temp_coi[c.col] != -1:  Check whether there is a crossover index stored under this column
...         crossed_above = vbt.nb.iter_crossed_above_nb(
...             fast_sma, slow_sma, temp_coi[c.col], c.col
...         )
...         crossed_below = vbt.nb.iter_crossed_below_nb(
...             fast_sma, slow_sma, temp_coi[c.col], c.col
...         )
...         if crossed_above:  If true, check the type of the crossover and whether it can still be confirmed. If it cannot be confirmed at this bar, remove the index.
...             if not vbt.pf_nb.iter_above_nb(c, fast_sma, slow_sma):  Use generic iterative functions (starting with vbt.nb.iter_, see generic.iter_) when an operation must be performed on a custom row/column, and portfolio iterative functions (starting with vbt.pf_nb.iter_, see portfolio.iter_) to perform an operation on the current row/column taken from the context
...                 temp_coi[c.col] = -1
...         if crossed_below:
...             if not vbt.pf_nb.iter_below_nb(c, fast_sma, slow_sma):
...                 temp_coi[c.col] = -1
...
...     curr_wait = vbt.pf_nb.select_nb(c, wait)
...     if temp_coi[c.col] != -1:  If there is still an index in the temporary array, the crossover has been confirmed so far, thus check if the confirmation period is over, and if true, return the signal
...         if c.i - temp_coi[c.col] == curr_wait:
...             if crossed_above:
...                 temp_coi[c.col] = -1
...                 return True, False, False, False
...             if crossed_below:
...                 temp_coi[c.col] = -1
...                 return False, False, True, False
...     else:  If there is no crossover index, check whether there is a crossover at this bar, and if true, store it inside the temporary array and move on. If the confirmation period is zero, return the signal right away.
...         if vbt.pf_nb.iter_crossed_above_nb(c, fast_sma, slow_sma):
...             if curr_wait == 0:
...                 return True, False, False, False
...             temp_coi[c.col] = c.i
...         if vbt.pf_nb.iter_crossed_below_nb(c, fast_sma, slow_sma):
...             if curr_wait == 0:
...                 return False, False, True, False
...             temp_coi[c.col] = c.i
...
...     return False, False, False, False

>>> pf = vbt.Portfolio.from_signals(
...     data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("fast_sma"),
...         vbt.Rep("slow_sma"),
...         vbt.Rep("wait"),
...         vbt.RepEval("np.full(wrapper.shape_2d[1], -1)")  Create the temporary array temp_coi. Use an evaluation template to run a code as an expression after all the arrays have been broadcasted and the final shape has been established. Use wrapper to access the shape information.
...     ),
...     broadcast_named_args=dict(
...         fast_sma=fast_sma,
...         slow_sma=slow_sma,
...         wait=vbt.Param([0, 1, 7, 30])
...     )
... )
>>> pf.orders.count()
wait  fast_sma_timeperiod  slow_sma_timeperiod  symbol 
0     20                   50                   BTCUSDT    40
                                                ETHUSDT    32
1     20                   50                   BTCUSDT    38
                                                ETHUSDT    32
7     20                   50                   BTCUSDT    36
                                                ETHUSDT    30
30    20                   50                   BTCUSDT    14
                                                ETHUSDT    16
Name: count, dtype: int64

vbt.nb.iter_
vbt.pf_nb.iter_
temp_coi
wrapper
crossed_above_nb
self.crossed_above()
temp_coi
self.temp_coi
>>> pf = vbt.Portfolio.from_signals(sub_data, entries=True)  When signals are provided as a single value, the value gets broadcast against the entire data shape such that it appears under each single row and column
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00000  0.000000
2021-02-20 00:00:00+00:00  0.00000  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000

>>> pf = vbt.Portfolio.from_signals(sub_data, entries=True, exits=True)
>>> pf.asset_flow
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      0.0      0.0
2021-02-19 00:00:00+00:00      0.0      0.0
2021-02-20 00:00:00+00:00      0.0      0.0
2021-02-21 00:00:00+00:00      0.0      0.0
2021-02-22 00:00:00+00:00      0.0      0.0
2021-02-23 00:00:00+00:00      0.0      0.0
2021-02-24 00:00:00+00:00      0.0      0.0

upon_long_conflict
>>> pf = vbt.Portfolio.from_signals(
...     sub_data, 
...     entries=True, 
...     exits=True,
...     upon_long_conflict="entry"
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00000  0.000000
2021-02-20 00:00:00+00:00  0.00000  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000

upon_dir_conflict
>>> pf = vbt.Portfolio.from_signals(
...     sub_data, 
...     entries=True, 
...     exits=True,
...     direction="both",
...     upon_dir_conflict="short"
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00 -0.00194 -0.051557
2021-02-19 00:00:00+00:00  0.00000  0.000000
2021-02-20 00:00:00+00:00  0.00000  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000

>>> pf = vbt.Portfolio.from_signals(
...     sub_data, 
...     entries=True, 
...     exits=vbt.index_dict({0: False, "_def": True}),
...     short_entries=vbt.index_dict({0: False, "_def": True}),
...     short_exits=vbt.index_dict({0: False, "_def": True}),
...     upon_long_conflict="entry",
...     upon_short_conflict="entry",
...     upon_dir_conflict="opposite",
...     upon_opposite_entry="reverse"
... )
>>> pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00 -0.003880 -0.103114
2021-02-20 00:00:00+00:00  0.003884  0.105377
2021-02-21 00:00:00+00:00 -0.003889 -0.107641
2021-02-22 00:00:00+00:00  0.004127  0.117085
2021-02-23 00:00:00+00:00 -0.004366 -0.126528
2021-02-24 00:00:00+00:00  0.004297  0.122982

None
np.inf
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     size=1,
...     size_type="value"
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000019  0.000516
2021-02-19 00:00:00+00:00  0.000019  0.000516
2021-02-20 00:00:00+00:00  0.000019  0.000516
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.000000  0.000000
2021-02-24 00:00:00+00:00  0.000000  0.000000

1 / 51552.60 = 0.000019
BTCUSDT
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=      pd.Series([X,   O, O, O, O,    O, O]),
...     exits=        pd.Series([O,   O, O, X, O,    O, O]),
...     short_entries=pd.Series([O,   O, O, O, X,    O, O]),
...     short_exits=  pd.Series([O,   O, O, O, O,    O, X]),
...     size=         pd.Series([0.5, 0, 0, 0, 0.25, 0, 0]),
...     size_type="valuepercent",  Treat the size as a percentage of the total portfolio value
... )
>>> pf.asset_value / pf.value  Get the allocation at each bar, that is, the ratio of the asset value in respect to the total portfolio value
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.500000  0.500000  << long entry of 50%
2021-02-19 00:00:00+00:00  0.520256  0.501976
2021-02-20 00:00:00+00:00  0.519967  0.496546
2021-02-21 00:00:00+00:00  0.000000  0.000000  << long exit
2021-02-22 00:00:00+00:00 -0.250000 -0.250000  << short entry of 25%
2021-02-23 00:00:00+00:00 -0.220680 -0.215853
2021-02-24 00:00:00+00:00  0.000000  0.000000  << short exit

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     size=     vbt.Param([np.inf,   1,       0.5], level=0),  Without the same level, both parameters would build a Cartesian product
...     size_type=vbt.Param(["amount", "value", "valuepercent"], level=0)
... )
>>> pf.total_return
size  size_type     symbol 
inf   amount        BTCUSDT    0.113592
                    ETHUSDT   -0.003135
1.0   value         BTCUSDT    0.001136
                    ETHUSDT   -0.000031
0.5   valuepercent  BTCUSDT    0.056796
                    ETHUSDT   -0.001567
Name: total_return, dtype: float64

level
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     size=1,
...     size_type="value",
...     accumulate=True  See AccumulationMode
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000019  0.000516
2021-02-19 00:00:00+00:00  0.000019  0.000516
2021-02-20 00:00:00+00:00  0.000019  0.000516
2021-02-21 00:00:00+00:00  0.000002  0.000000
2021-02-22 00:00:00+00:00  0.000002  0.000000
2021-02-23 00:00:00+00:00  0.000002  0.000000
2021-02-24 00:00:00+00:00  0.000002  0.000000

BTCUSDT
ETHUSDT
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=True,
...     size=1,
...     size_type="value",
...     accumulate=vbt.Param([False, True])
... )
>>> pf.asset_flow  Calculate and print the asset flow at each bar, that is, the absolute amount of units bought (positive) or sold (negative)
accumulate                    False                True          
symbol                      BTCUSDT   ETHUSDT   BTCUSDT   ETHUSDT
Open time                                                        
2021-02-18 00:00:00+00:00  0.000019  0.000516  0.000019  0.000516
2021-02-19 00:00:00+00:00  0.000000  0.000000  0.000018  0.000512
2021-02-20 00:00:00+00:00  0.000000  0.000000  0.000018  0.000523
2021-02-21 00:00:00+00:00  0.000000  0.000000  0.000017  0.000517
2021-02-22 00:00:00+00:00  0.000000  0.000000  0.000018  0.000563
2021-02-23 00:00:00+00:00  0.000000  0.000000  0.000020  0.000634
2021-02-24 00:00:00+00:00  0.000000  0.000000  0.000020  0.000616

size_type="value"
size_type=SizeType.Value
Amount
val_price
TargetAmount
TargetValue
TargetPercent(100)
TargetAmount
Percent
size_granularity
1
0.001
size_granularity
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     size_granularity=vbt.Param([1, 0.001]),
...     init_cash=1000_000
... )
>>> pf.asset_flow
size_granularity            1.000           0.001         
symbol                    BTCUSDT ETHUSDT BTCUSDT  ETHUSDT
Open time                                                 
2021-02-18 00:00:00+00:00    19.0   515.0  19.397  515.567
2021-02-19 00:00:00+00:00     0.0     0.0   0.000    0.000
2021-02-20 00:00:00+00:00     0.0     0.0   0.000    0.000
2021-02-21 00:00:00+00:00   -19.0  -515.0 -19.397 -515.566
2021-02-22 00:00:00+00:00     0.0     0.0   0.000    0.000
2021-02-23 00:00:00+00:00     0.0     0.0   0.000    0.000
2021-02-24 00:00:00+00:00     0.0     0.0   0.000    0.000

price
price
"close"
PriceType.Close
np.inf
NextOpen
NextClose
"open"
>>> price = sub_data.symbol_wrapper.fill()  Create a new price array of the same shape as the data
>>> entries = pd.Series([X, O, O, O, O, O, O]).vbt.broadcast_to(price)  Define signals and broadcast them to the price array to be able to use them as a mask
>>> exits =   pd.Series([O, O, O, X, O, O, O]).vbt.broadcast_to(price)
>>> price[entries] = sub_data.open  Set any element in price that falls under an entry signal to the opening price
>>> price[exits] = sub_data.close

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,
...     exits=exits,
...     price=price
... )

>>> pf.orders.price.to_pd() == sub_data.open  Generate a mask of filled orders which price is open (such a comparison works only if slippage is zero!)
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     True     True
2021-02-19 00:00:00+00:00    False    False
2021-02-20 00:00:00+00:00    False    False
2021-02-21 00:00:00+00:00    False    False
2021-02-22 00:00:00+00:00    False    False
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False

>>> pf.orders.price.to_pd() == sub_data.close  Generate a mask of filled orders which price is close
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    False    False
2021-02-19 00:00:00+00:00    False    False
2021-02-20 00:00:00+00:00    False    False
2021-02-21 00:00:00+00:00     True     True
2021-02-22 00:00:00+00:00    False    False
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     price="nextopen"
... )
>>> pf.orders.price.to_pd() == sub_data.open
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    False    False
2021-02-19 00:00:00+00:00     True     True
2021-02-20 00:00:00+00:00    False    False
2021-02-21 00:00:00+00:00    False    False
2021-02-22 00:00:00+00:00     True     True
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]).vbt.signals.fshift(),
...     exits=  pd.Series([O, O, O, X, O, O, O]).vbt.signals.fshift(),
...     price="open"  When shifting manually, use open/close of the current bar
... )
>>> pf.orders.price.to_pd() == sub_data.open
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    False    False
2021-02-19 00:00:00+00:00     True     True
2021-02-20 00:00:00+00:00    False    False
2021-02-21 00:00:00+00:00    False    False
2021-02-22 00:00:00+00:00     True     True
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False

open
high
low
close
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=  pd.Series([X, O, O, O, X, O, O]).vbt.fshift(1, False),
...     exits=    pd.Series([O, O, O, X, O, O, X]).vbt.fshift(1, False),
...     direction=pd.Series([L, L, L, L, S, S, S]).vbt.fshift(1, -1)
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.001789  0.051151
2021-02-20 00:00:00+00:00  0.001789  0.051151
2021-02-21 00:00:00+00:00  0.001789  0.051151
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00 -0.001979 -0.057624
2021-02-24 00:00:00+00:00 -0.001979 -0.057624

from_ago
from_ago=1
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=  pd.Series([X, O, O, O, X, O, O]),
...     exits=    pd.Series([O, O, O, X, O, O, X]),
...     direction=pd.Series([L, L, L, L, S, S, S]),
...     from_ago=1
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.001789  0.051151
2021-02-20 00:00:00+00:00  0.001789  0.051151
2021-02-21 00:00:00+00:00  0.001789  0.051151
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00 -0.001979 -0.057624
2021-02-24 00:00:00+00:00 -0.001979 -0.057624

order_type
price
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     price="nextopen",
...     slippage=0.005  0.01 is 1%
... )
>>> pf.orders.price.to_pd()  Get the filled order price
symbol                         BTCUSDT     ETHUSDT
Open time                                         
2021-02-18 00:00:00+00:00          NaN         NaN
2021-02-19 00:00:00+00:00  51810.37305  1948.60455
2021-02-20 00:00:00+00:00          NaN         NaN
2021-02-21 00:00:00+00:00          NaN         NaN
2021-02-22 00:00:00+00:00  57125.28825  1923.87230
2021-02-23 00:00:00+00:00          NaN         NaN
2021-02-24 00:00:00+00:00          NaN         NaN

>>> pf.orders.price.to_pd() / sub_data.open  Get the filled order price in relation to the opening price
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00    1.005    1.005
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00    0.995    0.995
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

order_type
"limit"
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     order_type="limit"
... )
>>> pf.orders.price.to_pd()
symbol                      BTCUSDT  ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00       NaN      NaN
2021-02-19 00:00:00+00:00  51552.60  1938.91
2021-02-20 00:00:00+00:00       NaN      NaN
2021-02-21 00:00:00+00:00       NaN      NaN
2021-02-22 00:00:00+00:00  57412.35  1933.54
2021-02-23 00:00:00+00:00       NaN      NaN
2021-02-24 00:00:00+00:00       NaN      NaN

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, O, O, X, O, O]),
...     direction="shortonly",
...     price=sub_data.high.vbt.fshift(),  Shift the high price by one bar to use the previous high price
...     order_type="limit"
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

limit_tif
np.timedelta64
pd.Timedelta
datetime.timedelta
np.timedelta64
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit"
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00  50710.2      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

BTCUSDT
ETHUSDT
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_tif="2d"  Gets translated into a time delta using freq_to_timedelta64
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

BTCUSDT
2021-02-20 00:00:00
2021-02-22 00:00:00
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_tif=vbt.Param([
...         -1,  Don't use TIF
...         pd.Timedelta(days=1),  TIF of 1 day in Pandas format
...         2 * np.timedelta64(86400000000000),  TIF of 2 days in NumPy format
...         3 * vbt.dt_nb.d_ns  TIF of 3 days in integer format (nanoseconds)
...     ], keys=["none", "1 days", "2 days", "3 days"])
... )
>>> pf.orders.price.to_pd()
limit_tif                     none          1 days          2 days          \\
symbol                     BTCUSDT ETHUSDT BTCUSDT ETHUSDT BTCUSDT ETHUSDT   
Open time                                                                    
2021-02-18 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   
2021-02-19 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   
2021-02-20 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   
2021-02-21 00:00:00+00:00      NaN  1891.0     NaN     NaN     NaN  1891.0   
2021-02-22 00:00:00+00:00  50710.2     NaN     NaN     NaN     NaN     NaN   
2021-02-23 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   
2021-02-24 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   

limit_tif                   3 days          
symbol                     BTCUSDT ETHUSDT  
Open time                                   
2021-02-18 00:00:00+00:00      NaN     NaN  
2021-02-19 00:00:00+00:00      NaN     NaN  
2021-02-20 00:00:00+00:00      NaN     NaN  
2021-02-21 00:00:00+00:00      NaN  1891.0  
2021-02-22 00:00:00+00:00  50710.2     NaN  
2021-02-23 00:00:00+00:00      NaN     NaN  
2021-02-24 00:00:00+00:00      NaN     NaN  

time_delta_format
Rows
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_tif=vbt.Param(
...         [-1, 1, 2, 3], 
...         keys=["none", "1 rows", "2 rows", "3 rows"]
...     ),
...     time_delta_format="rows"
... )

>>> index_td = vbt.dt.freq_to_timedelta("1 minute")
>>> tif_td = vbt.dt.freq_to_timedelta("1 day")
>>> int(tif_td / index_td)
1440

limit_expiry
pd.Timestamp
datetime.datetime
np.datetime64
>>> sub_data.symbol_wrapper.get_period_ns_index("1d")
array([1613692800000000000, 1613779200000000000, 1613865600000000000,
       1613952000000000000, 1614038400000000000, 1614124800000000000,
       1614211200000000000])

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_expiry="W-MON"  See Anchored offsets
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00  50710.2      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

>>> sub_data.symbol_wrapper.index.to_period("W-MON")  User warning "Converting to PeriodArray/Index representation will drop timezone information" can be ignored
PeriodIndex(['2021-02-16/2021-02-22', '2021-02-16/2021-02-22',
             '2021-02-16/2021-02-22', '2021-02-16/2021-02-22',
             '2021-02-16/2021-02-22', '2021-02-23/2021-03-01',
             '2021-02-23/2021-03-01'],
            dtype='period[W-MON]', name='Open time')

2021-02-20
2021-02-21
2021-02-22
2021-02-16/2021-02-22
BTCUSDT
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_expiry="W-SUN"
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

>>> sub_data.symbol_wrapper.index.to_period("W-SUN")
PeriodIndex(['2021-02-15/2021-02-21', '2021-02-15/2021-02-21',
             '2021-02-15/2021-02-21', '2021-02-15/2021-02-21',
             '2021-02-22/2021-02-28', '2021-02-22/2021-02-28',
             '2021-02-22/2021-02-28'],
            dtype='period[W-SUN]', name='Open time')

limit_expiry
limit_tif="2d"
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_expiry=vbt.RepEval("""
...     expiry_index = wrapper.index + pd.Timedelta(days=2)
...     expiry_arr = vbt.to_2d_array(vbt.dt.to_ns(expiry_index))
...     expiry_arr
...     """)  Since, after broadcasting, the final index might be different from the original one, we need to create a template that adds 2 days to the final index and converts it into a two-dimensional array. Leaving it as one-dimensional will broadcast values per column (!)
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

pd.Index
pd.Series
upon_adj_limit_conflict
upon_opp_limit_conflict
>>> all_conflict_modes = vbt.pf_enums.PendingConflictMode._fields  Get the list with all options
>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=      pd.Series([O, O, X, O, O, O, O]),
...     short_entries=pd.Series([O, O, O, X, O, O, O]),
...     price=sub_data.select("BTCUSDT").low.vbt.fshift(),
...     order_type=vbt.index_dict({2: "limit", "_def": "market"}),  Only the first order should be a limit order
...     upon_opp_limit_conflict=vbt.Param(all_conflict_modes)
... )
>>> pf.orders.price.to_pd()
upon_opp_limit_conflict    KeepIgnore  KeepExecute  CancelIgnore  \\
Open time                                                          
2021-02-18 00:00:00+00:00         NaN          NaN           NaN   
2021-02-19 00:00:00+00:00         NaN          NaN           NaN   
2021-02-20 00:00:00+00:00         NaN          NaN           NaN   
2021-02-21 00:00:00+00:00         NaN     53863.93           NaN   
2021-02-22 00:00:00+00:00     50710.2     50710.20           NaN   
2021-02-23 00:00:00+00:00         NaN          NaN           NaN   
2021-02-24 00:00:00+00:00         NaN          NaN           NaN   

upon_opp_limit_conflict    CancelExecute  
Open time                                 
2021-02-18 00:00:00+00:00            NaN  
2021-02-19 00:00:00+00:00            NaN  
2021-02-20 00:00:00+00:00            NaN  
2021-02-21 00:00:00+00:00       53863.93  
2021-02-22 00:00:00+00:00            NaN  
2021-02-23 00:00:00+00:00            NaN  
2021-02-24 00:00:00+00:00            NaN  

KeepIgnore
CancelExecute
price
limit_delta
price
delta_format
Percent
limit_delta=0.1
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     order_type="limit",
...     limit_delta=vbt.Param([0, 0.1, 0.5])
... )
>>> pf.orders.price.to_pd()
limit_delta                    0.0                0.1               0.5  \\
symbol                     BTCUSDT  ETHUSDT   BTCUSDT   ETHUSDT BTCUSDT   
Open time                                                                 
2021-02-18 00:00:00+00:00      NaN      NaN       NaN       NaN     NaN   
2021-02-19 00:00:00+00:00  51552.6  1938.91       NaN       NaN     NaN   
2021-02-20 00:00:00+00:00      NaN      NaN       NaN       NaN     NaN   
2021-02-21 00:00:00+00:00      NaN      NaN       NaN       NaN     NaN   
2021-02-22 00:00:00+00:00      NaN      NaN       NaN  1745.649     NaN   
2021-02-23 00:00:00+00:00      NaN      NaN  46397.34       NaN     NaN   
2021-02-24 00:00:00+00:00      NaN      NaN       NaN       NaN     NaN   

limit_delta                        
symbol                    ETHUSDT  
Open time                          
2021-02-18 00:00:00+00:00     NaN  
2021-02-19 00:00:00+00:00     NaN  
2021-02-20 00:00:00+00:00     NaN  
2021-02-21 00:00:00+00:00     NaN  
2021-02-22 00:00:00+00:00     NaN  
2021-02-23 00:00:00+00:00     NaN  
2021-02-24 00:00:00+00:00     NaN  

close * (1 - limit_delta)
close * (1 + limit_delta)
limit_reverse
>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     order_type="limit",
...     limit_delta=vbt.Param([0, 100, 5000, 10000]),
...     delta_format="absolute",
...     limit_reverse=True
... )
>>> pf.orders.price.to_pd()
limit_delta                   0        100      5000   10000
Open time                                                   
2021-02-18 00:00:00+00:00       NaN      NaN      NaN    NaN
2021-02-19 00:00:00+00:00  51552.61  51652.6      NaN    NaN
2021-02-20 00:00:00+00:00       NaN      NaN  56552.6    NaN
2021-02-21 00:00:00+00:00       NaN      NaN      NaN    NaN
2021-02-22 00:00:00+00:00       NaN      NaN      NaN    NaN
2021-02-23 00:00:00+00:00       NaN      NaN      NaN    NaN
2021-02-24 00:00:00+00:00       NaN      NaN      NaN    NaN

signal_func_nb
adjust_func_nb
adjust_func_nb
signal_func_nb
c
adjust_args
None
>>> @njit
... def adjust_func_nb(c):  Must take SignalContext 
...     limit_info = c.last_limit_info[c.col]  Get the pending limit order information for the current column (asset)
...     if limit_info.creation_idx != -1:  Check whether there is any limit order pending
...         if c.i - limit_info.creation_idx >= 1:  Check whether the order has been pending for one bar or longer
...             vbt.pf_nb.clear_limit_info_nb(limit_info)  Use clear_limit_info_nb to cancel the order

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="open",
...     order_type="limit",
...     limit_delta=vbt.Param([0, 0.1]),
...     adjust_func_nb=adjust_func_nb
... )
>>> pf.orders.price.to_pd()
limit_delta                     0.0             0.1        
symbol                      BTCUSDT ETHUSDT BTCUSDT ETHUSDT
Open time                                                  
2021-02-18 00:00:00+00:00  52117.67  1849.7     NaN     NaN
2021-02-19 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-20 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-21 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-22 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-23 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-24 00:00:00+00:00       NaN     NaN     NaN     NaN

>>> @njit
... def adjust_func_nb(c):
...     limit_info = c.last_limit_info[c.col]
...     if limit_info.creation_idx != -1:
...         if c.i - limit_info.creation_idx >= 1:
...             limit_info.init_price = -np.inf  Negative infinity will be translated into the valuation price, that is, the latest known price at the time of calling a callback, which is the opening price for adjustment and signal callbacks, and the closing price for post-segment callbacks. Note that you cannot use positive infinity for the closing price because it's unknown!
...             limit_info.delta = 0.01

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="open",
...     order_type="limit",
...     limit_delta=vbt.Param([0.1, 0.2]),
...     adjust_func_nb=adjust_func_nb
... )
>>> pf.orders.price.to_pd()
limit_delta                       0.1                    0.2           
symbol                        BTCUSDT    ETHUSDT     BTCUSDT    ETHUSDT
Open time                                                              
2021-02-18 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-19 00:00:00+00:00  51037.0839  1919.5209  51037.0839  1919.5209
2021-02-20 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-21 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-22 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-23 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-24 00:00:00+00:00         NaN        NaN         NaN        NaN

>>> @njit
... def adjust_func_nb(c, custom_delta):
...     limit_info = c.last_limit_info[c.col]
...     if c.i == 0:  Create a signal order at the very first bar only
...         curr_delta = vbt.pf_nb.select_nb(c, custom_delta)
...         vbt.pf_nb.set_limit_info_nb(limit_info, c.i, delta=curr_delta)  Use set_limit_info_nb. Take a look at its signature to learn more about defaults. For example, it uses the current valuation price, the infinity size, and both directions by default.

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     broadcast_named_args=dict(custom_delta=vbt.Param([0, 0.1])),  To make a custom argument parameterizable, we need to define it as a flexible argument in broadcast_named_args, that is, it must be able to be defined per element
...     adjust_func_nb=adjust_func_nb,
...     adjust_args=(vbt.Rep("custom_delta"),)  Don't forget to put a comma after a single element in a tuple
... )
>>> pf.orders.price.to_pd()
custom_delta                    0.0                0.1         
symbol                      BTCUSDT ETHUSDT    BTCUSDT  ETHUSDT
Open time                                                      
2021-02-18 00:00:00+00:00  52117.67  1849.7        NaN      NaN
2021-02-19 00:00:00+00:00       NaN     NaN        NaN      NaN
2021-02-20 00:00:00+00:00       NaN     NaN        NaN      NaN
2021-02-21 00:00:00+00:00       NaN     NaN        NaN      NaN
2021-02-22 00:00:00+00:00       NaN     NaN        NaN  1664.73
2021-02-23 00:00:00+00:00       NaN     NaN  46905.903      NaN
2021-02-24 00:00:00+00:00       NaN     NaN        NaN      NaN

broadcast_named_args
sl_stop
tsl_stop
tsl_th
tp_stop
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=0.1,
... )
>>> pf.orders.price.to_pd()
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  51552.60  1939.610
2021-02-19 00:00:00+00:00       NaN       NaN
2021-02-20 00:00:00+00:00       NaN       NaN
2021-02-21 00:00:00+00:00       NaN       NaN
2021-02-22 00:00:00+00:00       NaN  1745.649
2021-02-23 00:00:00+00:00  46397.34       NaN
2021-02-24 00:00:00+00:00       NaN       NaN

sub_data
open
high
low
close
51552.60 * 0.9 = 46397.34
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=vbt.Param([np.nan, 0.1, 0.2]),  NaN means no stop order
... )
>>> pf.orders.price.to_pd()
sl_stop                        NaN                0.1                0.2  \\
symbol                     BTCUSDT  ETHUSDT   BTCUSDT   ETHUSDT  BTCUSDT   
Open time                                                                  
2021-02-18 00:00:00+00:00  51552.6  1939.61  51552.60  1939.610  51552.6   
2021-02-19 00:00:00+00:00      NaN      NaN       NaN       NaN      NaN   
2021-02-20 00:00:00+00:00      NaN      NaN       NaN       NaN      NaN   
2021-02-21 00:00:00+00:00      NaN      NaN       NaN       NaN      NaN   
2021-02-22 00:00:00+00:00      NaN      NaN       NaN  1745.649      NaN   
2021-02-23 00:00:00+00:00      NaN      NaN  46397.34       NaN      NaN   
2021-02-24 00:00:00+00:00      NaN      NaN       NaN       NaN      NaN   

sl_stop                              
symbol                      ETHUSDT  
Open time                            
2021-02-18 00:00:00+00:00  1939.610  
2021-02-19 00:00:00+00:00       NaN  
2021-02-20 00:00:00+00:00       NaN  
2021-02-21 00:00:00+00:00       NaN  
2021-02-22 00:00:00+00:00  1551.688  
2021-02-23 00:00:00+00:00       NaN  
2021-02-24 00:00:00+00:00       NaN  

>>> atr = data.run("atr").atr
>>> sub_atr = atr.loc["2021-02-18":"2021-02-24"]

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=sub_atr / sub_data.close
... )
>>> pf.orders.price.to_pd()
symbol                          BTCUSDT      ETHUSDT
Open time                                           
2021-02-18 00:00:00+00:00  51552.600000  1939.610000
2021-02-19 00:00:00+00:00           NaN          NaN
2021-02-20 00:00:00+00:00           NaN  1805.283249
2021-02-21 00:00:00+00:00           NaN          NaN
2021-02-22 00:00:00+00:00  48335.716826          NaN
2021-02-23 00:00:00+00:00           NaN          NaN
2021-02-24 00:00:00+00:00           NaN          NaN

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=100,
...     delta_format="absolute"
... )
>>> pf.orders.price.to_pd()
Open time
2021-02-18 00:00:00+00:00    51552.6
2021-02-19 00:00:00+00:00    51452.6
2021-02-20 00:00:00+00:00        NaN
2021-02-21 00:00:00+00:00        NaN
2021-02-22 00:00:00+00:00        NaN
2021-02-23 00:00:00+00:00        NaN
2021-02-24 00:00:00+00:00        NaN
Freq: D, dtype: float64

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=51452.6,
...     delta_format="target"
... )
>>> pf.orders.price.to_pd()
Open time
2021-02-18 00:00:00+00:00    51552.6
2021-02-19 00:00:00+00:00    51452.6
2021-02-20 00:00:00+00:00        NaN
2021-02-21 00:00:00+00:00        NaN
2021-02-22 00:00:00+00:00        NaN
2021-02-23 00:00:00+00:00        NaN
2021-02-24 00:00:00+00:00        NaN
Freq: D, dtype: float64

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=1 * sub_atr / sub_data.close,
...     tp_stop=2 * sub_atr / sub_data.close
... )
>>> pf.orders.price.to_pd()
symbol                          BTCUSDT      ETHUSDT
Open time                                           
2021-02-18 00:00:00+00:00  51552.600000  1939.610000
2021-02-19 00:00:00+00:00           NaN          NaN
2021-02-20 00:00:00+00:00           NaN  1805.283249
2021-02-21 00:00:00+00:00  57986.366348          NaN
2021-02-22 00:00:00+00:00           NaN          NaN
2021-02-23 00:00:00+00:00           NaN          NaN
2021-02-24 00:00:00+00:00           NaN          NaN

>>> pf.orders.records_readable  Orders are represented as rows in a compressed record array. Specific information as a column.
   Order Id   Column              Signal Index            Creation Index  \\
0         0  BTCUSDT 2021-02-18 00:00:00+00:00 2021-02-18 00:00:00+00:00   
1         1  BTCUSDT 2021-02-18 00:00:00+00:00 2021-02-21 00:00:00+00:00   
2         0  ETHUSDT 2021-02-18 00:00:00+00:00 2021-02-18 00:00:00+00:00   
3         1  ETHUSDT 2021-02-18 00:00:00+00:00 2021-02-20 00:00:00+00:00   

                 Fill Index      Size         Price  Fees  Side    Type  \\
0 2021-02-18 00:00:00+00:00  0.001940  51552.600000   0.0   Buy  Market   
1 2021-02-21 00:00:00+00:00  0.001940  57986.366348   0.0  Sell  Market   
2 2021-02-18 00:00:00+00:00  0.051557   1939.610000   0.0   Buy  Market   
3 2021-02-20 00:00:00+00:00  0.051557   1805.283249   0.0  Sell  Market   

  Stop Type  
0      None  
1        TP  
2      None  
3        SL  

>>> pf.orders.stop_type.to_pd(mapping=True)  Specific information as a time series
symbol                    BTCUSDT ETHUSDT
Open time                                
2021-02-18 00:00:00+00:00    None    None
2021-02-19 00:00:00+00:00    None    None
2021-02-20 00:00:00+00:00    None      SL
2021-02-21 00:00:00+00:00      TP    None
2021-02-22 00:00:00+00:00    None    None
2021-02-23 00:00:00+00:00    None    None
2021-02-24 00:00:00+00:00    None    None

Side
Type
Fill Index
Stop Type
Signal Index
Fill Index
Creation Index
Fill Index
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop= vbt.Param([0.1,    np.nan, np.nan, np.nan], level=0),
...     tsl_stop=vbt.Param([np.nan, 0.1,    0.1,    np.nan], level=0),
...     tsl_th=  vbt.Param([np.nan, np.nan, 0.1,    np.nan], level=0),
...     tp_stop= vbt.Param([np.nan, np.nan, np.nan, 0.1   ], level=0),
... )
>>> pf.total_return
sl_stop  tsl_stop  tsl_th  tp_stop  symbol 
0.1      NaN       NaN     NaN      BTCUSDT   -0.100000
                                    ETHUSDT   -0.100000
NaN      0.1       NaN     NaN      BTCUSDT    0.018717
                                    ETHUSDT   -0.052332
NaN      0.1       0.1     NaN      BTCUSDT    0.018717
                                    ETHUSDT   -0.163033
NaN      NaN       NaN     0.1      BTCUSDT    0.100000
                                    ETHUSDT   -0.163033
Name: total_return, dtype: float64

level
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=vbt.Param([np.nan, 0.1]),
...     tsl_stop=vbt.Param([np.nan, 0.1]),
...     tsl_th=vbt.Param([np.nan, 0.1]),
...     tp_stop=vbt.Param([np.nan, 0.1]),
... )

tsl_th
tsl_stop
level
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=vbt.Param([np.nan, 0.1], level=0),
...     tsl_stop=vbt.Param([np.nan, 0.1, 0.1], level=1),
...     tsl_th=vbt.Param([np.nan, np.nan, 0.1], level=1),
...     tp_stop=vbt.Param([np.nan, 0.1], level=2),
... )

>>> total_return = pf.total_return
>>> sl_stops = total_return.index.get_level_values("sl_stop")
>>> tp_stops = total_return.index.get_level_values("tp_stop")

>>> total_return[np.isnan(sl_stops) | np.isnan(tp_stops)].mean()
-0.045462468872515135

>>> total_return[~np.isnan(sl_stops) & ~np.isnan(tp_stops)].mean()
0.0

>>> StopOrderPrep = vbt.IF.from_expr("""
...     sl_stop = @p_sl_mult * @in_atr / @in_close
...     tp_stop = @p_tp_mult * @in_atr / @in_close
...     sl_stop, tp_stop
... """)
>>> stop_order_prep = StopOrderPrep.run(
...     close=sub_data.close, 
...     atr=sub_atr,
...     sl_mult=[np.nan, 1],  
...     tp_mult=[np.nan, 1],
...     param_product=True
... )
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=stop_order_prep.sl_stop,
...     tp_stop=stop_order_prep.tp_stop,
... )
>>> pf.total_return
custom_sl_mult  custom_tp_mult  symbol 
NaN             NaN             BTCUSDT   -0.036398
                                ETHUSDT   -0.163033
NaN             1.0             BTCUSDT    0.062400
                                ETHUSDT   -0.163033
1.0             NaN             BTCUSDT   -0.062400
                                ETHUSDT   -0.069255
1.0             1.0             BTCUSDT    0.062400
                                ETHUSDT   -0.069255
Name: total_return, dtype: float64

stop_entry_price
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="open",
...     tp_stop=0.005,
...     stop_entry_price="fillprice"
... )
>>> pf.orders.price.to_pd()
symbol                         BTCUSDT  ETHUSDT
Open time                                      
2021-02-18 00:00:00+00:00  52117.67000  1849.70
2021-02-19 00:00:00+00:00  52378.25835  1938.91
2021-02-20 00:00:00+00:00          NaN      NaN
2021-02-21 00:00:00+00:00          NaN      NaN
2021-02-22 00:00:00+00:00          NaN      NaN
2021-02-23 00:00:00+00:00          NaN      NaN
2021-02-24 00:00:00+00:00          NaN      NaN

52378.26
52530.00
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     tp_stop=0.05,
...     stop_exit_price=vbt.Param(["stop", "close"])
... )
>>> pf.orders.price.to_pd()
stop_exit_price                stop               close         
symbol                      BTCUSDT    ETHUSDT  BTCUSDT  ETHUSDT
Open time                                                       
2021-02-18 00:00:00+00:00  51552.60  1939.6100  51552.6  1939.61
2021-02-19 00:00:00+00:00  54130.23        NaN  55906.0      NaN
2021-02-20 00:00:00+00:00       NaN  2036.5905      NaN  1913.00
2021-02-21 00:00:00+00:00       NaN        NaN      NaN      NaN
2021-02-22 00:00:00+00:00       NaN        NaN      NaN      NaN
2021-02-23 00:00:00+00:00       NaN        NaN      NaN      NaN
2021-02-24 00:00:00+00:00       NaN        NaN      NaN      NaN

stop_exit_type
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     tp_stop=0.01,  
...     stop_exit_type=vbt.Param(["close", "reverse"])
... )
>>> pf.orders.price.to_pd()
stop_exit_type                 close                reverse           
symbol                       BTCUSDT    ETHUSDT     BTCUSDT    ETHUSDT
Open time                                                             
2021-02-18 00:00:00+00:00  51552.600  1939.6100  51552.6000  1939.6100
2021-02-19 00:00:00+00:00  52068.126  1959.0061  52068.1260  1959.0061
2021-02-20 00:00:00+00:00        NaN        NaN  55346.9400  1935.4500
2021-02-21 00:00:00+00:00        NaN        NaN  56399.6019  1932.1300
2021-02-22 00:00:00+00:00        NaN        NaN  56834.4843  1914.1947
2021-02-23 00:00:00+00:00        NaN        NaN         NaN        NaN
2021-02-24 00:00:00+00:00        NaN        NaN         NaN        NaN

reverse
stop_exit_type
stop_exit_price
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     tp_stop=0.01,
...     stop_exit_type=vbt.index_dict({0: "reverse", "_def": "close"})
... )
>>> pf.orders.price.to_pd()
symbol                       BTCUSDT    ETHUSDT
Open time                                      
2021-02-18 00:00:00+00:00  51552.600  1939.6100
2021-02-19 00:00:00+00:00  52068.126  1959.0061
2021-02-20 00:00:00+00:00  55346.940  1935.4500
2021-02-21 00:00:00+00:00        NaN        NaN
2021-02-22 00:00:00+00:00        NaN        NaN
2021-02-23 00:00:00+00:00        NaN        NaN
2021-02-24 00:00:00+00:00        NaN        NaN

stop_order_type
stop_limit_delta
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     tp_stop=0.05,
...     stop_order_type=vbt.Param(["market", "limit"]),
...     stop_limit_delta=0.01
... )
>>> pf.orders.price.to_pd()
stop_order_type              market                  limit         
symbol                      BTCUSDT    ETHUSDT     BTCUSDT  ETHUSDT
Open time                                                          
2021-02-18 00:00:00+00:00  51552.60  1939.6100  51552.6000  1939.61
2021-02-19 00:00:00+00:00  54130.23        NaN  54671.5323      NaN
2021-02-20 00:00:00+00:00       NaN  2036.5905         NaN      NaN
2021-02-21 00:00:00+00:00       NaN        NaN         NaN      NaN
2021-02-22 00:00:00+00:00       NaN        NaN         NaN      NaN
2021-02-23 00:00:00+00:00       NaN        NaN         NaN      NaN
2021-02-24 00:00:00+00:00       NaN        NaN         NaN      NaN

ETHUSDT
2042.34
1939.61 * 1.05 * 1.01 = 2056.96
upon_adj_stop_conflict
upon_opp_stop_conflict
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, X, O, X, O, X]),
...     exits=  pd.Series([O, X, O, X, O, X, O]),
...     direction="both",
...     sl_stop=0.1,
...     upon_adj_stop_conflict="keepignore",
...     upon_opp_stop_conflict="keepignore",
... )
>>> pf.orders.price.to_pd()
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  51552.60  1939.610
2021-02-19 00:00:00+00:00       NaN       NaN
2021-02-20 00:00:00+00:00       NaN       NaN
2021-02-21 00:00:00+00:00       NaN       NaN
2021-02-22 00:00:00+00:00       NaN  1745.649
2021-02-23 00:00:00+00:00  46397.34  1577.890
2021-02-24 00:00:00+00:00  49676.20       NaN

>>> @njit
... def adjust_func_nb(c, max_loss, max_profit):
...     position_now = c.last_position[c.col]  
...     if position_now != 0:  
...         sl_info = c.last_sl_info[c.col]
...         tp_info = c.last_tp_info[c.col]
...         ml = vbt.pf_nb.select_nb(c, max_loss)  
...         mp = vbt.pf_nb.select_nb(c, max_profit)
...
...         if not vbt.pf_nb.is_stop_info_active_nb(sl_info):  
...             sl_info.stop = ml / (sl_info.init_price * position_now)  
...         if not vbt.pf_nb.is_stop_info_active_nb(tp_info):
...             tp_info.stop = mp / (sl_info.init_price * position_now)

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     adjust_func_nb=adjust_func_nb,
...     adjust_args=(vbt.Rep("max_loss"), vbt.Rep("max_profit")),
...     broadcast_named_args=dict(
...         max_loss=10,
...         max_profit=10
...     )
... )
>>> pf.exit_trades.pnl.to_pd()  
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00     10.0      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN    -10.0
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

tp_stop
stop_ladder
stop_ladder
stop_ladder
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder=True,
...     tp_stop=[0.01, 0.05]
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00 -0.00097 -0.025778
2021-02-20 00:00:00+00:00 -0.00097 -0.025778
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000

stop_ladder
StopLadderMode.Weighted
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder="weighted",
...     tp_stop=[0.01, 0.05]
... )
>>> pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00 -0.000388 -0.010311
2021-02-20 00:00:00+00:00 -0.001552 -0.041245
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.000000  0.000000
2021-02-24 00:00:00+00:00  0.000000  0.000000

(0.01 - 0) / (0.05 - 0) = 20%
(0.05 - 0.01) / (0.05 - 0) = 80%
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder="uniform",
...     tp_stop=[0.1, 0.2],
...     sl_stop=[0.05, 0.1, 0.15]
... )

>>> pf.orders.stop_type.to_pd(mapping=True)
symbol                    BTCUSDT ETHUSDT
Open time                                
2021-02-18 00:00:00+00:00    None    None
2021-02-19 00:00:00+00:00    None    None
2021-02-20 00:00:00+00:00      TP      SL
2021-02-21 00:00:00+00:00    None    None
2021-02-22 00:00:00+00:00      SL      SL
2021-02-23 00:00:00+00:00      SL      SL
2021-02-24 00:00:00+00:00    None    None

>>> pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.000000  0.000000
2021-02-20 00:00:00+00:00 -0.000970 -0.017186
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00 -0.000647 -0.017186
2021-02-23 00:00:00+00:00 -0.000323 -0.017186
2021-02-24 00:00:00+00:00  0.000000  0.000000

BTCUSDT
StopLadderMode.AdaptUniform
StopLadderMode.AdaptWeighted
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder="adaptuniform",
...     tp_stop=[0.1, 0.2],
...     sl_stop=[0.05, 0.1, 0.15]
... )
>>> pf.orders.stop_type.to_pd(mapping=True)
symbol                    BTCUSDT ETHUSDT
Open time                                
2021-02-18 00:00:00+00:00    None    None
2021-02-19 00:00:00+00:00    None    None
2021-02-20 00:00:00+00:00      TP      SL
2021-02-21 00:00:00+00:00    None    None
2021-02-22 00:00:00+00:00      SL      SL
2021-02-23 00:00:00+00:00      SL      SL
2021-02-24 00:00:00+00:00    None    None

>>> pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.000000  0.000000
2021-02-20 00:00:00+00:00 -0.000970 -0.017186
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00 -0.000323 -0.017186
2021-02-23 00:00:00+00:00 -0.000323 -0.017186
2021-02-24 00:00:00+00:00  0.000000  0.000000

1 / 3
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder=True,
...     tp_stop=np.column_stack((
...         np.array([0.1, 0.2, np.nan]),  
...         np.array([0.01, 0.02, 0.03])  
...     ))
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00000 -0.017186
2021-02-20 00:00:00+00:00 -0.00097 -0.017186
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder=True,
...     tp_stop=vbt.Param([
...         np.array([0.1, 0.2]),  
...         np.array([0.01, 0.02, 0.03])  
...     ])
... )
>>> pf.asset_flow
tp_stop                    array_0             array_1          
symbol                     BTCUSDT   ETHUSDT   BTCUSDT   ETHUSDT
Open time                                                       
2021-02-18 00:00:00+00:00  0.00194  0.051557  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.00000  0.000000 -0.000647 -0.017186
2021-02-20 00:00:00+00:00 -0.00097  0.000000 -0.000647 -0.017186
2021-02-21 00:00:00+00:00  0.00000  0.000000 -0.000647  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000  0.000000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000  0.000000  0.000000

step
>>> @njit
... def adjust_func_nb(c, exit_size, exit_size_type):
...     tp_info = c.last_tp_info[c.col]
...     if vbt.pf_nb.is_stop_info_ladder_active_nb(tp_info):  
...         if np.isnan(tp_info.exit_size):  
...             tp_info.exit_size = vbt.pf_nb.select_nb(c, exit_size, i=tp_info.step)  
...             tp_info.exit_size_type = vbt.pf_nb.select_nb(c, exit_size_type, i=tp_info.step)

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     adjust_func_nb=adjust_func_nb,
...     adjust_args=(vbt.Rep("exit_size"), vbt.Rep("exit_size_type")),  
...     stop_ladder=True,
...     tp_stop=vbt.Param([
...         np.array([0.1, 0.2]),
...         np.array([0.01, 0.02, 0.03])
...     ], level=0),  
...     broadcast_named_args=dict(
...         exit_size=vbt.BCO(  
...             vbt.Param([
...                 np.array([3, 3]),
...                 np.array([1, 2, 3])
...             ], level=0, hide=True),  We should provide the exit size in an adentical fashion to tp_stop. Additionally, we'll hide the parameter from the final columns as it's redundant.
...             axis=1,
...             merge_kwargs=dict(  These keyword arguments are required if parameter arrays have different shapes. In this case, smaller arrays will be padded with NaN to match the length of the longest array.
...                 reset_index="from_start", 
...                 fill_value=np.nan,
...             )
...         ),
...         exit_size_type=vbt.BCO(
...             vbt.pf_enums.SizeType.Amount,  In our example, the size type is the same for all sizes, thus we can just use a single value here
...             axis=1,
...             merge_kwargs=dict(
...                 reset_index="from_start", 
...                 fill_value=-1,
...             )
...         )
...     ),
...     size=6,
...     init_cash="auto"
... )
>>> pf.asset_flow
tp_stop                   array_0         array_1        
symbol                    BTCUSDT ETHUSDT BTCUSDT ETHUSDT
Open time                                                
2021-02-18 00:00:00+00:00     6.0     6.0     6.0     6.0
2021-02-19 00:00:00+00:00     0.0     0.0    -1.0    -1.0
2021-02-20 00:00:00+00:00    -3.0     0.0    -2.0    -2.0
2021-02-21 00:00:00+00:00     0.0     0.0    -3.0     0.0
2021-02-22 00:00:00+00:00     0.0     0.0     0.0     0.0
2021-02-23 00:00:00+00:00     0.0     0.0     0.0     0.0
2021-02-24 00:00:00+00:00     0.0     0.0     0.0     0.0

tp_stop
size
signal_func_nb
c
vbt.Rep("size")
signal_args
size
arr[c.i, c.col] = ...
arr[0, c.col] = ...
>>> @njit
... def signal_func_nb(c, long_signals, short_signals, size):
...     long_signal = vbt.pf_nb.select_nb(c, long_signals)
...     short_signal = vbt.pf_nb.select_nb(c, short_signals)
...     if long_signal:
...         size[c.i, c.col] = 10  Set the value under the current row and column to be picked by vectorbt
...     if short_signal:
...         size[c.i, c.col] = 5
...     return long_signal, False, short_signal, False

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("long_signals"), 
...         vbt.Rep("short_signals"), 
...         vbt.Rep("size")  Forward size we built below to the signal function using templates
...     ),
...     size=vbt.RepEval("np.full(wrapper.shape, np.nan)"),  Using this template we can delay the creation of the array to the point where the target shape is known (i.e., after broadcasting all arrays), which gets available via wrapper.shape
...     size_type="value",
...     accumulate=True,
...     broadcast_named_args=dict(
...         long_signals= pd.Series([X, O, O, O, X, O, O]),
...         short_signals=pd.Series([O, O, X, O, O, O, X]),
...     )
... )
>>> pf.orders.value.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     10.0     10.0
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00     -5.0     -5.0
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00     10.0     10.0
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00     -5.0     -5.0

size
wrapper.shape
>>> memory = {}
>>> pf = vbt.Portfolio.from_signals(
...     # ...
...     size=vbt.RepEval("""
...         size = np.full(wrapper.shape, np.nan)
...         memory["size"] = size
...         return size
...         """, 
...         context=dict(memory=memory),  Everything that we pass as context will be available as a variable in the template
...         context_merge_kwargs=dict(nested=False)  We don't want to lose the reference to the dictionary memory, that's why we disable nested dictionary merging when the local context is being merged with the global context
...     ),
...     # ...
... )
>>> memory["size"]
[[10. 10.]
 [nan nan]
 [ 5.  5.]
 [nan nan]
 [10. 10.]
 [nan nan]
 [ 5.  5.]]

context
memory
>>> @njit
... def signal_func_nb(c, long_signals, short_signals, size):
...     long_signal = vbt.pf_nb.select_nb(c, long_signals)
...     short_signal = vbt.pf_nb.select_nb(c, short_signals)
...     if long_signal:
...         size[0, c.col] = 10  Since our array has only one element, vectorbt will pick its value for any row and column, thus we need to override this exact element
...     if short_signal:
...         size[0, c.col] = 5
...     return long_signal, False, short_signal, False

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("long_signals"), 
...         vbt.Rep("short_signals"), 
...         vbt.Rep("size")
...     ),
...     size=vbt.RepEval("np.full((1, wrapper.shape_2d[1]), np.nan)"),  In contrast to the previous example, here we only need one row
...     size_type="value",
...     accumulate=True,
...     broadcast_named_args=dict(
...         long_signals= pd.Series([X, O, O, O, X, O, O]),
...         short_signals=pd.Series([O, O, X, O, O, O, X]),
...     )
... )
>>> pf.orders.value.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     10.0     10.0
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00     -5.0     -5.0
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00     10.0     10.0
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00     -5.0     -5.0

>>> from collections import namedtuple

>>> Signals = namedtuple("Signals", [  Create a namedtuple to define a container for multiple arrays. Otherwise, the amount of arrays passed to the signal function would take a lot of space and be poorly readable. This named tuple contains two signal arrays: entries and exits.
...     "entries",
...     "exits"
... ])
>>> Order = namedtuple("Order", [  The second named tuple contains order-related arrays that are meant to be overridden in the signal function: size and size type. You can add more fields to it if needed.
...     "size",
...     "size_type"
... ])
>>> ladder_dt = np.dtype([  Ladders are stored in a two-dimensional array where each column represents a ladder and each row a step within this ladder. Each step should define how many bars need to pass and how much we need to order (basically a tuple). Regular NumPy arrays cannot store such information, but structured arrays can! Here, we define the data type for such an array.
...     ("after_n_bars", np.int_),
...     ("exit_pct", np.float_)
... ])
>>> ladder_info_dt = np.dtype([  We also need to keep track of the index of the user-defined entry and the current step being processed. We need this per ladder (column), thus we will create a one-dimensional structured array.
...     ("init_idx", np.int_),
...     ("step", np.int_)
... ], align=True)

>>> @njit  If you're finished with the function, don't forget to put it into another cell to avoid re-compilation! Also, don't cache it because it might break if you change some named tuples.
... def signal_func_nb(c, signals, order, ladders, last_ladder_info):  Our function takes the context, and the initialized named tuples and structured arrays that we defined above
...     is_entry = vbt.pf_nb.select_nb(c, signals.entries)
...     is_exit = vbt.pf_nb.select_nb(c, signals.exits)
...     position_now = c.last_position[c.col]
...     ladder_info = last_ladder_info[c.col]  Similarly to built-in information arrays such as last_position and last_sl_info, our latest ladder information is also laid out per column
...     
...     if position_now > 0 and is_exit:  If we encounter a user-defined exit signal, close out the position
...         ladder_info["init_idx"] = -1  Reset the temporary information to deactivate our laddering logic at the next bars
...         ladder_info["step"] = -1
...         order.size[c.i, c.col] = np.inf  Override the order-related information for this signal, here we go all out
...         order.size_type[c.i, c.col] = vbt.pf_enums.SizeType.Amount
...         return False, True, False, False  Long exit
...     
...     if is_entry:  If we encounter a user-defined entry signal, write the current bar index to the temporary information, set the current step to zero to activate our laddering logic, and return no signal to skip to the next bar
...         ladder_info["init_idx"] = c.i
...         ladder_info["step"] = 0
...         return False, False, False, False
...         
...     step = ladder_info["step"]
...     if step != -1 and step < ladders.shape[0]:  Check whether the ladder is active and the current step is lower than the total number of steps
...         after_n_bars = ladders.after_n_bars[step, c.col]  Remember that ladders are stored in a two-dimensional array with a ladder per column, that is, the first axis represents steps and the second axis represents columns
...         if after_n_bars != -1 and c.i >= ladder_info["init_idx"] + after_n_bars:  Next, we need to check whether the condition of the current step has been satisfied, that is, whether the current bar index comes after the target bar index. But first, we must check whether the value is defined; this is because ladders may have a different number of steps.
...             ladder_info["step"] = step + 1  Increment the current step
...             order.size[c.i, c.col] = ladders.exit_pct[step, c.col]  Execute the signal by ordering the requested percentage of the available resources
...             order.size_type[c.i, c.col] = vbt.pf_enums.SizeType.Percent
...             return True, False, False, False  Long entry
...     
...     return False, False, False, False  If there was no action during this bar, return no signal

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     signal_func_nb=signal_func_nb,
...     signal_args=(  Use templates to construct named tuples and record arrays. The arrays defined in broadcast_named_args will be automatically recognized in template expressions.
...         vbt.RepEval(
...             "Signals(entries, exits)",
...             context=dict(Signals=Signals)  We need to provide the classes via a context since they are not automatically available in the expressions
...         ),
...         vbt.RepEval(
...             "Order(size, size_type)",
...             context=dict(Order=Order)
...         ),
...         vbt.Rep("ladders"),  We will define ladders in broadcast_named_args to broadcast it together with data
...         vbt.RepEval(  Create a structured NumPy array that holds temporary information per ladder (column). Here, np.array((-1, -1), ladder_info_dt) is a single value that instructs NumPy to use the data type ladder_info_dt and fill both fields with -1.
...             "np.full(wrapper.shape_2d[1], np.array((-1, -1), ladder_info_dt))",
...             context=dict(ladder_info_dt=ladder_info_dt)
...         )
...     ),
...     size=vbt.RepEval("np.full(wrapper.shape_2d, np.nan)"),  Size and size type are built-in arguments that we want to override, thus make them of the full shape
...     size_type=vbt.RepEval("np.full(wrapper.shape_2d, -1)"),
...     accumulate=True,  Don't forget to enable accumulation to be able to gradually increase the position
...     broadcast_named_args=dict(
...         entries=pd.Series([X, O, O, O, O, O, O]),
...         exits=  pd.Series([O, O, O, O, O, O, O]),
...         ladders=vbt.BCO(  Use BCO to control broadcasting of a single argument
...             vbt.Param([  Use Param to test multiple values (here ladder arrays)
...                 np.array([  Luckily, any structured array can be initialized using regular tuples
...                     (2, 1/4),  For example, in the first step, once we're two bars away from the user-defined entry signal, execute an order for 25% of the available cash balance
...                     (3, 1/3),
...                     (4, 1/2),
...                     (5, 1/1)
...                 ], dtype=ladder_dt),
...                 np.array([
...                     (2, 1/2),
...                     (4, 1/1),
...                 ], dtype=ladder_dt),
...             ]),
...             axis=1,  Remember that the first axis are steps, not bars, thus our array isn't broadcastable along the time axis (axis=0), only along the column axis (axis=1)
...             merge_kwargs=dict(  What happens if one array has fewer steps than another? Under the hood, both arrays will be stacked along columns, but stacking cannot work if they have different shapes. Here, we specify that whenever an array has insufficient number of rows, it should be padded using fill_value.
...                 reset_index="from_start", 
...                 fill_value=np.array((-1, np.nan), dtype=ladder_dt),
...             )
...         )
...     ),
... )
>>> pf.asset_flow
ladder                      array_0   array_1
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.000000  0.000000
2021-02-20 00:00:00+00:00  0.000448  0.000895
2021-02-21 00:00:00+00:00  0.000435  0.000000
2021-02-22 00:00:00+00:00  0.000462  0.000924
2021-02-23 00:00:00+00:00  0.000511  0.000000
2021-02-24 00:00:00+00:00  0.000000  0.000000

last_position
last_sl_info
broadcast_named_args
broadcast_named_args
np.array((-1, -1), ladder_info_dt)
ladder_info_dt
axis=0
axis=1
fill_value
BTCUSDT
ETHUSDT
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=      pd.Series([X, O, O, O, O, O, O]),
...     exits=        pd.Series([O, O, O, X, O, O, O]),
...     short_entries=pd.Series([O, O, O, O, X, O, O]),
...     short_exits=  pd.Series([O, O, O, O, O, O, X]),
... )
>>> pf.get_value(group_by=True)
Open time
2021-02-18 00:00:00+00:00    200.000000
2021-02-19 00:00:00+00:00    209.238037
2021-02-20 00:00:00+00:00    206.946937
2021-02-21 00:00:00+00:00    211.045749
2021-02-22 00:00:00+00:00    211.045749
2021-02-23 00:00:00+00:00    232.943589
2021-02-24 00:00:00+00:00    228.775332
Freq: D, Name: group, dtype: float64

>>> grouped_pf = pf.replace(wrapper=pf.wrapper.replace(group_by=True))  Group-by instruction is part of the portfolio's wrapper
>>> grouped_pf.value
Open time
2021-02-18 00:00:00+00:00    200.000000
2021-02-19 00:00:00+00:00    209.238037
2021-02-20 00:00:00+00:00    206.946937
2021-02-21 00:00:00+00:00    211.045749
2021-02-22 00:00:00+00:00    211.045749
2021-02-23 00:00:00+00:00    232.943589
2021-02-24 00:00:00+00:00    228.775332
Freq: D, Name: group, dtype: float64

>>> grouped_pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.000000  0.000000
2021-02-20 00:00:00+00:00  0.000000  0.000000
2021-02-21 00:00:00+00:00 -0.001940 -0.051557
2021-02-22 00:00:00+00:00 -0.002059 -0.056080
2021-02-23 00:00:00+00:00  0.000000  0.000000
2021-02-24 00:00:00+00:00  0.002059  0.056080

group_by=False
>>> @njit
... def signal_func_nb(c, entries, exits):
...     is_entry = vbt.pf_nb.select_nb(c, entries)
...     is_exit = vbt.pf_nb.select_nb(c, exits)
...     other_in_position = False
...     for col in range(c.from_col, c.to_col):  Go through the columns in this group and check whether there are open positions apart from this one
...         if col != c.col and c.last_position[col] != 0:
...             other_in_position = True
...             break
...     if other_in_position:  If true, return no signal
...         return False, False, False, False
...     return is_entry, is_exit, False, False  If false, return the signal

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(vbt.Rep("entries"), vbt.Rep("exits")),
...     broadcast_named_args=dict(
...         entries=pd.DataFrame({
...             0: [X, O, O, O, O, O, X], 
...             1: [O, X, O, X, O, O, O]
...         }), 
...         exits=pd.DataFrame({
...             0: [O, O, X, O, X, O, O], 
...             1: [O, O, O, O, O, X, O]
...         })
...     ),
...     group_by=True  Put all columns into the same group. For multiple groups, use vbt.ExceptLevel("symbol")
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.000000
2021-02-19 00:00:00+00:00  0.00000  0.000000
2021-02-20 00:00:00+00:00 -0.00194  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.051719
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000 -0.051719
2021-02-24 00:00:00+00:00  0.00218  0.000000

vbt.ExceptLevel("symbol")
ETHUSDT
BTCUSDT
call_seq="auto"
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price=[["close", "open"]],  If we want to provide any information per column, we need to build a two-dimensional array with exactly one row (any array-like object will be transformed into a NumPy array, thus passing a list works too)
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True  Turn on cash sharing
... )
>>> pf.orders.get_value(group_by=False).to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     50.0     50.0
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="close",
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True
... )
>>> pf.orders.get_value(group_by=False).to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    100.0      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="close",
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True,
...     call_seq="auto"  Here
... )
>>> pf.orders.get_value(group_by=False).to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     50.0     50.0
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price=[["close", "open"]],
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True,
...     call_seq="auto"
... )
ValueError: Cannot sort orders by value if they are executed at different times

-np.inf
np.inf
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price=sub_data.close,
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True,
...     call_seq="auto"
... )
ValueError: Cannot sort orders by value if they are executed at different times

PriceType.Close
StopExitPrice.Close
save_returns
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=      pd.Series([X, O, O, O, O, O, O]),
...     exits=        pd.Series([O, O, O, X, O, O, O]),
...     short_entries=pd.Series([O, O, O, O, X, O, O]),
...     short_exits=  pd.Series([O, O, O, O, O, O, X]),
...     save_returns=True
... )
>>> pf.returns
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.084446  0.007935
2021-02-20 00:00:00+00:00 -0.001159 -0.021483
2021-02-21 00:00:00+00:00  0.028069  0.010732
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.096079  0.112338
2021-02-24 00:00:00+00:00 -0.013245 -0.023012

>>> pf.get_returns()  Every attribute with get_ triggers a reconstruction
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.084446  0.007935
2021-02-20 00:00:00+00:00 -0.001159 -0.021483
2021-02-21 00:00:00+00:00  0.028069  0.010732
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.096079  0.112338
2021-02-24 00:00:00+00:00 -0.013245 -0.023012

get_
>>> pf.in_outputs
FSInOutputs(returns=array([[ 0.        ,  0.        ],
                           [ 0.08444579,  0.00793458],
                           [-0.00115927, -0.02148338],
                           [ 0.02806853,  0.01073183],
                           [ 0.        ,  0.        ],
                           [ 0.09607864,  0.11233812],
                           [-0.01324464, -0.02301153]]))

>>> pf.get_in_output("returns")
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.084446  0.007935
2021-02-20 00:00:00+00:00 -0.001159 -0.021483
2021-02-21 00:00:00+00:00  0.028069  0.010732
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.096079  0.112338
2021-02-24 00:00:00+00:00 -0.013245 -0.023012

save_returns
in_outputs
post_segment_func
>>> @njit
... def post_segment_func_nb(c):
...     returns = c.in_outputs.returns  SignalSegmentContext.in_outputs contains exactly the in-output tuple that we passed
...     total_return = c.in_outputs.total_return
...     i = c.i
...     g = c.group
...     if c.cash_sharing:  If cash sharing is enabled, returns must be computed per group
...         returns[i, g] = c.last_return[g]
...         total_return[g] = c.last_value[g] / c.init_cash[g] - 1
...     else:  If cash sharing is disabled, returns must be computed per column
...         for col in range(c.from_col, c.to_col):  Iterate over each column in the current group
...             returns[i, col] = c.last_return[col]
...             total_return[col] = c.last_value[col] / c.init_cash[col] - 1

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=      pd.Series([X, O, O, O, O, O, O]),
...     exits=        pd.Series([O, O, O, X, O, O, O]),
...     short_entries=pd.Series([O, O, O, O, X, O, O]),
...     short_exits=  pd.Series([O, O, O, O, O, O, X]),
...     post_segment_func_nb=post_segment_func_nb,  Overriding this callback is enough to switch to the flexible mode
...     in_outputs=dict(  In-output tuple can be provided either as a named tuple or a dict. If the latter, if will be automatically transformed into a named tuple by the method.
...         returns=vbt.RepEval(  Use expression evaluation templates to build arrays only after the target shape is established. Use len(cs_group_lens) to get the number of groups when cash sharing is enabled, and the number of columns when cash sharing is disabled.
...             "np.full((target_shape[0], len(cs_group_lens)), np.nan)"
...         ),
...         total_return=vbt.RepEval(
...             "np.full(len(cs_group_lens), np.nan)"
...         )
...     )
... )
>>> pd.testing.assert_frame_equal(  Check the pre-computed and reconstructed arrays for equality
...     pf.get_in_output("returns"),
...     pf.get_returns()
... )
>>> pd.testing.assert_series_equal(
...     pf.get_in_output("total_return"),
...     pf.get_total_return()
... )

len(cs_group_lens)
>>> import numpy as np
>>> import pandas as pd
>>> from numba import njit
>>> import vectorbtpro as vbt

>>> data = vbt.BinanceData.pull("BTCUSDT", end="2022-11-01 UTC")
>>> data.index
DatetimeIndex(['2017-08-17 00:00:00+00:00', '2017-08-18 00:00:00+00:00',
               '2017-08-19 00:00:00+00:00', '2017-08-20 00:00:00+00:00',
               ...
               '2022-10-28 00:00:00+00:00', '2022-10-29 00:00:00+00:00',
               '2022-10-30 00:00:00+00:00', '2022-10-31 00:00:00+00:00'],
    dtype='datetime64[ns, UTC]', name='Open time', length=1902, freq='D')

>>> @vbt.parameterized(merge_func="concat")  The decorator @parameterized enhances sma_crossover_perf to take arguments wrapped with Param, build the grid of parameter combinations, run sma_crossover_perf on each parameter combination, and merge the results using concatenation
... def sma_crossover_perf(data, fast_window, slow_window):
...     fast_sma = data.run("sma", fast_window, short_name="fast_sma")  Use Data.run to run an indicator on a data instance
...     slow_sma = data.run("sma", slow_window, short_name="slow_sma")
...     entries = fast_sma.real_crossed_above(slow_sma)
...     exits = fast_sma.real_crossed_below(slow_sma)
...     pf = vbt.Portfolio.from_signals(
...         data, entries, exits, direction="both")  Enable shorting by marking exit signals as short entry signals
...     return pf.sharpe_ratio  Our function returns a single value, which will become a Series once all parameter combinations are processed

sma_crossover_perf
sma_crossover_perf
fast_window
slow_window
>>> perf = sma_crossover_perf(  Even while being decorated with parameterized, the function has the same arguments as sma_crossover_perf
...     data["2020":"2020"],  Data instances can be sliced like regular Pandas objects. Remember that the right bound is inclusive.
...     vbt.Param(np.arange(5, 50), condition="x < slow_window"),  There is no sense in having a window for a fast SMA longer than a window for a slow SMA, thus let's define a condition that cuts the number of parameter combinations in half
...     vbt.Param(np.arange(5, 50)),  We don't need to define any condition for the second parameter since it's already captured by the first parameter
...     _execute_kwargs=dict(  Any arguments that we want to pass to the @parameterized decorator instead must have the prefix _
...         show_progress=True,
...         clear_cache=50,  By default, the parameterized decorator is calling the SerialEngine to execute each parameter combination. This engine has arguments to clear the cache and collect any memory garbage. Here, we're doing it once every 50 iterations to keep memory in check.
...         collect_garbage=50
...     )
... )
>>> perf
fast_window  slow_window
5            6              0.625318
             7              0.333243
             8              1.171861
             9              1.062940
             10             0.635302
                                 ...   
46           48             0.534582
             49             0.573196
47           48             0.445239
             49             0.357548
48           49            -0.826995
Length: 990, dtype: float64

parameterized
sma_crossover_perf
_
parameterized
>>> perf.sort_values(ascending=False)
fast_window  slow_window
15           20             3.669815
14           19             3.484855
15           18             3.480444
14           21             3.467951
13           19             3.457093
                                 ...   
36           41             0.116606
             37             0.075805
42           43             0.004402
10           12            -0.465247
48           49            -0.826995
Length: 990, dtype: float64

fast_window=15
slow_window=20
>>> best_fast_window, best_slow_window = perf.idxmax()  Get the parameter values with the highest Sharpe ratio
>>> sma_crossover_perf(
...     data["2021":"2021"],
...     best_fast_window,  No need to wrap the parameters with Param if they are single values or if we don't want to build the grid out of them. They are simply forwarded down to sma_crossover_perf.
...     best_slow_window
... )
-1.1940481501019478

sma_crossover_perf
>>> data["2021":"2021"].run("from_holding").sharpe_ratio  Data.run not only accepts indicator names but also portfolio method names. First run will take a while because it has to be compiled first.
0.9641311236043749

>>> from tqdm.auto import tqdm

>>> start_index = data.index[0]  Get the first date in the index. Has the type pandas.Timestamp.
>>> period = pd.Timedelta(days=180)  Define the period as pandas.Timedelta to be able to combine it with pandas.Timestamp
>>> all_is_bounds = {}  Range start (inclusive) and end (exclusive) index, baseline (i.e., buy-and-hold) performance, and optimization performance of each IS period
>>> all_is_bl_perf = {}
>>> all_is_perf = {}
>>> all_oos_bounds = {}  Range start and end index, baseline performance, and validation performance of each OOS period
>>> all_oos_bl_perf = {}
>>> all_oos_perf = {}
>>> split_idx = 0
>>> period_idx = 0

>>> with tqdm() as pbar:  Make sure to install tqdm to display the progress bar
...     while start_index + 2 * period <= data.index[-1]:  Why not a for-loop? Because we don't know the number of splits in advance. We process one split, increment the starting date, and repeat, which translates perfectly into a while-loop. We do it as long as both periods are fully contained in our index.
...         pbar.set_description(str(start_index))
...
...         is_start_index = start_index
...         is_end_index = start_index + period - pd.Timedelta(1)  The right bound of the IS period should be exclusive since we don't want it to overlap with the left bound of the OOS period, but since pandas.DataFrame.loc includes the right bound, we simply subtract one nanosecond from it (remember this trick!)
...         is_data = data[is_start_index : is_end_index]
...         is_bl_perf = is_data.run("from_holding").sharpe_ratio
...         is_perf = sma_crossover_perf(
...             is_data,
...             vbt.Param(np.arange(5, 50), condition="x < slow_window"),
...             vbt.Param(np.arange(5, 50)),
...             _execute_kwargs=dict(
...                 clear_cache=50,
...                 collect_garbage=50
...             )
...         )
...
...         oos_start_index = start_index + period  The IS period covers the days from [0, 180) while the OOS period covers the days from [180, 360)
...         oos_end_index = start_index + 2 * period - pd.Timedelta(1)
...         oos_data = data[oos_start_index : oos_end_index]
...         oos_bl_perf = oos_data.run("from_holding").sharpe_ratio
...         best_fw, best_sw = is_perf.idxmax()
...         oos_perf = sma_crossover_perf(oos_data, best_fw, best_sw)
...         oos_perf_index = is_perf.index[is_perf.index == (best_fw, best_sw)]
...         oos_perf = pd.Series([oos_perf], index=oos_perf_index)  The function call above returns a single value, but let's make it a Series with a proper index to know which parameter combination it corresponds to
...
...         all_is_bounds[period_idx] = (is_start_index, is_end_index)
...         all_oos_bounds[period_idx + 1] = (oos_start_index, oos_end_index)
...         all_is_bl_perf[(split_idx, period_idx)] = is_bl_perf
...         all_oos_bl_perf[(split_idx, period_idx + 1)] = oos_bl_perf
...         all_is_perf[(split_idx, period_idx)] = is_perf
...         all_oos_perf[(split_idx, period_idx + 1)] = oos_perf
...         start_index = start_index + period  The next IS period starts where the current IS period ends
...         split_idx += 1
...         period_idx += 1
...         pbar.update(1)  Tell the progress bar that we processed this split

>>> is_period_ranges = pd.DataFrame.from_dict(  If each value of a dict consists of multiple values, use pandas.DataFrame.from_dict to concatenate them into one DataFrame
...     all_is_bounds, 
...     orient="index",
...     columns=["start", "end"]
... )
>>> is_period_ranges.index.name = "period"
>>> oos_period_ranges = pd.DataFrame.from_dict(
...     all_oos_bounds, 
...     orient="index",
...     columns=["start", "end"]
... )
>>> oos_period_ranges.index.name = "period"
>>> period_ranges = pd.concat((is_period_ranges, oos_period_ranges))  We know the bounds of each IS and OOS set, but since OOS periods are effectively next IS periods, let's create just one Series keyed by period
>>> period_ranges = period_ranges.drop_duplicates()
>>> period_ranges
                           start                                 end
period                                                              
0      2017-08-17 00:00:00+00:00 2018-02-12 23:59:59.999999999+00:00
1      2018-02-13 00:00:00+00:00 2018-08-11 23:59:59.999999999+00:00
2      2018-08-12 00:00:00+00:00 2019-02-07 23:59:59.999999999+00:00
3      2019-02-08 00:00:00+00:00 2019-08-06 23:59:59.999999999+00:00
4      2019-08-07 00:00:00+00:00 2020-02-02 23:59:59.999999999+00:00
5      2020-02-03 00:00:00+00:00 2020-07-31 23:59:59.999999999+00:00
6      2020-08-01 00:00:00+00:00 2021-01-27 23:59:59.999999999+00:00
7      2021-01-28 00:00:00+00:00 2021-07-26 23:59:59.999999999+00:00
8      2021-07-27 00:00:00+00:00 2022-01-22 23:59:59.999999999+00:00
9      2022-01-23 00:00:00+00:00 2022-07-21 23:59:59.999999999+00:00

>>> is_bl_perf = pd.Series(all_is_bl_perf)  If each value of a dict is a single value, wrap the dict directly with Series
>>> is_bl_perf.index.names = ["split", "period"]
>>> oos_bl_perf = pd.Series(all_oos_bl_perf)
>>> oos_bl_perf.index.names = ["split", "period"]
>>> bl_perf = pd.concat((  The same as with bounds, but we need to remove the split level beforehand
...     is_bl_perf.vbt.select_levels("period"), 
...     oos_bl_perf.vbt.select_levels("period")
... ))
>>> bl_perf = bl_perf.drop_duplicates()
>>> bl_perf
period
0    1.846205
1   -0.430642
2   -1.741407
3    3.408079
4   -0.556471
5    0.954291
6    3.241618
7    0.686198
8   -0.038013
9   -0.917722
dtype: float64

>>> is_perf = pd.concat(all_is_perf, names=["split", "period"])  If each value of a dict is a Series, use pandas.concat to concatenate them into one Series
>>> is_perf
split  period  fast_window  slow_window
0      0       5            6              1.766853
                            7              2.200321
                            8              2.698365
                            9              1.426788
                            10             0.849323
                                                ...   
8      8       46           48             0.043127
                            49             0.358875
               47           48             1.093769
                            49             1.105751
               48           49             0.159483
Length: 8910, dtype: float64

>>> oos_perf = pd.concat(all_oos_perf, names=["split", "period"])
>>> oos_perf
split  period  fast_window  slow_window
0      1       19           34             0.534007
1      2       6            7             -1.098628
2      3       18           20             1.687363
3      4       14           18             0.035346
4      5       18           21             1.877054
5      6       20           27             2.567751
6      7       11           18            -2.061754
7      8       29           30             0.965434
8      9       25           28             1.253361
dtype: float64

>>> is_best_mask = is_perf.index.vbt.drop_levels("period").isin(  Get the performance of the best parameter combination in each IS set using the index of the corresponding OOS set, but since both Series are connected using split, fast_window, and slow_window, remove period prior to the operation
...     oos_perf.index.vbt.drop_levels("period"))
>>> is_best_perf = is_perf[is_best_mask]
>>> is_best_perf
split  period  fast_window  slow_window
0      0       19           34             4.380746
1      1       6            7              2.538909
2      2       18           20             4.351354
3      3       14           18             3.605775
4      4       18           21             3.227437
5      5       20           27             3.362096
6      6       11           18             4.644594
7      7       29           30             3.379370
8      8       25           28             2.143645
dtype: float64

split
split
fast_window
slow_window
period
split
period
fast_window
slow_window
>>> pd.concat((
...     is_perf.describe(),
...     is_best_perf.describe(),
...     is_bl_perf.describe(),
...     oos_perf.describe(),
...     oos_bl_perf.describe()
... ), axis=1, keys=[
...     "IS", 
...     "IS (Best)", 
...     "IS (Baseline)", 
...     "OOS (Test)", 
...     "OOS (Baseline)"
... ])
                IS  IS (Best)  IS (Baseline)  OOS (Test)  OOS (Baseline)
count  8882.000000   9.000000       9.000000    9.000000        9.000000
mean      0.994383   3.514881       0.818873    0.639993        0.511770
std       1.746003   0.843435       1.746682    1.480066        1.786012
min      -3.600854   2.143645      -1.741407   -2.061754       -1.741407
25%      -0.272061   3.227437      -0.430642    0.035346       -0.556471
50%       1.173828   3.379370       0.686198    0.965434       -0.038013
75%       2.112042   4.351354       1.846205    1.687363        0.954291
max       4.644594   4.644594       3.408079    2.567751        3.408079

period
>>> fig = is_perf.vbt.boxplot(  Use GenericAccessor.boxplot
...     by_level="period",  This Series has many levels, we need to decide by which one(s) to aggregate
...     trace_kwargs=dict(  This argument controls the appearance of the trace (see plotly.graph_objects.Box)
...         line=dict(color="lightskyblue"), 
...         opacity=0.4,
...         showlegend=False
...     ),
...     xaxis_title="Period",  The first plotting method will create a Plotly figure and set it up, thus pass layout settings such as axis labels here
...     yaxis_title="Sharpe",
... )
>>> is_best_perf.vbt.select_levels("period").vbt.plot(  Use GenericAccessor.plot
...     trace_kwargs=dict(
...         name="Best", 
...         line=dict(color="limegreen", dash="dash")
...     ), 
...     fig=fig  Don't forget to pass the figure to plot the new trace on, otherwise it will create a new one
... )
>>> bl_perf.vbt.plot(
...     trace_kwargs=dict(
...         name="Baseline", 
...         line=dict(color="orange", dash="dash")
...     ), 
...     fig=fig
... )
>>> oos_perf.vbt.select_levels("period").vbt.plot(
...     trace_kwargs=dict(
...         name="Test", 
...         line=dict(color="orangered")
...     ), 
...     fig=fig
... )
>>> fig.show()

6
1.07
4.64
>>> is_perf_split6 = is_perf.xs(6, level="split")  Select the values with the split level being 6 and collect statistics from them
>>> is_perf_split6.describe()
count    990.000000
mean       3.638821
std        0.441206
min        1.073553
25%        3.615566
50%        3.696611
75%        3.844124
max        4.644594
dtype: float64

>>> first_left_bound = period_ranges.loc[6, "start"]
>>> first_right_bound = period_ranges.loc[6, "end"]
>>> data[first_left_bound : first_right_bound].plot().show()  Display the candlestick chart during that period for visual confirmation

split
6
6
>>> oos_perf.xs(6, level="period")
split  fast_window  slow_window
5      20           27             2.567751
dtype: float64

>>> is_perf_split6.quantile(0.25)  25% of parameter combinations have a Sharpe ratio below that value, including our parameter combination used for testing
3.615566166097048

